url,title,subtitle,date,author,content,newsletter
https://www.lennysnewsletter.com/p/productpass,Lenny's Product Pass: 19+ free premium products available exclusively to paid annual subscribers,"Over $20,000 in value for just $350. All the tools, community, and advice you need to build and grow a world-class product and career.",2026-01-02T16:53:36.623588,Lenny Rachitsky,"Iâ€™m excited to announce the newest, most unbelievable collection of premium products now available exclusively to paid annual subscribers.

Paid subscribers now get a free year of (while supplies last):

Building faster with

Lovable

[Insider only],

Replit

[Insiders only],

Bolt

[Insiders only],

n8n

,

Devin

[Insider only],

Warp

,

PostHog

,

Magic Patterns

,

ChatPRD

Collaborating better with

Linear

,

Superhuman

,

Granola

Making it more beautiful with

Descript

[Insider only]

,

Gamma

,

Mobbin

Getting more done with

Wispr Flow

,

Raycast

,

Perplexity

Incorporating your startup with

Stripe Atlas

[Insider only] ($200 off the one-time setup fee).

This curated collection includes the most innovative and beautifully crafted products out thereâ€”some that you already love, some that are under the radar, and all of which I use regularly.

The collection is worth over $20,000, for just $200-350/year (depending on your tier).

Yes, this is for real.

Itâ€™s arguably the most ambitious product bundle ever attempted, and none of these companies have ever offered this deal on their products before.

Offer codes are limited, and granted on a first-come, first-served basis. So act quickly. Weâ€™ll be adding new products quarterly, and products will rotate in and out of the collection.

Subscribe

Why am I doing this?

Things are moving fast. Jobs are disappearing, new skills are emerging, and every week, a new tool launches that we need to become experts in to stay relevant.

What we need most to navigate this moment is practical advice from people in the trenches, community to support us through all the change, and access to the latest tools and technologies to truly understand where things are heading.

Thatâ€™s precisely what Iâ€™m trying to provide here.

Weâ€™ve already built the worldâ€™s largest newsletter and podcast for product builders, and a thriving global Slack community of tens of thousands (with local meetups, AMAs, mentorship programs, and more). Now, I want to do more to increase access.

Think of your Lennyâ€™s Newsletter subscription as your product builderâ€™s starter pack: all the advice, community, and tools you need to build and grow a world-class product (and career). For just $200-350/year.

Subscribe

In the coming months, Iâ€™ll share guidance on how to take full advantage of these products to help you work better, deeper, and faster.

Important offer details (read before subscribing):

You must be a new customer of the products

to take advantage of the free year. If youâ€™ve already paid for one of the products before (monthly or yearly), or redeemed a code from our previous bundle, you wonâ€™t be able to get a free additional year of that specific product.

You must get an

Annual or Insider subscription

to Lennyâ€™s Newsletter

to be eligible for this offer. Monthly subscribers arenâ€™t eligible.

Both existing and new subscribers

of Lennyâ€™s Newsletter are eligible for this deal. Existing subscribers can

redeem the offer here

.

We may run out of codes for a product at any time

. Your code is only secured once you

claim it

. You need to claim each code individually, so hop on the products you are most excited about.

Your one free year of the product begins when you redeem the deal on the partnerâ€™s website

,

not when you purchase this newsletter subscription or claim your code.

All sales are final. We wonâ€™t issue refunds if a couple of products are out by the time you claim them.

The deal is already so good, even if a couple of products are missing. If you request a chargeback for your subscription, all of your codes will be deactivated.

Subscribe

Introducing the Insider tier: guaranteed access to every product

Since our offer codes are limited (i.e. our partners arenâ€™t giving us unlimited free accounts), weâ€™re expecting to run out of some of the more popular products. To make sure you get access to everything you want (without worrying about missing out),

weâ€™re introducing the Insider tierâ€”for $350/yearâ€”which includes two benefits:

Guaranteed access:

While Annual subscribers get access on a first-come, first-served basis, Insider members get reserved access to every tool in the collection. If we offer it when you become a subscriber, youâ€™ll get it.

Exclusive premium partner offers:

We will be adding a limited number of select premium partner products on an ongoing basis, which will be available exclusively to Insider members. Currently, there are no exclusive premium partners. Weâ€™ll be announcing them in the coming months.

Over $20,000 in value for just $350/yearâ€”without the stress.

Many readers expense this subscription out of their workplaceâ€™s learning and development budget.

Hereâ€™s an email to send to your manager

. If youâ€™re already an annual subscriber, you can upgrade

here

.

Subscribe

ğŸ’¥ How to redeem the offer ğŸ’¥

Step 1:

Get an Annual or Insider subscription

to Lennyâ€™s Newsletter.

Step 2:

Redeem your codes here

.

Step 3:

Profit.

Subscribe

To summarize, your paid subscription now includes:

A deeply researched newsletter and podcast with a 5+ year back catalog of evergreen content

An invite to a 30,000+ member-only global Slack community for product builders, with local meetups, AMAs, mentorship programs, book clubs, and more

This growing collection of free world-class tools

Itâ€™s everything you need to build a world-class productâ€”and career.

Subscribe

If youâ€™ve already got a subscription, consider

gifting a subscription

to a colleague or

purchasing a group subscription

for your team.

Sincerely,

Lenny",https://www.lennysnewsletter.com
https://www.lennysnewsletter.com/p/ai-tools-are-overdelivering-results,AI tools are overdelivering: results from our large-scale AI productivity survey,"What exactly AI is doing for people, which AI tools have product-market fit, where the biggest opportunities remain, and what it all means",2026-01-02T16:53:38.153994,Noam Segal,"ğŸ‘‹  Hey there, Iâ€™m Lenny. Each week, I tackle reader questions about building product, driving growth, and accelerating your career. For more:

Lennybot

|

Lennyâ€™s Podcast

|

How I AI

|

Lennyâ€™s Reads

|

Fav AI/PM courses

|

Fav public speaking course

Subscribe

Annual subscribers get

19 premium products for free for one year

:

Lovable, Replit, Gamma, n8n, Bolt, Devin, Wispr Flow, Descript, Linear, PostHog, Superhuman, Granola, Warp, Perplexity, Raycast, Magic Patterns, Mobbin, ChatPRD, and Stripe Atlas

(while supplies last).

Subscribe now

.

You can also listen to this post in convenient podcast form on

Spotify

/

Apple

/

YouTube

.

Iâ€™m excited to share my (record) fourth collaboration with the great

Noam Segal

, AI Insights Manager at Figma and former UXR leader at Zapier, Airbnb, Meta, Twitter, Intercom, and Wealthfront. Letâ€™s get to it.

Authorâ€™s note:

Names have been changed to preserve participant anonymity.

Thereâ€™s no shortage of debate about AIâ€™s impact on work. Is it delivering real productivity gains? Whereâ€™s the ROI? Hot takes abound, but data have been scarce.

We took it upon ourselves to find out whatâ€™s actually happening on the ground by running one of the largest independent, in-depth surveys on how AI is affecting productivity for tech workers (1,750 respondents). We surveyed product managers, engineers, designers, founders, and others about how theyâ€™re using AI at work.

tl;dr: AI is

overdelivering

.

55% of respondents say AI has

exceeded

their expectations, and almost 70% say itâ€™s improved the

quality

of their work.

More than half of respondents said AI is saving them at least half a day per week

on their most important tasks. Weâ€™ve never seen a tool deliver a productivity boost like this before.

Founders are getting the most out of AI.

Half (49%) report that AI saves them

over

6 hours per week

, dramatically higher than for any other role. Close to half (45%) also feel that the quality of their work is â€œmuch betterâ€ thanks to AI.

Designers are seeing the fewest benefits.

Only 45% report a positive ROI (compared with 78% of founders), and 31% report that AI has fallen below expectations, triple the rate among founders.

Engineers have accepted AI as a coding partner and now want it to handle the more boring (but necessary) work of building products:

documentation, code review, and writing tests

.

n8n is currently dominating the agent landscape

, though actual adoption of agentic platforms in 2025 has been slow.

A whopping 92.4% of respondents report at least one significant downsides to using AI tools.

Thereâ€™s definitely room for improvement.

AI is far from the novelty it was a year or two ago. It has clearly cemented a place as work and productivity infrastructure, and AI tools are improving at a breathtaking pace. If AI is already giving most people back at least half a day per week in late 2025, what does 2026 look like? What about 2027?

Weâ€™re watching the early innings of a compounding productivity revolution.

As Kevin Weil (VP at OpenAI) noted

,

â€œThe AI model that youâ€™re using today is the worst AI model you will ever use for the rest of your life.â€

What exactly AI is doing for people, function by function

PMs

are seeing the most value from AI tools to (1)

write PRDs

(21.5%), (2)

create mockups/prototypes

(19.8%), and (3)

improve their communication

across emails and presentations (18.5%).

Prototyping, at #2, signals one of many role-boundary shifts happening now. With tools like Lovable, v0, and others, PMs are increasingly going from idea to prototype without waiting on design.

But look farther down the list and a pattern emerges: AI is helping PMs

produce

, but it lags in helping them

think

. The top jobs are all production tasks (docs, prototypes, comms), while strategic and discovery work sits near the bottom (

user research

at 4.7%,

roadmap ideas

at 1.1%). PMs have cracked how to use AI for the â€œlast mileâ€ of getting ideas out of their head, but they still have a big opportunity to embrace AI for the upstream work of figuring out what to build.

Designers

are finding AI most helpful with

user research synthesis

(22.3%),

content and copy

(17.4%), and

design concepts ideation

(16.5%).

Visual design ranks #8, at just 3.3%.

AI is helping designers with everything

around

design (research synthesis, copy, ideation), but pushing pixels remains stubbornly human. Meanwhile, compare prototyping: PMs have it at #2 (19.8%), while designers have it at #4 (13.2%). AI is unlocking skills for PMs

outside

of their core work (at least in the case of prototyping), whereas designers arenâ€™t seeing the marginal improvement benefits from AI doing their core work.

Founders

lean heavily toward

productivity and decision support

(32.9%),

product ideation

(19.6%), and

vision/strategy

(19.1%).

Unlike others, founders are using AI to

think

, not just to produce.

The top three jobs are all

strategic

:

decision support

,

ideation

, and

vision/strategy

. Thatâ€™s a stark contrast to PMs (whose top jobs are documents and prototypes) and designers (research synthesis and copy). And look at that #1 category: â€œproductivity/decision support,â€ at 32.9%, is unlike anything else in the survey. No other role has a single use case this dominant. Founders are treating AI as a thought partner and sounding board, not just a tool for specific deliverables. (This tracks with

Talâ€™s excellent post on building AI copilots as long-term thinking partners

and Amirâ€™s recent post on

building your second brain using ChatGPT

.)

The surprise misses: Financial modeling

sits at just 1.8%, despite founders living in spreadsheets during fundraising. Same with

recruiting

, at 1.3%, even though hiring consumes enormous founder time. These feel like opportunities waiting for better tools.

This pattern may explain why founders report the highest satisfaction throughout the surveyâ€”theyâ€™ve figured out how to use AI for higher-leverage strategic work, not just production tasks.

Engineers are the outlier.

For them, AI is doing just one big job:

writing code

, the core engineering task. Whereas for the PMs and designers, AI is helping them with supporting work.

Farther down the list are jobs like

documentation

(7.7%),

testing

(6.2%), and

code review

(4.3%). These are the â€œboring but necessaryâ€ tasks engineers typically dislike. As youâ€™ll see in the opportunities data below, thatâ€™s about to change. Engineers have accepted AI as a coding partner; now they want it to handle the tedious work that comes after the code has been written.

One more pattern worth noting: engineers report the most mixed results on quality later in the survey (51% better but 21% worse, the highest â€œworseâ€ of any role).

Engineers are the only role where ChatGPT isnâ€™t #1

ChatGPT is the #1 most popular AI tool for most roles

: 57.7% of PMs, 49.6% of designers, and 72.1% (!!!) of founders use ChatGPT over any other AI tool, with Claude coming in second for those three roles.

But engineers have a very different behavior.

GitHub Copilot was first to market, has Microsoft and GitHubâ€™s distribution muscle, and is baked into the worldâ€™s most popular code repository. Yet it sits behind three tools that launched after it. Engineers are choosing newer (better) alternatives over the incumbent.

For engineers, the top three are in a dead heat:

Cursor

(33.2%),

ChatGPT

(30.8%), and

Claude Code

(29.0%) are all within 4 percentage points. This market hasnâ€™t consolidated, and switching costs are low. Also notable: Claude Code (29.0%) outpaces Claudeâ€™s chat interface (20.7%). Purpose-built tools are winning, but Claude is also helpful with several core coding-related tasks (e.g. code migration and

more

) that put it at fourth.

Gemini

sits at a distant 10.6%, but a caveat: this space shifts

fast

. A few strong model releases or product updates could reshape these rankings quickly. Whatâ€™s true today may look very different in six months.

ChatGPT

is a far-and-away winner for PMs.

Perplexity

is also surprisingly highly ranked, probably due to its strong research capabilities.

However, farther down the list,

Lovable

(8.7%) and

Cursor

(7.7%) are cracking the top seven for PMs. This reinforces the pattern we saw earlier: PMs are increasingly building things themselves, encroaching on whatâ€™s traditionally design and engineering work. The PM toolkit is expanding beyond documents and decks.

One note:

Copilot

(8.4%) edges out

Cursor

(7.7%) among PMs, though the reverse is true for engineers. This may reflect Microsoft ecosystem lock-in at larger companies, or simply that PMs discovered Copilot first and havenâ€™t yet explored alternatives.

AI is driving significant time

and

quality gains (for most)

63% of PMs and 83% of founders report that AI saves 4+ hours per week.

Even the most skeptical group, designers, still shows 47.5% reporting 4+ hours saved. Only 1% to 5% of respondents across roles say AI is â€œno faster than manual work.â€

On quality, though, the story is more nuanced. PMs and founders are bullish (over 70% report quality improvements), but engineers are more mixed.

51% of engineers tell us that AI makes the quality of their work better, but 21% say itâ€™s worse.

Designers fall in between, at 60% better, 13% worse. The quality ratings among engineers may reflect the higher bar for correctness in code: a â€œsomewhat betterâ€ first draft of a PRD is useful; a â€œsomewhat betterâ€ but buggy function is not. Also, bad code is easier to spot than a bad PRD.

Where are the opportunities for more AI help?

The gap between where people are using AI today and where they

want

to use it next reveals a lot about where the opportunities are for founders and startups to jump in and deliver new tools and functions.

For PMs, the biggest opportunity story is

research

.

User research shows the largest demand gap of any task (+27.2pp). Only 4.7% say itâ€™s their primary AI use case today, but nearly a third want it to be. The pattern is clear: PMs have figured out how to use AI for output tasks like writing PRDs and drafting communications, but theyâ€™re hungry to apply it upstream, to the messy work of understanding

what

to build.

Prototyping is a breakout category.

For PMs, â€œ

creating mockups/prototypes

â€ jumps from 19.8% (currently using) to 44.4% (want to use next), a +24.6pp swing that makes it the single most-wanted future use case. For designers, prototyping and interaction design show similar momentum (+27.8pp). This tracks with the rise of tools like Lovable, v0, Replit, and Figma Make: people have seen whatâ€™s possible and want more.

Engineers are shifting their use of AI to handle work

after

writing the code.

Writing code was by far their most popular use case (51% current), but it has a demand gap of only +5.6pp. However,

documentation

(+25.8pp),

code review

(+24.5pp), and

writing tests

(+23.5pp) all show massive opportunities for growth in engineering AI tooling.

Founders are doubling down on AI as a thinking partner. Product ideation

shows massive demand, jumping from 19.6% (currently using) to 48.6% (want to use next), a +29.0pp gap.

Growth strategy and GTM planning

(+24.7pp) and

market analysis

(+24.0pp) follow close behind.

Founders already use AI heavily for personal productivity (32.9% currently), but they want to move upstream. Theyâ€™re looking for a strategic collaborator to pressure-test ideas, explore markets, and think through go-to-marketâ€”AI as a

co-founder

,

not just an assistant.

Based on these reported gaps, the next wave of AI adoption will require not just better models but better workflows for human-AI collaboration on fuzzy problems. Writing a PRD has a clear output; competitive research does not. Writing code can be tested; â€œproduct ideationâ€ cannot.

Which AI tools have product-market fit?

We asked: â€œWhich AI tool(s) would you be very disappointed to lose access to?â€ The classic Sean Ellis PMF question. 83.6% named at least one tool, which is itself a remarkable signal of how embedded AI has become in daily workflows. But the relationship between the number of people who regularly use a tool and would miss that tool if it went away tells the story of the products that have truly found product-market fit.

ChatGPT dominates, perhaps only for now.

Half of respondents (50.2%) would be very disappointed to lose ChatGPT, but thatâ€™s notably lower than the 60% to 75% of respondents across most roles who say they regularly use the tool. This, in part, explains why OpenAI recently declared a â€œCode Redâ€ as it watches Gemini and Claude begin to erode market share. Switching costs in AI are still very low.

ChatGPT, Claude, and Gemini top the list for PMsâ€”theyâ€™re such multi-purpose tools well-suited to the PM job.

Itâ€™s most interesting to see Cursor right behind Gemini (we wouldnâ€™t expect an engineering tool like Cursor to be so popular among PMs), followed by Lovable (which currently seems to be winning in the prototyping market).

Designers (23.3%) and founders (20.6%) index highest on Claude. The Claude ecosystem (Claude and Claude Code combined) reaches 27.5% overall. This feels like a big win for Anthropic.

Specialized engineering tools have found loyal users and a clear product-market fit among engineers.

For engineers, the PMF leaderboard looks completely different from everyone else:

ChatGPT

(25.3%),

Cursor

(20.7%),

Claude Code

(17.1%), and

Claude

(13.4%). Three of the top four products theyâ€™d miss are coding-specific tools. Engineers have foundâ€”and want to hold ontoâ€”specialized tools that fit their needs, rather than relying on general-purpose chat interfaces. Cursorâ€™s 20.7% PMF among engineers (vs. 7% to 9% for other roles) shows how deeply it has embedded into coding workflows.

In fact, a handful of role-specific tools are winning their niches.",https://www.lennysnewsletter.com
https://www.lennysnewsletter.com/p/how-to-build-your-pm-second-brain,How to build your PM second brain with ChatGPT,"Use AI to amplify your craft, not replace it",2026-01-02T16:53:39.643134,Amir Klein,"ğŸ‘‹  Hey there, Iâ€™m Lenny. Each week, I tackle reader questions about building product, driving growth, and accelerating your career. For more:

Lennybot

|

Lennyâ€™s Podcast

|

How I AI

|

Lennyâ€™s Reads

|

AI/PM courses

|

Public speaking course

Subscribe

Annual subscribers get

19 premium products for free for one year

:

Lovable, Replit, Gamma, n8n, Bolt, Devin, Wispr Flow, Descript, Linear, PostHog, Superhuman, Granola, Warp, Perplexity, Raycast, Magic Patterns, Mobbin, ChatPRD + Stripe Atlas

(while supplies last).

Subscribe now

.

Someone smarter than me once said,

â€œAI wonâ€™t replace you, but a person using AI better than you might.â€

I believe this is exactly right. Right now, we all need to be building the skills that help us become that person using AI better. Lucky for us, Amir Klein is already that person and has written a guide for the rest of us. Though itâ€™s targeted at product managers, the advice and workflows can be implemented by anyone in any function. Thank you, Amir, for giving us a glimpse into the future and the concrete steps to get there.

For more, follow Amir on

LinkedIn

. You can also listen to this post in convenient podcast form on

Spotify

/

Apple

/

YouTube

.

The first month in my new role at monday.com, I was tasked with building our first AI agent. The goal was to create an AI co-pilot, something users could turn to for insights, explanations, or building complex workflows they wouldnâ€™t know how to create on their own. To build that, I needed a ton of contextâ€”all the internal knowledge, decisions, assumptions, and scattered inputs that shape any product direction. And gathering all of that felt completely overwhelming.

I was drowning.

All that context lives everywhere: Slack channels, Notion pages, Monday boards, decks, Google Docs. Hundreds of tiny fragments I could never quite piece together. I kept running into mental blocks, forgetting what I knew from where, and getting stuck. Instead of trying to keep all of that context in my head like I always had, this time I wanted to try something new. I dumped everything I had into a ChatGPT Project, word-vomited all that was on my mind, and asked if it could help me get started. And boy, did it.

Finally, I felt like I could smell a roadmap on the horizon, a direction was forming, and things began to click. Even better, I felt

somewhat

in control without being stressed about storing everything in my head. I could store it in the AI insteadâ€”a second brain. Instead of all that information overloading my own brain and pulling my attention in a hundred different directions, I could finally focus on the product work I love and need to get right to be successful: understanding the problem, shaping the vision, and building something meaningful.

My good friend Tal taught us

how to think with AI

. Iâ€™m building on Talâ€™s post by showing what happens when AI becomes an extension of your mindâ€”when it carries your context, grows alongside you, and ultimately amplifies what youâ€™re capable of as a PM.

Context is importantâ€”but comes with a heavy mental load

No matter

what

weâ€™re doing, weâ€™re constantly trying to hold way too much information in our heads. I always imagine it like carrying a giant basket filled with random things like eggs, water bottles, watermelons, toy cars, a cactus (I hope youâ€™re picturing a Dr. Seuss scene). And Iâ€™m on the go, so things are rocking all around the basket, and more things keep being added, and then an egg falls and cracks, one of the water bottles starts to spill over, a toy car keeps banging into one of the watermelons . . . basically anxiety in a metaphor.

Thatâ€™s what it feels like trying to hold all the context required to do product work. But the hardest part isnâ€™t just

carrying

it; itâ€™s that none of these pieces arrive neatly fitted together. Context comes in fragments: user feedback, metrics, market changes, internal constraints, past decisions, intuition. As PMs, our job is to assemble those pieces into a clear pictureâ€”shaping the problem, forming the hypothesis, and defining the solution space.

When you can pull that together, you build products that solve real problems so well that customers change habits for them, pay for them, and genuinely feel the impact. But doing that synthesis in your head, and doing it over and over again, can literally feel impossible.

Thatâ€™s where AI comes in. When you feed in all of that context that youâ€™ve been trying to juggle yourself, your ChatGPT Project becomes a second brain that can store the information and synthesize it for you. That means that it can know and retrieve the right piece of data for the right problemâ€”like an instantaneous librarianâ€”and even use what it knows to run analyses and generate recommendations, like an associate PM.

Itâ€™s important to say: using a second brain doesnâ€™t dull your role but actually sharpens it. Your reasoning, product sense, knowledge, and taste are still doing the real work; AI just amplifies them. You canâ€™t outsource judgment or creativity. This isnâ€™t AI thinking instead of youâ€”itâ€™s you thinking with more clarity because all the mental overhead is gone. You still make every decision; the second brain just clears the path from your insight to the output.

Step 1: Create its personality

If someone were to ask you, â€œHey, do you want a really smart, eager, motivated, capable person by your side who knows what you know and is super-enthusiastic to tackle anything you want?â€ youâ€™d probably say yes in a heartbeat. Well, youâ€™re in luck, because thatâ€™s exactly what Projects can be.

At monday.com, I had all of this information (my ever-growing basket) that I was in dire need to create a plan from. So I turned to ChatGPT, opened a Project, and got started. Cue the music.

If youâ€™re trying this yourself, Projects live in ChatGPTâ€™s left sidebar under â€œNew project.â€

(Iâ€™ll share how to do the same thing in Claude and Gemini later on.)

Once youâ€™re inside, if you approach Projects like a second brain that youâ€™re growing, you want to first make sure it thinks in the way you think. In other words, you need to build its personality. Each Project has an instructions section where you can first define this â€œpersonalityâ€ in plain language.

A really awesome way of doing this is with the help of ChatGPT (of course). Open a new chat and describe what you want. For example:

Iâ€™m a Monday PM working on AI agents. Iâ€™m building a ChatGPT Project to be my thought partner, something thatâ€™ll work with me on my initiatives, something thatâ€™ll know how to challenge me in all the right places, push back on areas that feel weak, and creatively think of alternatives with me. This Projectâ€™s â€œpersonalityâ€ has to be sharp, smart, fun, and not always agreeing with everything I come up with. It also needs to be a pro at product managementâ€”this includes product sense and product execution, with a strong sense for product taste and delight. Can you help me write the instructions for this project? :) Cheers!

Itâ€™ll output instructions for what you want:

Tweak what you want or just copy the whole thing as is into the instructions page, and your second brain is now eagerly waiting for you to feed it information!

Step 2: Feed it information

Now comes the fun and legitimately relieving part: feeding it all that context that you are barely holding onto. Go to â€œAdd filesâ€ and just dump it in.

I think the biggest â€œwhoaâ€ moment for me is realizing that

everything

is essentially text. The classics, like PRDs and docs, are a given, but decks, websites, Excel/CSV sheets, dashboards, and Slack channels all contain text too. They just need to be exported into PDFs and then you can upload them into the files section too.

Once you see everything as text, you start to understand how much context you can actually give your second brain. Hereâ€™s what that looks like in practice:

Iâ€™ll scroll through a massive Slack channel thatâ€™s gotten impossible to navigate. Iâ€™ll export it or, if thatâ€™s not possible, Iâ€™ll copy and paste the entire channelâ€™s contents into a doc and save it as a PDF. Then I drop it into the Project. Now ChatGPT knows what has already been discussed, what decisions were made, and what issues keep resurfacing.

When I need it to understand our productâ€™s capabilities, instead of rambling on to it about how my product works, I go to our support or docs site, hit Command + P, and save the entire page as a PDF to drop in. That way, whenever I mention a feature, my second brain already knows how it works.

I do the same with research data like transcripts, interviews, surveys, CSVs. Everything becomes fuel. Each file adds depth to the brain.

In the case of the first initiative I led at monday.com, I started with a few decks that different colleagues of mine had made, downloaded PDFs of monday.com documentation pages explaining how specific things work, and added a bunch of CSV files containing Reddit threads of conversations thousands of people had about monday.com in relation to AI and our competitors. This was enough to get the ball rolling and start formulating a plan. The beauty of Projects is that youâ€™ll start creating new artifacts from it. Whatever it is youâ€™re creatingâ€”whether it be PRDs, overview docs, or strategy decksâ€”once finished, you can take those, put them in a doc, download them, and feed them back into your second brain. Each new thread you open will be up to date with you on your work. It becomes a living thing.

This is what my Project ended up containing after hundreds of threads:

Step 3: Let it cook

Thereâ€™s no one specific thing to use Projects for. As your second brain becomes more and more knowledgeable about your work, you can lean on it for everything that you donâ€™t want to do but needs to get done. For example:

1. Sign-up forms

I needed to create a sign-up form for users to get early access to the agent we were building. This is a classic case of something that seems pretty easy at first but ends up making you bang your head against a wall. How should I phrase what I want to ask? How do I make the output from the form clear and purposeful for me without making filling out the form exhausting for users? Your second brain can now swoop in to save the day as it holds all the context on your initiative. In this case, I asked:

Iâ€™m sending out a form to users to sign up for a waitlist for our first agent. I want to put 2-3 questions on the form which gauges their expectation to ensure weâ€™re aligned on what weâ€™re building and to receive another level of verification around the pain point. These questions should be concise, and the userâ€™s answer will be open-ended (free text).

My sign-up form looked like this:

2. Prototypes",https://www.lennysnewsletter.com
https://www.lennysnewsletter.com/p/how-to-spot-a-top-1-startup-early,How to spot a top 1% startup early,Three key lessons from people who picked multiple iconic companies before they were obvious,2026-01-02T16:53:41.179190,Terrence Rohan,"ğŸ‘‹  Hey there, Iâ€™m Lenny. Each week, I tackle reader questions about building product, driving growth, and accelerating your career. For more:

Lennybot

|

Lennyâ€™s Podcast

|

How I AI

|

Lennyâ€™s Reads

|

AI/PM courses

|

Public speaking course

Subscribe

Annual subscribers get a

free year of 19 premium products

:

Lovable, Replit, Gamma, n8n, Bolt, Devin, PostHog, Linear, Wispr Flow, Descript, Superhuman, Granola, Warp, Perplexity, Raycast, Magic Patterns, Mobbin, ChatPRD + Stripe Atlas

(while supplies last).

Subscribe now

.

After reading the early draft of this post, my approach to investing and giving career advice immediately shifted. Thank you, Bob, Cristina, Soleio, Rasmus, and Sean, for sharing your incredible insights with us ğŸ™

For more from

Terrence Rohan

(i.e. one of my absolute favorite seed investors), find him on

X

and

LinkedIn

. Also, donâ€™t miss his previous beloved guest post:

Raising a seed round 101

.

P.S. You can listen to this post in convenient podcast form:

Spotify

/

Apple

/

YouTube

.

There are endless posts and podcasts about how investors pick startups. But Lenny and I were curious about a quiet class of employees who seem just as good asâ€”if not better thanâ€”the most famous VCs at spotting generational companies before they blow up.

How do these rare folks keep joining world-changing companies before most of the world even notices them?

To find out, we interviewed five people whose resumes include some of Silicon Valleyâ€™s most remarkable companies:

Palantir

,

OpenAI

,

Facebook

,

Stripe

,

Linear

,

Figma

,

Notion

,

Slack

,

Box

,

Spotify

, and

Dropbox

.

Each joined at least two of these companies earlyâ€”an extraordinary feat, especially since they committed as full-time employees, not diversified investors. Their â€œhit rateâ€ is phenomenal.

We were curious: What did they see? How did they choose? Are there lessons to take from their experiences?

Across their stories, we saw three distinct factors that mattered most.

Though originally written for job seekers, these insights apply much more broadlyâ€”for founders, investors, or anyone trying to recognize greatness early.

Hereâ€™s what to look for.

1. Ambition bordering on â€œludicrousâ€

This was the most novel takeaway for us: One of the clearest markers of a future generational company, according to our interviewees, is

ambition

. It came up again and again.

Bob pointed out that â€œ

both Palantir and OpenAI were considered ludicrous when the companies were first started.

Joining Palantir in particular seemed like a very risky proposition. But actually it wasnâ€™t risky at all. If the company failed, at worst, Iâ€™d wasted a year of my life and would have to go back to my PhD. But if the company succeeded, it would be life-changing.â€

Soleio â€œwas surprised by the

ferocity

and

ambition

of the early Facebook team. They all seemed too smart to be working on â€˜social networkingâ€™ but had a lofty idea for where the internet was ultimately headed and how Facebook might completely reshape it.â€

Sean added, â€œIf a companyâ€™s thesis is marked by

extraordinary

ambition

, itâ€™s probably worth paying attention.â€

â€œThe key tell,â€ Bob said, â€œwas always the

ambition of the goal.

â€

Rasmus explained, â€œThe logic here is simple: If everyone says, â€˜Yes, thatâ€™s clearly a great idea, and you have direct competitors on day one, you are definitely late to the game. Even if you excel and go above and beyond expectations, the chance of making a meaningful difference in this world is small-ish. However, if someone has sailed across the sea of exploration, waded through the bog of research, and is still going on about an idea, thereâ€™s a small chance that they are ahead of the rest of us and see something Iâ€™ve yet to see.â€

How to judge ambition

What are signs that the founderâ€™s ambition is big enough?

The founder sounds a bit crazy.

Sean shared, â€œFor Meter, to start from scratch to make your own hardware across many platforms . . . it really indicated that this company is taking a huge swing. Or theyâ€™re just

crazy

. Along those same lines, when I met Dylan Field [CEO and co-founder of Figma] on a bus to a developer conference in 2013, he was just starting Figma. I remember talking to him about what he was building, what he wanted to do with it, and how it seemed

completely insane

to do in the browser. It was clear Dylan was not going to stop, no matter what, and we know how that has gone.â€

People laugh at them.

Rasmus recalled, â€œSpotify, a little group of â€˜nobodiesâ€™ in Sweden, said, â€˜Letâ€™s build a catalog of all the music in the world and give everyone access.â€™ People

laughed at us

â€”I mean that literally, as in record-label officials

laughing at Daniel

[Ek, CEO and co-founder], asking them to give us a chance. The key here was that businesspeople thought it was a bad, bonkers idea while friends thought it sounded like a perfect future.â€

No one has ever attempted this idea before.

Rasmus added, â€œIt may be a bit of a clichÃ© at this point, but

if

no one else is trying to do what a company is trying to change in this world, then there might be something interesting going on

,

especially if itâ€™s ambitious.â€

To close, in the words of Bob, â€œAt one point at Palantir, [co-founder and CEO Alex] Karp said, â€˜

We want Palantir to be the most important company in the world

, not the most valuable one. But if itâ€™s really important, itâ€™s going to be valuable too.â€™â€

2. Founders, founders, founders

Building on the above insight, every interviewee emphasized the same point: the founders matter above all else. Not as one variable among manyâ€”this was

the

variable.

Cristina (early Stripe, Notion, Linear) said it outright: â€œ

The founders (and early team)â€”nothing matters more than this to me.

Iâ€™m going to work hard, and I want to win, but I want to do it with people whom I want to see win too. When I joined Stripe, I joined more because I thought the people were special. I had more conviction about the company itself later.â€

Sean (early Slack, Box, Meter) echoed her: â€œ

Quality (and authenticity) of founders

have always been the most important variable to me.â€

Rasmus (early Spotify, Figma, Dropbox) distilled it even further: â€œ

People and mission.

Who and why (not as much â€˜howâ€™).â€

Bob (early Palantir, OpenAI) added, â€œThe common pattern was an incredibly

ambitious goal combined with a credible team.â€

Thereâ€™s that ambition again.

How to judge founders

Joining a company with great founders is easy to say and hard to do. What exactly makes a founder great? Many people call out intelligence, and we have a whole section highlighting the importance of ambition. But our interviewees mentioned some less obvious traits.

1.

Ability to adapt

Bob pointed out that â€œboth Palantir and OpenAI started with an unworkable initial strategy that perhaps the world was correct to mock. But both iterated over time to something that worked.

Itâ€™s more important to be able to learn quickly than to have a good strategy.

â€

Cristina shared the same sentiment: â€œItâ€™s so difficult to build a company, and thereâ€™s so much you need to learn going from one stage to the next and scaling.

Founders who truly love to learn and look at company-building as a learning experience are quite predictive of whether founders will build durable, special companies.

I remember saying early on at Stripe that Patrick and John [Collison, co-founders] didnâ€™t just want to build a great product; they wanted to build a great company. And in looking back, I remember their apartment had books piled to the ceilingâ€”they knew so much about so many different topics, and they always sought advice from others. They had a learning mindset, and I think they still do.â€

Sean described this as â€œ

rate of change

.â€

Soleio called it â€œ

clock speed

,â€ adding, â€œThe common thread Iâ€™ve observed with all of these companies is how fast they operated and how extraordinarily capable their early employees were. Are they building their ideas in real time or does it take months to see their visions crystallize in software?â€

2.

Ferocity

Rasmus looked for â€œclear

passion

from the people who are â€˜the company.â€™ Passionate and interesting people have been a core aspect of my professional life.â€

Cristina said she â€œloved Ivanâ€™s [Zhao, founder and CEO of Notion]

intensity

â€ and has always been drawn to â€œintensity plus intelligence.â€

And as youâ€™ll recall from above, Soleio â€œwas surprised by the

ferocity

and

ambition

of the early Facebook team.â€

3.

Founder-market fit

Sean always asked himself, â€œDo these people seem like theyâ€™re doing what theyâ€™re meant to be doing, and is there no question that no one else can do it as well as them?â€

Bob was optimistic about Palantirâ€™s chances because

â€œ

it was founded to take the ideas behind the analysis software that solved fraud at PayPal and apply them to the intelligence community.â€

To close, in the words of Cristina: â€œIf the three most important things in real estate are location, location, location,

the three most important things in startups are people, people, people

.â€

3. Judging todayâ€™s product is a trap

Youâ€™d think that for legendary product companies like Stripe, Figma, Slack, Notion, and Facebook, you could tell how special they were going to be by how good their early product was. It turns out this way of thinking is a trap.

Soleio said that when he first logged in to Facebook, â€œ

I remember being disappointed

. The version their team had described was light-years ahead of what I saw that day.â€ Likewise, Figma was

more prototype than product

the day Dylan laid out his vision to me for building a collaborative design platform.â€

Cristina had a similar perspective: â€œMany of the companies Iâ€™ve joined were developer products or products that were meant for teams, so I couldnâ€™t truly try the product myself, as Iâ€™m not a developer or didnâ€™t have a team use case for it. So in general,

I discount my own thoughts about a product in those cases.

â€

Sean told us that

â€œin the earliest days of Slack, it was rough around the edges

.

To quote Stewart [Butterfield, co-founder],

it was a giant piece of shit

.

The bulk of the vision was there in that beta period from 2013 to 2014, but still awaiting refinement.â€

Rasmus rightly pointed out, â€œ

Almost every product Iâ€™ve worked on started out as one thing but was something quite different at the time of consumer success.

Spotify was going to be a video streaming platform and Figma a meme generator.â€ So even if the product is great, it may have to radically change anyway.

How to judge the product",https://www.lennysnewsletter.com
https://www.lennysnewsletter.com/p/a-year-free-of-posthog-16500-value,"A year free of PostHog ($16,500 value): The all-in-one analytics, experimentation, feature flag, surveys, session replay, error tracking, data warehouse, LLM analytics platform","A deal you wonâ€™t find anywhere elseâ€”bringing the total value of the Product Pass to over $25,000",2026-01-02T16:53:42.977117,Lenny Rachitsky,"ğŸ‘‹  Hey there, Iâ€™m Lenny. Each week, I tackle reader questions about building product, driving growth, and accelerating your career. For more:

Lennybot

|

Lennyâ€™s Podcast

|

How I AI

|

Lennyâ€™s Reads

|

Fav AI/PM courses

|

Fav public speaking course

Subscribe

This Giving Tuesday, Iâ€™m partnering again with

GiveDirectly

and over two dozen other Substack writers to send cash to every family across three villages in rural Rwanda. If weâ€™re successful, over

800 families

in extreme poverty will each receive $1,100, no strings attached, to spend on what they need most.

All donations today, December 2, 2025, will also be 1.5x matched

(while funds last).

Please donate ğŸ‘‡

Give now and get matched

Iâ€™m excited to welcome

PostHog

to the

Lennyâ€™s Newsletter Product Pass

ğŸ‰

All annual Lennyâ€™s Newsletter subscribers now get one full yearâ€™s worth of PostHog Scale

and

2x the free-tier credits, a $16,500 value.

This deal is available to

all

annual subscribers, and brings the total value of the

Product Pass

to

over $25,000

. Simply absurd.

Subscribe

This is the first time PostHog has ever offered a deal like this, and you wonâ€™t find it anywhere else. Like all Product Pass deals, you must be a new PostHog customer to take advantage of the deal. See

Product Pass terms here

.

PostHog helps you build successful products by giving you literally every piece of SaaS that you need:

analytics

,

experimentation

,

feature flags

,

session replay

,

error tracking

,

data warehouse

,

LLM analytics,

surveys

, and

more

â€”all in one platform. In the words of the founders, â€œWe keep adding new capabilities at a pace thatâ€™s borderline unreasonable.â€

The all-in-one platform strategy that PostHog is executing is really interesting to me. I rarely see it done this well, because itâ€™s hard to do. But if you pull it offâ€”like they areâ€”you can build a generational business. Being able to follow an issue from a session recording, to its impact in analytics, to shipping a fix as a feature flag, to testing a variant, to collecting feedback with surveysâ€”thatâ€™s the holy grail.

Side note: I stupidly had a chance to invest in PostHog early on and passed because I didnâ€™t think this strategy would work ğŸ¤¦.  Iâ€™m just happy to get to work with them on this collaboration now.

Oh, they also just launched

PostHog AI

, which integrates with all of the products and lets you ask questions about your data, magically create dashboards, build surveys  using natural language, summarize session replays, and more.

More how-to videos here

.

In addition to all the wonderful products PostHog offers, thereâ€™s so much to love about how the PostHog team operates:

Their

website

is ğŸ¤Œ

Their

story

is inspiring.

Their

vibes

are hilarious.

Their

company blog

is one of the most genuinely useful and high-signal-to-noise company blogs out there. A model for how to do content marketing. Some examples:

Iâ€™ve consistently underestimated how important communication is as a CEO

How we got our first 1,000 users

How to do startup sales with no experience

Job interview questions engineers should ask (but donâ€™t)

How not to be boring

Over 300,000 companies use PostHog, including 65% of every YC batch, Lovable, Supabase, ElevenLabs, and even Y Combinator itself.

Twice the free credits gives you tons of room to scale (90% of their users never exceed their already-generous free tier), and the Scale add-on gives you all the features you need to grow: priority support, white labeling, unlimited projects, HIPAA BAA, SAML, SSO + advanced permissions, audit logs, extended session replay retention, and more. If youâ€™re already a subscriber,

redeem your free account here

. Otherwise ğŸ‘‡

Subscribe

Increasingly, your Lennyâ€™s Newsletter subscription is becoming your product builderâ€™s starter pack:

all the tools, guidance, and community you need to build, launch, and grow a world-class product (and career). For just $200-$350/year.

Subscribe

Have a fulfilling and productive week ğŸ™

If youâ€™re finding this newsletter valuable, share it with a friend, and consider subscribing if you havenâ€™t already. There are

group discounts

,

gift options

, and

referral bonuses

available.

Subscribe

Sincerely,

Lenny ğŸ‘‹",https://www.lennysnewsletter.com
https://www.lennysnewsletter.com/p/a-holiday-gift-guide-for-tech-people,A holiday gift guide for tech people with taste ğŸ¤Œ,No slop here,2026-01-02T16:53:46.040253,Lenny Rachitsky,"ğŸ‘‹  Hey there, Iâ€™m Lenny. Each week, I tackle reader questions about building product, driving growth, and accelerating your career. For more:

Lennybot

|

Lennyâ€™s Podcast

|

How I AI

|

Lennyâ€™s Reads

|

My fav AI/PM courses

|

My fav public speaking course

Subscribe

Become an annual subscriber and get an unbelievable deal:

A full free year of 17+ premium products, including

Devin, Lovable, Replit, Bolt, n8n, Wispr Flow, Descript, Linear, Gamma, Superhuman, Granola, Warp, Perplexity, Raycast, Magic Patterns, Mobbin, ChatPRD, and Stripe Atlas

(while supplies last).

Learn more

.

For the fifth consecutive year, Iâ€™m excited to present my holiday gift guide.

The theme this year is taste ğŸ¤Œ:

high-quality non-obvious stuff. No slop. Iâ€™ve included more high-end and handmade products this year than in previous years, but there are also still plenty of affordable items.

As always, nothing below contains affiliate links, Iâ€™m not an investor in any of these companies, and Iâ€™m not benefiting from recommending any of these products (except one, which will be clear). This list is simply a collection of things I love and think youâ€™ll love too.

For more ideas, check out my previous gift guides from

2024

,

2023

(for new parents),

2022

, and

2021

.

Did I miss something absolutely awesome? Share it in the comments ğŸ™

Leave a comment

Enjoy!

For the home

Handcrafted Modern maple donut bowl

:

My wife got us this, and I was like â€œwtf is it?â€ But then I realized how much a beautiful wood piece adds to your space.

The artist makes lots of other great stuff, too

.

Coyuchi 100% organic cotton sheets

:

Pricey, but oh so nice. You spend so many hours in your sheets, imho it makes sense to splurge here.

Deadline candle

:

â€œTransforms your most dreaded moments into sacred ritual.â€

Matic

:

Wirecutter

,

Wired

,

Gizmodo

,

The Verge

, and

ZDNet

have all reviewed it and said itâ€™s the best smart vacuum ever made. I agree. We use ours constantly. They are raising prices on December 2 (from $1,095 to $1,245â€”tariffs ğŸ« ), so ordering before then will save you $150. They also offered me a deal to share with this community: use

this link

to get a free 6-to-9-month supply of replacement HEPA bags. Their team sent me a unit to try out when they were just getting started, and I loved it so much Iâ€™ve been telling everyone about it.

Living Tea seasonal tea club

:

A friend got this for me, and itâ€™s so nice. They include background on each tea, how to best steep them, and all kinds of extras that make the experience feel special.

These stainless-steel infusers

are really handy (especially if youâ€™re wary of tea bags these days). We also love

Leaves and Flowers

and

Rishi

teas.

Koshi chimes

:

The sound of these is unique and delightful. â€œSimply move the chime gently holding it by its cord to produce a crystalline, relaxing sound that mesmerizes and calms.â€

Plant-based desk lamp

:

You may have noticed one of these in my podcast background. We bought it at the West Coast Craft fair in SF, but they also sell them at

SFMOMA

. I donâ€™t recommend the ones with legs, though, as they tend to come off.

These are also cute

. As are

these classic Noguchi lamps

.

Angels Horn vinyl record player

:

Includes built-in speakers (and Bluetooth!), so itâ€™s a really easy, versatile, and affordable way to enjoy records. Weâ€™ve had this for years and love it.

Graf Lantz Bierfilzl merino wool round coasters

:

I canâ€™t get enough of these around the house. Lots of color options, and the

square ones

are nice too.

Vera salt

:

Microplastic-free salt. After learning that I was

half-man, half-plastic

and also that sea salt is one of the most microplastic-laden foods one can consume (

thanks, Huberman

ğŸ¥´), we discovered this product (through

Superpower

marketplace), and itâ€™s now our go-to salt. Pretty, healthy, and yummy. Pairs well with this plastic-free

Klean Kanteen

water bottle (which is not as fun as other bottles but easy to clean).

For parents and kids

Charts for Babies

by Michelle Rial

:

MY WIFE HAS A NEW BOOK COMING OUT. I know Iâ€™m biased, but itâ€™s genius. Itâ€™s also cute as hell, will warm your heart, and will incept your kids into liking STEM. Itâ€™s coming out in April, and

you can pre-order it here

. If you havenâ€™t seen

her first book

, that one makes a great gift too (for adults).

Emerging Artist beanie for babies

:

We gifted this to our nephew when he was younger, and itâ€™s never not funny.

Kidsâ€™

guitar

or

piano

:

They come with guides to help you learn, and they are really high-quality. My wife and I even practice on it while weâ€™re playing with our son because weâ€™re both learning piano.

Yoto mini player

:

A neat screen-free audio player for kids. Our son loves his so much. He rotates between the Beatles, Raffi, Queen, and Spanish

cards

while he walks around dancing, listening to it (rather than staring at a screen). How can you not love that? Sir Paul McCartney and Chan Zuckerberg Ventures are investors, to encourage more screen-free entertainment options for kids. Many people love the

Tonies

as an alternative.

Personalized ring

(

or necklace

):

For your BFF, mom, lover, or just get it for yourself. As a fun twist, make it the joke name you call your kid/pet.

Personalized

pennant

or

blanket

:

Is there anything kids/parents/humans love more than their name on stuff?

Coyuchi handstitched organic toddler blanket

:

Nontoxic naps for your little one.

Kidsâ€™ Adirondack chair

:

Extra-cute if you place it near your adult-sized chair on the porch.

Organic floppy brown teddy bear

:

This teddy bear is relatively expensive, but itâ€™s a rare non-polyester option for something your kid will be snuggling all night. Itâ€™s made in Germany from certified organic cotton and pure organic wool.

They have other animals too

.

John Klassen prints

:

There are also

originals

, but most are sold out.

More prints here

. IYKYK ğŸ”º

For gadget lovers

ModRetro Chromatic

:

A modern remake of the original Game Boy by Palmer Luckey (founder of Anduril and Oculus). It feels great, plays great, and makes a unique gift for gamer friends.

Peak Design tech pouch

:

They also make a

larger version

, and

their toiletry bag

is great.

Aranet CO

2

monitor

:

Did you know high CO

2

impairs your cognition?

Research

shows that brain function decreases by 15% when CO

2

levels are over 1,000 ppm, and by over 50% above 1,400 ppm. Freaky shit. Iâ€™ve got one of these in my office and open a window anytime it gets into the yellow zone.

Belkin MagSafe 3-in-1 charger

:

I saw these at a hotel once, immediately got it, and love it. Better than any other nightstand charger Iâ€™ve tried.

Herschel tech backpack

:

This has been my go-to bag for years.

Sony WH-1000XM5 noise-canceling wireless headphones

:

These have become my workhorse headphones. Grab

this cute stand

if you use headphones at your desk. My wife prefers the

Bose QuietComfort

because, as the name suggests, they are comfy.

iPhone case with built-in stand

:

I learned about this from

Kevin Roseâ€™s newsletter

. Thereâ€™s also

an Anker version

, but I like this one better.

Hooga amber book light

:

I originally got it to change the babyâ€™s diaper during the night while keeping the lights dim and super warm. Now use it nightly while reading in bed. Lasts forever, charges easily, and has multiple brightness settings.

Amazon Kindle Colorsoft

:

Did you know they make Kindles in color? Thanks for the tip,

Laura Fingal-Surma

.

AirPods

:

For all your friends who have â€œAirPods #1,â€ â€œAirPods #2,â€ . . . â€œAirPods #5â€ in their Bluetooth devices list.

For your stressed founder friend or family member

Function Health

or

Superpower

membership:

Make sure they arenâ€™t pushing themselves

too

far.

Spirit Rock gift certificate

:

I did a 10-day silent retreat there a number of years back, and it was life-changing.

Check their upcoming retreats here

.

The Anti-Anxiety Notebook

: â€œ

Utilizing Cognitive Behavioral Therapy, a rigorously-tested and widely-used treatment, youâ€™ll develop the skills to identify, challenge, and change unhelpful thought patterns so you can feel better.â€

Japanese Hot Spring Minerals bath salts

:

ğŸ§˜ğŸ§˜ğŸ§˜

Leaves and Flowers Sleep Tea

:

ğŸ’¤ğŸ’¤ğŸ’¤

Rice wax Japanese candles

:  ğŸ•¯ï¸ğŸ•¯ï¸ğŸ•¯ï¸

Candle holder

:

Goes well with

beeswax tea lights

, and creates a really nice light effect.

Runninâ€™ Down a Dream: How to Thrive in a Career You Actually Love

by Bill Gurley

:

I havenâ€™t read this yet, but Iâ€™m 100% sure itâ€™ll change many peopleâ€™s lives, considering the impact

his talk

on the same subject has had on people I know.

Squishy stress ball set

:

Iâ€™ve got fidgety hands, so I often need something to play with. You may see me holding these during my podcast chats sometimes. â€œFree from BPAs, phthalates, and latex.â€

â€œThings will work outâ€ keychain

:

A comforting reminder to have around.

For the PM who just cashed out their OpenAI stock

Donate

to

GiveDirectly

,

St. Jude Research Hospital

,

SF-Marin Food Bank

, or your favorite charity.

Evil eye charm

or

stoneware

100-pound Parmesan wheel

:

â€œCrafted by hand from the milk of cows grazing high in the alpine meadows of Benedello di Pavullo, these monumental Parmesan wheels are aged by Giorgio Cravero, whose family has been perfecting the art of affineurship in Bra, Italy, since 1855. Simply putâ€”the best Parmigiano Reggiano that exists.â€",https://www.lennysnewsletter.com
https://www.lennysnewsletter.com/p/ecosystem-is-the-next-big-growth,Ecosystem is the next big growth channel,How to stand out in a noisy landscape by leveraging partners who already have access and trust with your target audience,2026-01-02T16:53:49.523830,Emily Kramer,"ğŸ‘‹  Hey there, Iâ€™m Lenny. Each week, I tackle reader questions about building product, driving growth, and accelerating your career. For more:

Lennybot

|

Lennyâ€™s Podcast

|

How I AI

|

Lennyâ€™s Reads

|

AI/PM courses

|

Public speaking course

Subscribe

Annual subscribers get a

free year of 17+ premium products

:

Devin, Lovable, Replit, Bolt, n8n, Wispr Flow, Descript, Linear, Gamma, Superhuman, Granola, Warp, Perplexity, Raycast, Magic Patterns, Mobbin, ChatPRD + Stripe Atlas

(while supplies last).

Subscribe now

.

Iâ€™m always on the lookout for alpha to share with you on what the most successful companies are doing differentlyâ€”so you can do it as well. This post is the epitome of that.

Emily Kramer

has identified a pattern in how top AI (and non-AI) startups are able to break through the noise and grow unlike at any other time in history. Once you see it, you immediately rethink your growth strategy. I did! After reading the first draft of this post, I got a whole new level of understanding for why my

product pass

has been such a success (fun fact: it doubled my growth, and is the most win-win-win idea Iâ€™ve ever concocted).

Below, Emily shares how you can implement this strategy yourself, along with dozens of real-world examples of how top companies are executing it. For more insights from Emily, check out her excellent

newsletter

, and find her on

LinkedIn

and

X

.

P.S. You can listen to this post in convenient podcast form:

Spotify

/

Apple

/

YouTube

.

Youâ€™ve built something great. Maybe even shipped it faster than you thought possible, thanks to AI. But hereâ€™s the catch: everyone else has that same speed advantage.

So you turn to distribution. But hereâ€™s the second catch: the B2B channels youâ€™ve long relied on for growth are increasingly becoming less effective, for that same reason.

Inbound:

Search traffic is declining due to LLMs, and social is flooded with derivative content.

Outbound:

With AI SDRs and easier access to contact data, everyoneâ€™s inboxes are overflowing.

Product virality:

Weâ€™re overwhelmed with new products (thanks, AI!), so with endless choice, virality is much harder to achieve.

Events:

While people crave authentic connection in the AI and hybrid-work era, the explosion of trade shows, micro-conferences, and webinars of mixed quality has led to event fatigue.

Lifecycle emails:

With inboxes saturated with cold outbound, even lifecycle emails focused on driving engagement and retention get tuned out.

So in this new competitive, AI-driven landscape, how do you get your great product noticed?

The answer is

ecosystem

. Instead of going directly to your prospects, go through intermediaries who already have access and trust with your audience.

Tom Orbach

, the director of growth marketing at Wiz, said it best: â€œWhy start at zero when you can start at 10,000?â€

Youâ€™re likely familiar with the individual tactics within an ecosystem strategy: influencer and creator relationships, channel partnerships, developer relations, communities, product integrations, and customer marketing. But the growth unlock doesnâ€™t come from any one of these activities on their own.

Instead, a flywheel emerges when you properly implement this overarching strategy: your ecosystem partners create and distribute content, you amplify and repurpose their efforts, together you drive greater reach and credibility, new customers and partners come on board, and the cycle strengthens with each turn. Now your ecosystem is a powerful extension of your GTM team.

Ecosystem is a force multiplier

and

differentiator

When you look at the fastest-growing companies today, you probably see AI-first companies known for their lean teams and great products. But I see something else: companies that wouldnâ€™t have achieved these growth trajectories without their ecosystems.

Supabase

The Postgres development platform and database became the default backend for vibe-coding products like Lovable. As these companies grew, so did Supabase. In addition to these â€œintegrationâ€ partners, Supabaseâ€™s open source community shared docs, tutorials, and videos, further driving growth and building credibility.

â†’

Supabase went from 1 million to over 4.5 million

developers in under a year, which would not have been possible without two sets of ecosystem players: the open source community and the awareness from integration partners.

â€œCommunity and virality arenâ€™t â€˜intangible.â€™ Supabase shows how to turn organic pull (GitHub stars, YC adoption, vibe-coding buzz) from developers into structured funnels, lifecycle paths, and monetization.â€ â€”

Aaron Cort, operating partner at Craft Ventures, investors in Supabase

Clay

Clay grows through close partnerships with â€œGTM engineerâ€ power usersâ€”consultants, agencies, and in-house operators. These Clay creators make videos and tutorials that drive adoption and awareness.

â†’ Without this creator ecosystem showing whatâ€™s possible, Clayâ€™s flexibility could have been a liability instead of an advantage.

Lovable

Lovable helps builders quickly create apps that are naturally shareableâ€”a classic driver of viral growth (think â€œBuilt with Webflowâ€ or â€œMade in Typeformâ€ badges). But Lovable accelerates this virality through community-driven initiatives like

hackathons

and their

Discord, which just surpassed 100,000 members

.

â†’ Lovableâ€™s product is excellent, but there are many other vibe-coding products out there. Lovableâ€™s investment in building a brand alongside their creators has catapulted them above the rest.

â€œFor years, the marketing playbook was predictable, but not anymore. All the attention has shifted to the creator economy.

The goal is no longer to produce everything in-house but to empower creators to generate content about you

,

and then repurpose it across paid and organic.â€ â€”

Elena Verna, head of growth at Lovable

Gamma

Gamma built an ecosystem of micro-influencers across TikTok, Instagram, X, and LinkedIn to show off presentations built with Gammaâ€™s AI design tool. They paid them not just to post but offered a bonus for virality. They also extensively catalogued successful hooks and formats to make this scale.

â†’ Gamma attributes

over half of their growth

to influencers, and founder Grant Lee breaks down

the details in this post

.

Vercel

Vercelâ€™s huge developer community builds on

Next.js

(which Vercel leads work on), and those projects naturally scale into Vercel enterprise deals.

â†’ Without the Next.js ecosystem and community contributions, Vercel likely never would have had the credibility or reach to move upmarket.

ElevenLabs

ElevenLabs

turned users into contributors through its Voice Library. Voice actors upload voices and earn money when theyâ€™re used, which attracts more creators. These voices also power viral content like â€œ

AI presidents

â€ on YouTube and TikTok. Viral hits drive press coverage, too.

â†’

ElevenLabs hit $100 million ARR with just 50 employees

. Without its paid creator ecosystem (over

$1 million to creators in 2025

), the product itself would have fewer voices and it wouldnâ€™t have spread as quickly.

Youâ€™ll see ecosystem flywheels behind nearly every fast-growing AI startup, from

Baseten

â€™s strategic partnerships with cloud providers, to

Midjourney

embedding its product in Discord from day one, to

Anthropic

building

MCP

to make it easier for developers to connect tools and data to Claude and other LLMs.

Why ecosystem works

I initially became obsessed with the power of ecosystems firsthand at

Carta

in 2018, where lawyers as channel partners drove massive growth, and at Astro in 2017, where building for the Slack platform led to

Slack acquiring Astro, in its largest acquisition at the time

. Back then, these plays felt like outliers. But today, this strategy is essential. An ecosystem is fast becoming the rule for growth, not an exception.

Hereâ€™s why:

1. Your ecosystem makes every channel more effective

The best reason to invest in an ecosystem is that, when done well, it makes every channel better by serving as both a distribution path and a source of content.

Ecosystem partners fuel inbound, add value and credibility to your outbound messages, accelerate product virality, make events more enticing, and ground lifecycle marketing in real use cases.

Example: HubSpot partnered with a bunch of marketing influencers

to promote their new framework, â€œLoop Marketing,â€

when they announced it at their Inbound conference this year. This combination spurred lots of discussion, driving a bunch of organic Linkedin posts too. All of these efforts came together to make it feel like the Loop Marketing framework was everywhere that week (at least if you were a B2B marketer).

2. Your ecosystem knows how to speak to your audience (maybe better than you do)

AI has made it easy to pump out content, making it harder than ever for people to know who and what to trust. This means they are turning to voices they already value and respect (like Substack writers!). Ecosystem players like these already know how to speak to your audience. Your brand simply canâ€™t build that trust overnightâ€”if everâ€”without these partners.

You see this especially on LinkedIn, where individual people (employees, customers, or partners) perform much better than brandsâ€”eight times better from employees vs. brands, according to

LinkedIn

.

Example: Vanta may be best known for its clever

play-on-words billboards

, but youâ€™ve likely also seen them sponsoring business Substacks (like Lennyâ€™s) or podcasts. Since their product centers on trust, itâ€™s smart to partner with high-trust creators who their (compliance-minded) audience already knows. Itâ€™s clear that a significant share of their paid spend now goes not just to billboards but to creators too. Fun fact: Vanta is one of Lennyâ€™s top 3 podcast sponsors.

3. GTM teams are smaller now and need leverage

AI was supposed to make GTM teams more efficientâ€”and in some ways it has. But

shrinking

GTM teams are scrambling to get efficiency through AI while also finding creative, compelling ways to differentiate.

Your ecosystem is an accelerant for a lean team, like bringing on a contracted team of expert marketers. Working with partners, customers, and communities may not be as turnkey as paid search, for instance, but with the right workflows (and a boost from AI), the ROI can be extremely high.

Example:

Navattic

built a team of marketing advisors, which makes the team seem much larger than it is. They set up long-term or advisory arrangements with micro-influencers who get early access to products, give product feedback, and also regularly post on LinkedIn as part of the relationship.

4. You have a ton of options within an ecosystem strategy

Ecosystems arenâ€™t limited to products you integrate with or creators with big LinkedIn followings. And if one ecosystem partner starts showing diminishing returns, you arenâ€™t out of luck. Thereâ€™s a broad network of individuals and companies that can reach your audience.

If a person or company, no matter the size or reach, has spent time building up the audience you want, you can figure out how to work with them.

Example: If you have a niche audience, you can often find organizations and trade associations to partner with.

Clio

works with state bar associations to reach lawyers, and

Upstart

has long been a preferred partner with the

NAFCU

, a regional credit union trade association.

How to implement an ecosystem strategy

Growth strategies are never one-size-fits-all, but most B2B startups today can grow faster with a well-designed ecosystem strategy. With audience attention fragmented and harder than ever to earnâ€”and startups hitting revenue milestones fasterâ€”you canâ€™t afford

not

to leverage the people, companies, and platforms that already have your audienceâ€™s trust.

Because there are so many types of partners and ways to collaborate, thereâ€™s opportunity here for nearly every company.

The question isnâ€™t

if

you should build an ecosystem strategy, itâ€™s

who

your best partners are,

how

to activate them specifically, and

how

to reshape your team and process to prioritize that work.",https://www.lennysnewsletter.com
https://www.lennysnewsletter.com/p/part-2-of-how-to-get-the-most-out,"Part 2 of how to get the most out of your product passâ€”and welcome, Stripe Atlas, to the bundle!",Tips for taking full advantage of the 17+ free premium products you have access to as a paid subscriber,2026-01-02T16:53:51.271100,Lenny Rachitsky,"ğŸ‘‹  Hey there, Iâ€™m Lenny. Each week, I tackle reader questions about building product, driving growth, and accelerating your career. For more:

Lennybot

|

Lennyâ€™s Podcast

|

How I AI

|

Lennyâ€™s Reads

|

AI/PM courses

|

Public speaking course

Subscribe

First of all, some exciting news:

Starting today,

Insider subscribers

get 40% off

Stripe Atlas

.

I want yâ€™all to build amazing things, and not get bogged down in legal and tax hell. Stripe Atlas magically automates the entire startup incorporation process.

For just a few hundred dollars, they retrieve your EIN, issue founder stock, and file your 83(b) tax elections. Within two business days of registering, youâ€™ll be able to open a bank account, fundraise, and start charging customersâ€”even before your EIN has been issued.

In addition, Atlas startups get world-class company legal documents, the same breadth and caliber youâ€™d get if you spent thousands on a top-tier law firm.

Cursor, Lovable, and Linear were all incorporated through Atlas (along with 80,000 other startups), and itâ€™s the no-brainer first step to making your startup a real thing.

Learn more about Stripe Atlas here

and

get the deal here

.

This brings the total value of the

Lennyâ€™s Newsletter product pass

to $15,546. For just $200-$350/year. Thatâ€™s also what Iâ€™d call a no-brainer.

With this additional puzzle piece, more and more your newsletter subscription becomes your product builderâ€™s starter pack: all the guidance, community, and products you need to build and grow a world-class product (and career). For just $200-$350/year.

And this will only get better. Over the next few months, weâ€™ll be adding several more premium products thatâ€™ll be available to all annual subscribers (not just Insiders).

Subscribe

Next is the second part of my comprehensive

guide

for getting the most out of the product pass products. Below are all the ways that

Lovable

,

Bolt

,

Granola

,

ChatPRD

,

Superhuman

,

Raycast

, and

Perplexity

have changed my life and are transforming peopleâ€™s lives at work and at home. Again, my goal here isnâ€™t to sell you the products (

you already get them for free

!) but to help you get the most value possible out of your paid subscription.

Enjoy.

1.

Lovable

What it is

Not only the fastest-growing company in history ($100 million ARR just eight months after launch, faster than Cursor ğŸ¤¯), Lovable is an AI-powered vibe-coding platform that enables anyone to build apps and websites by chatting with AI. PMs can turn sketches, screenshots, and ideas into live interactive prototypes. Designers can refine the UX inline. Engineers review the code in their GitHub repos. As of last month, Lovable includes cloud and AI features that allow you to build AI-native, full-stack apps with zero configuration or tedious work required.

Why I love it

Whenever Iâ€™m vibe coding something, I try a bunch of the tools in parallel. Lovable has been producing the best stuff for me lately.

What I use it for

A bunch of personal use cases. I built

a YouTube preview tool

,

a YouTube thumbnail downloader

,

a strikethrough text tool for X

, and

a tool to download all of the images inside a Google Doc

. So fun!

How other people are using it

Product teams: turning PRDs or screenshots into live, interactive prototypes to validate ideas

Marketing teams: launching landing pages; turning customer collateral into interactive websites with trackable CTAs

Sales teams: building ROI dashboards that quantify product value

Finance teams: taking FP&A out of spreadsheets and into live, easy-to-use dashboards

Hereâ€™s a fun directory

of what people have built with Lovable.

Learn more

Check out their

website

, and

my chat with Lovable CEO Anton Osika

.

2.

Bolt

What it is

Also one of the fastest-growing companies in history (0 to $40 million ARR in five months ğŸ¤¯), Bolt is an enterprise-grade AI-powered vibe-coding platform that helps PMs, founders, and designers turn ideas into fully functional prototypes using AI and natural languageâ€”no code required. Bolt is used by 72% of Fortune 500 product teams, and even allows you to build using your companyâ€™s native design system.

Why I love it

What makes Bolt unique in my mind is that, unlike other popular vibe-coding platforms, which rely on their own homegrown agents, Bolt integrates with state-of-the-art frontier agents, like Claude Code (with OpenAI Codex and Gemini CLI), so you always get access to the most advanced coding agents, without switching tools. In other words, if you love Claude Code, youâ€™ll love Bolt.

What I use it for

Also a bunch of personal use cases.

A feed of the latest AI news (updated daily)

,

a dial of how stressed I am based on workload that I can share with my wife

,

a simple app to tell me if my son needs a jacket at school today

, and

a tool to learn basic Spanish

.

How other people are using it

Managing and reducing subscription costs

Building video games (and building tools that build them)

Helping people learn better

A lawyer created an app to transform complex legal documents into clear, actionable insights

At a recent hackathon, a solo builder named Adrian created

Tailor Labs

, an AI-powered video editor

Learn more

Check out their

website

, and

my chat with Bolt CEO Eric Simons

.

3.

Granola

What it is

An AI notepad designed for people in lots of Zoom/Teams/Meet meetings (i.e. everyone). As you take notes, it silently transcribes everything said in the background. Once the meeting is complete, Granola enhances your notes, summarizing what everyone said, and makes it easy to share (and search) what was said. Best of all, Granola doesnâ€™t join your calls. Instead, it works behind the scenes using your laptop audio. Thereâ€™s an iPhone app for in-person meetings and phone calls too.

Why I love it

I was blown away by both the UX and the quality of the notes it generates. You can use it to quickly share meeting notes with colleagues, recall decisions youâ€™ve made in past meetings, analyze themes over time, and even

notice when youâ€™re avoiding conflict (with help from Claude Code)

. The product is so thoughtfully designed in every way, and everyone I introduce it to gets hooked. Itâ€™s quickly become the obvious-tool-to-use for AI meeting note-taking.

What I use it for

Every single meeting I have.

How other people are using it

Linearâ€™s CEO uses Granola to stay focused

Intercomâ€™s founder uses it to stay on top of all his meetings

Getting coached in the style of a top CEO coach

Searching for something someone said in any of your meetings

For stressful non-work situations

Learn more

Check out their

website

and their cool new

Recipes feature

.

4.

ChatPRD

What it is

An AI platform built specifically for product managers. It writes product specs, standardizes your documents across your team, and integrates with the best-in-class PM tools, including other product-pass products like Lovable, Bolt, Gamma, Magic Patterns, and Linear. Itâ€™s especially great at taking raw ideas, turning them into clear PRDs, and then sharing that with your favorite vibe-coding tool.

Why I love it

Thereâ€™s no other AI tool out there thatâ€™s this laser-focused on both making PMsâ€™ lives better

and

helping everyone else on the team play PM.

Itâ€™s also

a first-class citizen in Linear

now, meaning you can use it to improve task descriptions, break down work into actionable sub-issues, and get product feedback (by simply mentioning @chatprd in comments).

Neat

.

What I use it for

Since Iâ€™m not building products with teams these days, I donâ€™t use ChatPRD much, but Iâ€™ve heard nothing but rave reviews of it from readers. And having worked closely with

Claire Vo

on

her pod

, the

Lenny & Friends Summit

, and all kinds of other side projects, I know anything she builds will be top-notch.

How other people are using it

Turning messy notes into â€œa banger PRD in no timeâ€

Automating your software development lifecycle

by connecting it to Linear

Planning features for agentic coding tools

Using Lennybot and ChatPRD together

Helping you improve your structured thinking

Learn more

Check out the

website

and

this rad templates library

.

5.

Superhuman

What it is

The most beautifully designed and thoughtfully crafted email productivity tool, specifically focused on helping you get to inbox zero. Superhumanâ€™s research shows that the average user gets through their inbox

twice

as fast (vs. using Gmail or Outlook) and saves four hours a week.

Why I love it

If you want to see what a well-crafted consumer product looks like, study every pixel of Superhuman.

What I use it for

Email!

How other people are using it

Drafting automatic replies with AI

(

in your

own voice and tone)

Collaborating on emails with your colleagues

Creating team snippets of commonly used phrases, paragraphs, or even whole emails

Booking meetings and sharing your availability

Automatically filtering and organizing incoming email, smartly

Learn more

Check out their

website

, and

my podcast chat with founder Rahul Vohra

.

6.

Raycast",https://www.lennysnewsletter.com
https://www.lennysnewsletter.com/p/a-builders-guide-to-living-a-long,A builderâ€™s guide to living a long and healthy life,For something a little different,2026-01-02T16:53:53.039075,Justin Mares,"ğŸ‘‹  Hey there, Iâ€™m Lenny. Each week, I tackle reader questions about building product, driving growth, and accelerating your career. For more:

Lennybot

|

Lennyâ€™s Podcast

|

How I AI

|

Lennyâ€™s Reads

|

My favorite AI and PM courses

|

My favorite public speaking course

Annual subscribers get a

free year

of 17 premium products:

Devin, Lovable, Replit, Bolt, n8n, Wispr Flow, Descript, Linear, Gamma, Superhuman, Granola, Warp, Perplexity, Raycast, Magic Patterns, Mobbin, and ChatPRD

(while supplies last).

Subscribe now

.

Subscribe

Since turning 40, Iâ€™ve felt a lot less invincible. For the first time in my life, my annual bloodwork results werenâ€™t 100% healthy. A few months ago, I broke my pinkie toe on the edge of a wall. Last month, I sprained my wrist trying to adjust an A/C unit. Last week, I banged up my knee after slipping on a staircase.

Then I took a toxins screen and learned Iâ€™m half-man, half-plastic:

So over the past year Iâ€™ve started getting serious about my health: tracking my nutrition, exercising, experimenting with supplements, focusing on my sleep, etc. Along that journey, I came across

Justin Mares

.

Unlike the health folks everyone knowsâ€”Andrew Huberman, Rhonda Patrick, Bryan Johnsonâ€”

Justin

is a full-time builder. He co-founded

Kettle & Fire

,

Perfect Keto

,

Surely

, and now

Truemed

, and heâ€™s basically a health nerd who spends hundreds of hours researching what to buy for himself and his family and shares what he learns in blog posts and tweets. When I decide what products to buy and which brands to trust, Iâ€™ve found myself

constantly

referencing his recommendations, more than anyone elseâ€™s.

But Iâ€™ve always wanted more. So I pitched Justin on putting together a comprehensive and specific list of his favorite products and brandsâ€”the safest, least toxic, and highest-quality products he himself buys for clothing, sleep, food, toxin mitigation, and more. This is what youâ€™ll find below.

And as a bonus, at the end of the post, weâ€™ve compiled a handy bullet-list summary of every product and brand mentioned.

Note, except for a couple that Justin explicitly calls out, neither Justin nor I is an investor in any of these companies, and there are no affiliate deals involved. This is just a well-meaning post to help you all live a long, healthy life . . . so that you can keep building beloved products for many years to come.

A huge thank-you to Justin for spending endless hours compiling this list. I will be referencing it

frequently

.

For more from Justin, check out his

newsletter

and

Truemed

(buy health products at a steep discount using your HSA). Theyâ€™re also

hiring a marketer

!

You can also listen to this post in convenient podcast form:

Spotify

/

Apple

/

YouTube

.

In 2011, I was working for my first company while also taking a full college course load. As a committed Tim Ferriss acolyte, I read a blog post about polyphasic sleep. For a whole month, I slept 3.5 hours a night and took a 20-minute nap every four hours, even if that meant leaving class or coming late to a fraternity party.

This was insane. After a month of polyphasic sleepâ€”and two crash-outs where I slept 18 hours in a row to recoverâ€”I realized it wasnâ€™t for me. But what

was

for me, and what Iâ€™ve mostly stuck to in the 14 years since, is the paleo diet, which I decided to do after reading about it in the same blog post.

I went paleo for two weeks my junior year of college (yes, I was the weird kid who would pass on the pizza and beer). And during this two-week experiment, my acne disappeared, I got leaner, slept better, and just had more energy.

As it turns out, what you put in your body affects how you feel! If youâ€™re not healthy, youâ€™re not able to perform at your best, period. This is why so many in Silicon Valley are so obsessed with sleep tracking, diet, and other tools to help improve performance at work.

Unfortunately, the West is in the

throes of a chronic disease crisis

. Americans today are the sickest population of humans to ever exist. Nearly 50% of adults have prediabetes or diabetes, 73% are obese or overweight, and the richest American men

live 15 years longer

than the poorest, almost entirely due to chronic disease burden. Diabetes alone has an economic cost of around

$106 billion

.

In this post, I wonâ€™t cover traditional health tips (exercise, get eight hours of sleep, etc.), though theyâ€™re crucial. Instead, I want to cover some of the lesser-known health gotchas that consistently sap your health. Iâ€™ve spent the past decade of my career building companies in this world: first with

Kettle & Fire

(now a $100M+ annual revenue brand) and now with

Truemed

. And Iâ€™ve spent hundreds (thousands?) of hours searching for products that are the most effective both at driving real health outcomes and at avoiding many of the common toxins, microplastics, and other unhealthy compounds we encounter on a daily basis. Here are my most recommended products:

Sleep

Getting good sleep is among the most critical things you can do to improve your health and set yourself up for peak work performance. Thereâ€™s a ton of literature on why sleep is so important, but for the purposes of this post Iâ€™ll assume youâ€™re already a believer.

In my view, the 80/20 of sleep health boils down to three things:

Exercise during the day

Get morning sunlight

Optimize your sleep setup (no external light, quiet, low CO

2

, cold) as much as possible

Beyond the basics, Iâ€™ve found a few products that really help me get my best sleep. I love my

Eight Sleep

, and have found their pod works

incredibly

well to help me both increase my deep sleep and avoid wake-ups. For those who have trouble falling and staying asleep, magnesium

has been shown

to radically improve sleep quality. I think the

Momentous magnesium L-threonate

supplement is the best out there. If youâ€™re really, really struggling to get to sleep, I have a few friends who swear by the peptide Epitalon . . . though itâ€™s technically a research chemical, so I canâ€™t point you anywhere to get it. Ask your doctor. Or buy it illegally online, but beware of sourcing!

Studies have found that

mattresses can be a source of harmful chemicals

(especially foam ones), as they emit phthalates, benzophenones, and other compounds that have been linked to asthma, developmental issues, and reproductive harm. Sadly, many of these chemicals have not been thoroughly tested (thanks to

the insane way we regulate chemicals in the U.S.

), but the few studies we do have are concerning.

You may or may not know this, but our current approach to chemical regulation relies on industries to run their own tests and make their own assertions that the novel, never-before-seen chemicals theyâ€™re inventing and putting in our products are safe and sound.

When a pharma company invents a drug that humans take, it goes through a rigorous FDA approval process that takes a decade and is tested for safety. When a chemical company invents a compound that your body canâ€™t break down and ends up in food, the water supply, and our bodies, regulators require . . . nearly nothing. How does this make sense?

It gets worse! Thatâ€™s our approach to

new

chemicals. But for the chemicals there

have

been safety testing forâ€”the thousands that we know are harmful at some dose but where the dose makes the poisonâ€”how are exposure limits determined?

To determine exposure limits, scientists follow a relatively simple process. First, theyâ€™ll expose a lab animal, often a rat, to small amounts of said chemical. Theyâ€™ll then sit and watch for behavioral changes: Does the rat slow down, look like itâ€™s in pain, or start wobbling a bit?

They then continue to increase the dosing of the suspected toxin until they observe behavioral changes in the rat. At that point, they take the dose they exposed the rat to and multiply it by a â€œsafety factorâ€ (usually 100) to account for the difference in weight between rats and humans. And thatâ€™s it. Anything below the dose number thatâ€™s spit out is now considered . . . safe!

To ensure youâ€™re not inhaling toxic compounds that mattresses often off-gas (like VOCs, benzene, and formaldehyde) while sleeping, I highly recommend the

Woolshire pillow

, the

Avocado

mattress and pillows, and

Coyuchi sheets

. Besides Avocado, I have heard good things about

Essentia

and

Naturepedic

. And lastly, if you have $80,000 burning a hole in your pocket, Iâ€™m sure you canâ€™t go wrong buying

HÃ¤stens

(though I wouldnâ€™t know). Apparently, Drake bought eight of these mattresses for his Toronto home.

Clothing

Your skin is the largest organ in your body, and it consistently absorbs compounds and chemicals that itâ€™s exposed to. Yet we rarely think about whatâ€™s in the clothes we spend nearly all our time wearing.

Even though many clothes use dyes and chemicals known to be hazardous, thereâ€™s no such thing as a nutrition label, or even a disclaimer, as to what might be in the clothes you put on your body.

Turning fibers into clothing is a complicated and chemically intensive process. Thousands of chemicals are used to make clothing, some 10% of which have been shown to

disrupt the immune system, increase cancer risk, mess with hormones, and create reproductive issues

. Even worse, the fun, stretchy performance fabrics you love to wear probably have the highest level of chemical treatment, and heavily utilize PFAS (forever chemicals) and PBDEs (chemicals used as flame retardants). This could be one reason why PFAS are

found in 97%

of Americansâ€™ blood.

Itâ€™s unfortunately not just the chemicals, dyes, and other additives that make your clothing choices important for your health. Extremely common materials like polyester can

lower sperm count

, yet itâ€™s by far the most popular material used in underwear. These fabrics, and the dyes and chemicals on them, can fairly easily enter the body, especially in areas where your skin is most permeable: your nether regions, feet, and armpits.

I strongly recommend

at minimum

wearing toxin-free, organic cotton or linen socks and underwear. My favorite brands are

Industry of All Nations

and

Pact

for underwear (

Nads

as runner-up), and Pact for socks.

For shirts, I like

Paka

â€™s stuff, especially their T-shirts, as they use a blend of 85% organic cotton and 15% alpaca fibers.

Faherty

has a surprisingly good selection of organic tees that Iâ€™m into. And lastlyâ€”though I probably shouldnâ€™t share my secretsâ€”

Wax London

is my go-to fashionable brand that has tons of organic options.

Food

This is the area I feel most passionate about. The Standard American Diet (SAD) is terrible. Right now, many Americans get 55% of their calories from

ultraprocessed foods

. The typical American diet is often bereft of nutrients, is riddled with toxins, and has created the sickest population of humans to ever exist on this planet ğŸ‡ºğŸ‡².

In large part, this is because we have a food system that is uniquely permissive in what it allows in our foods. Worse, international companies (like Mondelez) actually have

American versions of the same product

, where the American version is simply more processed!

When it comes to sourcing foods, the biggest things to optimize for are (1) nutrient density and (2) toxin avoidance. The most nutrient-dense foods are sourced more locally, which inherently means eating seasonally. The farther your food travels, the less nutrient-dense it is. One study found that after nine days of travel, spinach was

90% less nutrient-dense

than it was at harvest. Here in Austin, I am fortunate to live near an amazing grocery store (

Radius

), but many major cities have excellent farmers markets or grocery stores to buy local.

Beyond nutrient density, itâ€™s important to mitigate common toxins like pesticides (glyphosate, atrazine), phthalates, microplastics (though itâ€™s nearly impossible), and PFAS. That means buying organic where possibleâ€”especially the most-sprayed fruits and vegetables, called the â€œ

dirty dozen

â€â€”and, again, buying whole, unprocessed foods that require fewer chemicals to ensure some amount of shelf life. There are certifications (like glyphosate-free) that are fairly good signs that youâ€™re buying from a brand that cares about sourcing, but these are few and far between.

Beyond those principles, there are a few foods from companies I feel comfortable recommending:

Protein bars/snacks:

I absolutely love these

Maui Nui meat sticks

. Maui Nui has the best-sourced meat I am aware of, anywhere in the world. (We partnered with them at Kettle & Fire.) They actually hunt and harvest the animals they make the jerky from, and it shows in the taste and nutrient density of their meat sticks. It should be no surprise, but when animals live a longer time, forage, exercise, and are outside for their natural lifetimes, well, they tend to be more nutrient-dense and protein-rich: exactly what Maui Nui products are.

I prefer these sticks to a protein bar most of the time. But if Iâ€™m running low, I absolutely love the

Jacob bar

in a pinch. Just be sure to

avoid David bar

. ğŸ˜‰

Bone broth:

You probably knew I would say this, but I drink at least a carton of

Kettle & Fire

bone broth each day. We made the first

wild-harvested bone broth

(with Maui Nui), which is my go-to. But all our flavors are good!

Beef:

People should be eating far more organ meats, because theyâ€™re such a great source of key amino acids and other nutrients. I love Force of Nature and their

ancestral blend

, which makes it easy to get organ meats in your diet while also buying some of the highest-quality regenerative beef out there.

White Oak Pastures

also does an incredible job from a sourcing standpoint.

Coffee:

This is a tough one to source locally. I buy a lot of coffee from the

Groundwork

team, who have worked hard to build one of the only regenerative coffee supply chains in the world. Bonus points if you want to stir in some

$400 manuka honey

(which is apparently lightly psychoactive, though I havenâ€™t personally tried it).

At this point in my life, there are very few national food brands that I support, as I tend to prefer buying locally and seasonally where possible. Scaling a food business while maintaining high standards is extremely difficult, and, to be frank, most companies out there donâ€™t do a good job. Even Whole Foodsâ€™s beef

has a high degree of phthalates!

Supplements

Many supplements are a waste of time and money. Much (most?) of what youâ€™ll find on Amazon is probably fake, overhyped, or doesnâ€™t have enough effective ingredients to do anything. A recent consumer test found that 4 of 6 creatine gummy brands

contained no creatine

at all!

For almost everyone, getting core nutrients and amino acids from whole food sources is

far

superior to getting them from supplements. That said, many folks are still missing some of the building blocks the body requires to thrive. Thatâ€™s where supplements can come in, to . . .

supplement

the diet. Some I like and recommend:

Protein powder:

Protein is key, and most Americans donâ€™t get anywhere close to the recommended 0.36 grams per pound of body weight. For those looking to increase protein intake,

Equip

is probably my favorite protein powder, with the

Momentous whey

a close second. I would recommend avoiding plant protein powders, as they (1) basically donâ€™t work and are

not bioavailable

, and (2) are often made from pea or soybean protein, which is among the most pesticide-sprayed crops out there. For example, a

Mamavation test

found Orgainâ€™s organic pea protein powder had glyphosate levels roughly

1,000 times

higher than recommended.

Beyond protein for building muscle, Americans also criminally underconsume collagen and gelatin, both of which are key building blocks for healthy joints, skin, hair, and gut. I have four or five servings of collagen per week (on top of my daily bone broth) and recommend the collagen from Equip, Momentous,

Lineage

, and a company I started years ago,

Perfect Keto

(if you want the extra boost that comes with coconut fats).

Creatine:

Creatine is a naturally occurring amino acid stored in muscles and the brain that helps with energy creation. Itâ€™s been safely used by bodybuilders since time immemorial and is increasingly associated with a host of

mental performance benefits

. Pretty much the only creatine I would buy is

Momentous

, as they are one of the only non-Chinese-sourced brands out there.

Nattokinase:

This is a new compound that I am very bullish on. Heart disease is the leading cause of death in the U.S., and 70% to 80% of heart attacks are caused by atherosclerotic plaque. Certain dosages of nattokinase (at least 10,000 fibrinolytic units) were found to be more effective than statins at reducing arterial plaque, a significant contributor to

heart

disease

. For the many folks whose labs show high cholesterol or concerning ApoB/Lp(a) markers, I think taking nattokinase preventively is a good idea. Iâ€™ve been taking

Toku

, which has the clinically effective dose plus vitamin K2 (I also invested in the company). A friend

found recently

that his ApoB dropped 16% and his LDL came down 20% after 60 days of taking Toku. Josh (the founder of Levels)

found similarly

!

B vitamins and MTHFR:

If youâ€™re like meâ€”and 30% of other peopleâ€”you may have some variety of the MTHFR mutation. This mutation impairs the bodyâ€™s ability to detox and can lead to higher levels of homocysteine, an amino acid in the blood that, at elevated levels, is linked to increased risks of heart disease, blood clots, strokes, and cognitive decline. I take this

B complex

from Thorne daily to help with methylation (basically, how your body clears toxins) and give my body support to detox and function the way it should.

Organ supplementation:

As Iâ€™ve mentioned, organ meats and connective tissue contain key amino acids and nutrients that are not found in muscle meats like steak or chicken breast. Unfortunately, most Americans primarily eat cuts of muscle meats, and thus donâ€™t get many key amino acids in their diets. I get it: eating straight kidney and liver can be gross! Many people (aka me) prefer supplementing with organs in capsule form. My favorite

organ supplement

is from Maui Nui (a partner of my company Kettle & Fire), which I believe sources the best and most ethical animal protein in the world.

Outside of those daily staples, I have a whole host of supplements Iâ€™ll take for specific life events, often from brands like Momentous or Thorne, which both have good sourcing practices. For example, when Iâ€™m sick, I follow the

Huberman protocol

and take 600 to 900 milligrams of N-acetylcysteine (NAC), along with 100 mg of zinc, per day for three to five days. I take

AHCC

and

astaxanthin

to help my body fight off the infection. I also mega-dose vitamin C and try to spend at least 30 minutes in the sauna.

Though this regimen seems to help, nothing is better than not getting sick at all. Thatâ€™s why whenever I fly, I use

Viraldine

before getting on the plane to prevent catching anything (including feelings).

Toxin mitigation

In my view, environmental toxins are the trans fats of our generation.

PFAS

, phthalates, endocrine-disrupting chemicals, pesticides, and heavy metals are everywhere. Theyâ€™re in your clothing, detergents, soap, shampoo, paint, furniture, water, and air. As of this writing, 92% of Americans have measurable phthalates in their body and 97% have PFAS in their blood. These chemicals have been shown to affect

testosterone levels

,

anxiety

,

cancer risk

, and even

sperm count

.

Some researchers believe

that they even have a role to play in the fact that girls are hitting puberty one to two years sooner than they were 40 years ago.

In large part, we are here because the FDA has totally dropped the ball on regulating new chemicals. The European Union, on the other hand, takes a â€œdo no harmâ€ approach and assumes that new chemicals may cause harm. So they regulate them! The EU currently bans more than 1,300 chemicals and compounds that the U.S. allows.

The FDA takes the opposite approach: Sure, humans have never before been exposed to these classes of chemicals, but whateverâ€”letâ€™s assume theyâ€™re safe. The U.S. has over 40,000 chemicals allowed in our food, water, and environment, with minimal pushback from the FDA. Between this and the 50 years it took the FDA to act on

the whole trans fats thing

(and then, only after multiple lawsuits were brought against them), I think itâ€™s safe to say that the FDAâ€™s approach to chemical regulation has lots of room for improvement.

Anyway, back to toxins. Theyâ€™re everywhere (in breast milk, our blood, our urine), have tremendous health impacts, and are highly likely causing many of the chronic conditions that plague Americans today.

Detoxing your environment is critically important but almost impossible to do completely. Thatâ€™s why I think an 80/20 approach makes a lot of sense here: cut out toxins in the things youâ€™re regularly exposed to, and use detox tools (like sauna, or fancy new blood cleaning approaches like

Inuspheresis

/

Proxima

) to attack the remaining 20%.

For many, this means cleaning up your home environment, your office environment, and your kitchen. Most of what Iâ€™ve learned about this I learned after having the

Lightwork

(think: functional medicine doctor for your house) team do a thorough test and review of my home.

Here are the things Iâ€™d buy to mitigate toxin exposure in all of those spaces:

Air purifiers:

Most microplastics are inhaled, not ingested via water or food. On the high end, I like the

Jaspr

, but if youâ€™re looking for a great everyday purifier, the

PuroAir

is also quite good.",https://www.lennysnewsletter.com
https://www.lennysnewsletter.com/p/everyone-should-be-using-claude-code,Everyone should be using Claude Code more,"How to get started, and 50 ways non-technical people are using Claude Code in their work and life",2026-01-02T16:53:54.590225,Lenny Rachitsky,"ğŸ‘‹  Hey there, Iâ€™m Lenny. Each week, I tackle reader questions about building product, driving growth, and accelerating your career. For more:

Lennybot

|

Lennyâ€™s Podcast

|

How I AI

|

Lennyâ€™s Reads

|

My favorite AI and PM courses

|

My favorite public speaking course

Subscribe

Annual subscribers get a

free year

of 17 premium products:

Devin, Lovable, Replit, Bolt, n8n, Wispr Flow, Descript, Linear, Gamma, Superhuman, Granola, Warp, Perplexity, Raycast, Magic Patterns, Mobbin, and ChatPRD

(while supplies last).

Subscribe now

.

Ever since

my chat with Dan Shipper

, I couldnâ€™t stop thinking about his hot take that

Claude Code

was the most underrated AI tool for non-technical people. A few weeks ago, I finally started playing around with it, and holy sh*t, weâ€™ve all been sleeping on Claude Code.

The key is to forget that itâ€™s called Claude Code and instead think of it as Claude

Local

or Claude

Agent

.

Itâ€™s essentially a super-intelligent AI running locally, able to do stuff directly on your computerâ€”from organizing your files and folders to enhancing image quality, brainstorming domain names, summarizing customer calls, creating Linear tickets, and, as youâ€™ll see below,

so much more

.

Since itâ€™s running locally, it can handle huge files, run much longer than the cloud-based Claude/ChatGPT/Gemini chatbots, and itâ€™s fast and versatile. Claude Code is basically Claude with even more powers.

To inspire your own ideas, Iâ€™ve collected 50 of my favorite and most creative ways non-technical people are using Claude Code in their work and life. This list includes my own favorite use cases, and the best examples yâ€™all shared with me on

X

and

LinkedIn

of how you use Claude Code.

A huge thank-you to the more than 500 of you who shared your stories. ğŸ™

But first, letâ€™s install Claude Code on your computer

Open your Terminal app

On a Mac, press

Command (âŒ˜) + Space

, type

â€œTerminalâ€

, and hit

Return

In Windows, press

Windows key + R

, type

â€œwtâ€

, and press

Enter

Install Claude Code

On a Mac, run this command:

curl -fsSL https://claude.ai/install.sh | bash

In Windows, run this command:

irm https://claude.ai/install.ps1 | iex

Launch Claude Code:

claude

If you run into any trouble, just ask your favorite chatbot for help. Or better yet, install

Warp

(

free with your newsletter subscription

!), which replaces your local terminal app and automagically solves any issues you encounter trying to install stuff like Claude Code. Thatâ€™s how I solved the problems I ran into, and I highly recommend you do the same.

Five ways Iâ€™ve been using Claude Code this month

1. Clearing space on my computer

Prompt: â€œ

How can I clear some storage on my computer?

â€ I then discuss my options.

2. Improving the image quality of screenshots

Prompt: â€œ

Improve the image quality of [filename]

â€. I used this many times for the screenshots below.

3. Downloading YouTube videos

Prompt: â€œ

Download this YouTube video: [URL]

â€. Then I ignored all the warnings ğŸ¤«

4. Downloading all of the images embedded inside a Google Doc

Prompt: â€œ

Download all of the images in high-res from this Google Doc: [URL]â€.

This paired well with item #2.

5. Picking a random raffle winner from a Google Sheet of submissions

Prompt: â€œ

Pick a random row from this Google Sheet to select a winner for a giveaway.

â€ I used this for a recent Sora 2 giveaway in our subscriber Slack community.

50 creative ways non-technical people are using Claude Code

Out of the over 500 ideas you shared with me on

X

and

LinkedIn

, here are my favorites (with screenshots):

1. Brainstorming domain names, from

Ben Aiad

â€œJust describe your project, and itâ€™ll suggest creative options across multiple TLDs (.com, .io, .dev, etc.) while verifying whatâ€™s actually available to register.â€

2. Finding high-quality leads, from

Jeff Lindquist

â€œI literally just typed: look at what Iâ€™m building and identify the top 5 companies in my area that would be good for a pilot for this. Then I go to LinkedIn and message them. If itâ€™s not clear, I do this in the source directory of the code of my app so the first thing it does is figure out what it is that Iâ€™m building.â€

3. Same as above, but instead, scraping GitHub repos, from

Sergei Zotov

â€œMy product masks sensitive data in code assistant queries. So Claude Code proposed the idea to find potential leads in the GitHub repos, by searching for the actual sensitive values in them (and whether in the repo we see some evidence of using coding agents). This was actually geniusâ€”not only does it filter out a lot of companies, but it also provides instant value to the lead. Hereâ€™s what it came up with: repos list, priority score, even LinkedIn URL.â€

4. Noticing when youâ€™re avoiding conflict, from

Dan Shipper

â€œI download all of my meeting recordings, put them in a folder, and ask Claude Code to tell me all of the times Iâ€™ve subtly avoided conflict.â€

Pro tip: Use

Granola

(

first year free

!), and ask Claude Code how to download your meeting notes into a folder.

5. Figuring out why your computer is running slow, from

Anthony Roux

â€œI sometimes use Claude Code for system diagnostics when my Mac slows down.

I use it to check load averages, memory pressure, disk space, stuck processes, and swap activity, then it dives deeper to find whatâ€™s actually causing issues. It can calculate cache sizes, check Docker usage, find Time Machine snapshots eating space, etc.

It is usually faster and more user-friendly than running all the commands and trying to extract the right numbers myself. It can explain what the analyses mean and why they matter, and suggests fixes with the actual commands while assessing the risk of running each of them.â€

6. Cleaning up messy invoice files, from

Martin Merschroth

â€œI use Claude Code to sort my invoices for taxes. It reads each file in a messy folder, renames it to â€˜YYYY-MM-DD Vendor - Invoice - ProductOrService.pdfâ€™, and moves it into the right folder.â€

7. Organizing files and folders across your computer, from

Justin Dielmann

â€œFor me, staying organized is a huge chore. The cognitive load of figuring out where to store files and keeping everything clean and up to date was insane. My hack: I run Claude Code from my home directory and use it as my personal organization assistant. Iâ€™ll ask it things like:

â€˜Find duplicate files and help me decide which to keepâ€™

â€˜Organize these downloads into proper foldersâ€™

â€˜Review my directory structure and suggest improvementsâ€™

â€˜Find old files I probably donâ€™t need anymoreâ€™

Itâ€™s like having a thoughtful assistant who actually understands context and can make smart decisions about file organization. Game changer for reducing mental clutter.â€

8. Building a slide for your child, from

John Conneely

â€œI built my own DIY subagent last week to help me build a slide tower for my son ğŸ˜€â€

The finished product:

9. Organizing scattered thoughts, from

Helen Lee Kupp

â€œIâ€™m a mom who voice-records ideas during morning stroller walks, not a developer. The terminal interface? Overwhelming at first. The word â€˜Codeâ€™ . . . but what if I donâ€™t have a â€˜coding projectâ€™? After 3 weeks of struggling to organize my scattered thoughts, I tried it anyway. And discovered something wild: Claude Code isnâ€™t about coding at all. Itâ€™s about having an AI that manages your entire processâ€”whatever the goal might be.

How I use it:

â†’ Fed it rambling voice notes from stroller walks

â†’ It organized them into coherent research themes

â†’ Wrote a full article in

my

exact voice (pulled from my own examples!)

â†’ Created LinkedIn versions automatically (this post is one of them!)

â†’ Everything saved and ready to publish (including grabbing a screenshot of the template repository that Iâ€™m adding here!)â€ [

Hereâ€™s the repo

]

10. Writing a job description, from

Justin Bleuel

â€œI used it to generate a full job description, hiring plan, interview plan, and rubric for a new role at Clay.

I made a folder with internal Notions of our hiring material for PMs, added similar role JDs from other companies, then in planning mode asked to generate the same collateral for this new role.â€

11. Synthesizing transcripts of calls with customers, from

Derek DeHart

â€œMy current Claude Code jam is synthesizing transcripts of calls with customers to compile evidence that supports or invalidates a running tally of assumptions/requirements/hypotheses/whatever. Given MCPs to interact with other tools in our productivity stackâ€”Fireflies, Linear, Notion, etc.â€”itâ€™s become my hub for ongoing product research and development.â€

12. Improving your writing, from

Teresa Torres

â€œI now write all of my content with Claude Code in VS Code. We iterate on an outline, it helps me improve the hook, it conducts research for me and adds citations to my outline, and it reviews and gives feedback on each section as I write. It has completely changed the way I write.â€ [

Much more on Teresaâ€™s process here

]

13. Working with audio files, from

Dan Heller

â€œIâ€™m working with multiple audio files. I use Claude Code to manipulate them, convert the sample rates, rename them, and translate them from Portuguese to English.â€

14. Creating â€œself-drivingâ€ documentation, from

James Pember

â€œThe most interesting use case weâ€™re playing with is something I call â€˜self-driving documentation.â€™ Basically, how can we give an Agent the responsibility of figuring out how/where our documentation can be better and more comprehensive. Weâ€™ve been experimenting with using Claude Code together with

Playwright

to automatically explore our software independently, identify knowledge gaps in our documentation, and then create those changes itself. Very promising!â€ [

More here

]

15. Creating a self-improving feedback loop, from

Gang Rui

â€œI created a slash command that analyzes my journal entries + Git commits (for the past 7 days; usually I use this weekly), spots gaps between what I said vs. did, and suggests system improvements. Like having a COO that learns from my patterns.â€

16. Getting inspiration from competitorsâ€™ ads, from

Sumant Subrahmanya

â€œExtract ads from competitors to find the problem, use case, or copy/asset thatâ€™s working for them, and then repurpose it for my ads. Claude Code built out these scripts that would screen-grab all ads running on the ad library, and itâ€™s super cool to watch it navigate to the browser and grab all screenshots in an almost â€˜agenticâ€™ way.â€

17. Automatically creating changelogs, from

Manik Aggarwal

â€œI use Claude Code to create user-facing changelogs. I ask Claude Code to scan all commits from a specific time period, then pull in my changelog guidelines. It drafts a clean, structured changelog that usually needs few quick edits. What earlier took me hours is now down to 10â€“15 minutes. Most of our changelog output is created with Claude Code, with a final polish done by me.â€

18. Building presentations, from

Hank Yeomans

â€œAll my slide work is done in Claude Code as HTML, then imported into PPT.

First, I make a couple of slides that I need with some simple but well-prompted data, while also prompting to create them as html pages. Otherwise they might get created as SVGs or â€˜slop AI images.â€™ I work with Claude Code making any adjustments.

When I get it the way I want it, I ask Claude Code to make a template that I can use to ensure all future slides added are the same format, branding, look, and feel.

This template becomes an .md file (Iâ€™ve put a snippet of that file below). With html you can be extremely specificâ€”â€˜change this wording to say thatâ€™â€”and then you can use an MCP to view the â€˜slidesâ€™ to show what you mean as well for greater context. With this, I have added several more slides as well as changed others using the template to keep things the same. Yes, hallucinations happen, that has to be accepted going in. Someone who knows what they want and can articulate what they want will not have too much trouble.

When I want to put these into PPT, I just do well-thought-out screenshots. But honestly, If Iâ€™m using these for a customer or some presentation, I just open a browser and I have â€˜previousâ€™ and â€˜nextâ€™ buttons built into them so that I can click through. Claude Code creates these as interactive html â€˜slidesâ€™ so they react to the mouse.â€

19. Doing social media research, from

Danny Shmueli",https://www.lennysnewsletter.com
https://www.lennysnewsletter.com/p/a-free-year-of-devin-the-worlds-first,A free year of Devin: the worldâ€™s most advanced autonomous AI software engineer,"Insider subscribers now get $1,350/year of Devin, on top of 15 additional premium products. All the advice, community, and tools you need to build and grow a world-class product (and career).",2026-01-02T16:53:59.004972,Lenny Rachitsky,"ğŸ‘‹  Hey there, Iâ€™m Lenny. Each week, I tackle reader questions about building product, driving growth, and accelerating your career. For more:

Lennybot

|

Lennyâ€™s Podcast

|

How I AI

|

Lennyâ€™s Reads

|

Favorite AI and PM courses

|

Favorite public speaking course

Subscribe

The vision for the Lennyâ€™s Newsletter

Product Pass

is to give you access to the most cutting-edge and important tools and technologies, so that you can experience themâ€”not just read about them. Beginning this week, Iâ€™ll be adding a new premium product to the collection each month. Some will be available to all annual subscribers, while others will be exclusive to

Insiders

(our premium annual tier).

Starting today,

Insider subscribers

get a free year of

Devin

, the worldâ€™s most advanced autonomous AI software engineer.

The team behind Devin has never offered a deal like this before. This partnership is a significant investment from them. With this collab and more, I hope to get more people exposed to the future of how products will be built.

This adds an additional $1,350 in value to your

Insider subscription

(for just $350/year)â€”on top of the more than 15 premium products you already have a year of free access to (worth over $10,000):

Lovable, Replit, Bolt, n8n, Wispr Flow, Descript, Linear, Gamma, Superhuman, Granola, Warp, Perplexity, Raycast, Magic Patterns, Mobbin, and ChatPRD

.

Not to mention this newsletter and a thriving private Slack community.

Subscribe

And thereâ€™s much more in store in the coming months.

What is Devin

Devin is an autonomous AI engineer. You assign it bugs, features, or complex refactors through Slack, Linear, Jira, or its website. It writes the code, then sends you a pull request to review.

Tens of thousands of people are using Devin in production today, including top teams at Ramp, Nubank, Goldman Sachs, Citi, and Microsoft.

Why I love Devin

Itâ€™s the most widely deployed enterprise-ready AI engineer on the market, and every forward-thinking builder I know whoâ€™s used Devin absolutely loves it:

Claire Vo

told me that Devin is the #2 contributor to her six-figure

ChatPRD

business (behind only her, for now) and touches 100% of their PRs, by reviewing code, updating documentation, or writing all of the code.

Hereâ€™s Claireâ€™s interview with Devin co-creator Scott Wu

.

Sahil Lavingia

has been proudly sharing that Devin is

Gumroad

â€™s #1 contributor of code, with over 1,500 merged PRs (averaging 10 per day).

Dan Shipper

said, â€œWorking with Devin is familiar because it feels like adding a few junior engineers to your team. You toss them tasks, and theyâ€™ll get started with enthusiasm . . . Itâ€™s programming leverageâ€”itâ€™s productivity power.â€

Hereâ€™s Danâ€™s chat with Scott Wu

comparing Devin to Claude Code.

Devin ranks #1 in the

Builder Arena

, ahead of Cursor, Lovable, Replit, and Figma Makeâ€”a benchmark based on thousands of people blindly comparing the output of each AIâ€™s product being given the same prompt.

Also,

Scott Wu

(CEO of

Cognition

) is something else:

How you might use Devin

Most people use Devin to take on their bugs and feature requests, but you can work with Devin to:

Scope new roadmap ideas

Update UI and add visual polish to your site

Handle QA on product changes

Keep internal documentation up to date

Take a first pass at data analysis requests

Take on the most tedious items on your backlog

Build SaaS integrations

Send daily summaries of shipped changes

Tackle complex migrations:

Nubank

used Devin to migrate their over-6-million-line ETL monolith over the course of a couple of weeks, which otherwise would have been an 18-month and 1,000-engineer project.

Increase your unit test coverage:

Litera

used Devin to increase their test coverage by 40%, which reduced their regression cycles from three weeks to two days.

Most people begin with Devin as an AI software engineer, handing off well-scoped software engineering tasks. They then expand into QA, product, analytics, and CX.

The goal is to take on all of the lower-level work so that you have more time to focus on figuring out

what

to build and how to get it into the hands of users.

Check out their

website

for more.

How to redeem the deal

As an

Insider

subscriber, you get one free year of Devin Core with 50 ACUs a month, valued at $1,350. This is roughly equivalent to a full-time engineer working for a week straight, each month.

To redeem this deal:

Become an

Insider

member (this is an upgrade from the regular annual membership).

Claim your free code here

.

Additional offer details:

You must be a new customer of Devin

to take advantage of the free year. If youâ€™ve already paid for Devin before (monthly or yearly), you wonâ€™t be able to get a free additional year of that specific product.

Both existing and new subscribers

of Lennyâ€™s Newsletter are eligible for this deal. Existing subscribers can

redeem the offer here

.

Your one free year of the product begins when you redeem the deal on the partnerâ€™s website

,

not when you purchase this newsletter subscription or claim your code.

Enjoy! And much more to come.

Sincerely,

Lenny ğŸ‘‹",https://www.lennysnewsletter.com
https://www.lennysnewsletter.com/p/introducing-the-gain-framework-for,"Introducing the GAIN framework for feedback: an evidence-based approach to giving feedback that people love, appreciate, and act on",A step-by-step guide to crafting feedback youâ€™ll want to give and people will want to get,2026-01-02T16:54:04.245250,Jack Cohen,"ğŸ‘‹  Hey there, Iâ€™m Lenny. Each week, I tackle reader questions about building product, driving growth, and accelerating your career. For more:

Lennybot

|

Lennyâ€™s Podcast

|

How I AI

|

Lennyâ€™s Reads

|

Courses

Annual subscribers get a

free year

of 15+ premium products:

Lovable, Replit, Bolt, n8n, Wispr Flow, Descript, Linear, Gamma, Superhuman, Granola, Warp, Perplexity, Raycast, Magic Patterns, Mobbin, and ChatPRD

(while supplies last).

Subscribe now

.

Subscribe

If feedback is a gift, why is it so hard to receive? And to give.

Itâ€™s because no one taught us how to give feedback.

I was introduced to

Jack Cohen

by (frequent collaborator)

Tal Raviv

, who told me, â€œJack is the single biggest influence on my personal productivity and â€˜inner game.â€™ â€ Over the past 11 years, Jack has coached hundreds of founders, executives, and managers to help them grow themselves and their colleagues as fast as their startups.

Growth hinges on getting honest and ongoing feedback. Through thousands of feedback conversations and hundreds of hours of research, Jackâ€™s developed an evidence-based, immediately applicable framework for giving feedback that he calls

GAIN

. As Jack puts it, â€œGAIN is both a useful acronym and the core of the philosophy.â€ When you tell people what they

stand to gain from the feedback, it suddenly starts feeling like a gift again.

This guide is essentially a how-to for Radical Candor, and the most specific (and easy-to-remember) framework for giving great feedback that youâ€™ll find anywhere. Itâ€™s going to blow your mind. Enjoy.

For more, Jack teaches a highly rated course,

ManagerGPT: The AI Tools and Human Systems to Scale Yourself and Your Team Fast

, focused on helping high-growth leaders build their individual operating systems and interpersonal skills to stop fighting fires (or each other) and start creating what matters most: great products and great teams. For a taste of the course and a demo of both GAIN feedback and the AI tool Jack built to support it, check out his free 30-minute lightning lesson, â€œ

FeedbackGPT: Give Feedback People Thank You For

.â€

You can also listen to this post in convenient podcast form:

Spotify

/

Apple

/

YouTube

.

Over my past 11 years coaching several hundred people from dozens of organizations and teaching thousands more, thereâ€™s one challenge Iâ€™ve seen people struggle with across every level, industry, company stage, personality type, and culture: giving feedback.

Many people come to coaching and workshops already describing themselves as â€œconflict-averse,â€ dreading having difficult conversations and giving direct feedback. A good number arrive fed up with failed attempts to get people to change their behavior, with many missing the irony as they bemoan, â€œI keep giving them the same feedback, and

they

never change. Maybe they just donâ€™t want to.â€

What these folks miss is their own power to catalyze change by shifting the framing and approach of the feedback they give.

Iâ€™ve seen and felt firsthand the difference between the â€œfeedback mastersâ€ and â€œfeedback disasters.â€ After every feedback conversation from one colleague who I had initially respected, I felt deflated. As a result, everything suffered: our product, our relationship, and my overall well-being.

Later, I worked with another colleague whose feedback left me elated every time (actually!). I couldnâ€™t wait for meetings with her because I enjoyed them so much and I knew that her feedback would help even my best work reach another level.

On reflection, I realized that the content of the feedback from these two colleagues was strikingly similar, but the way my second colleague framed and delivered hers lifted me up instead of shutting me down.

That difference answered an age-old question: How on earth do you get someone to change or grow? And how can you do that in ways that strengthen your connection instead of breaking it? By mastering the art (and science) of giving great feedback.

The good news is that this is a learnable skillâ€”and one that is core to my work as an executive coach and organizational behavior consultant.

On a daily basis, I give feedback to clients and help them give feedback to their colleagues, as well as establish effective feedback systems across their teams and organizations.

At my company, we also practice this in almost every meeting, and the growth it elicits is remarkable. One colleague said that after regularly getting and giving feedback using the GAIN framework, he felt â€œmuch more driven by purpose, rather than by fear.â€

So what is the difference between the feedback masters and disasters? How can you shift your framing and approach to ensure that the feedback you offer inspires people to take action and keep growing? My experience, informed by a wealth of research from multiple fields, shows that effective feedback shares several key qualities:

I call this feedback approach the

GAIN framework

. GAIN is both a useful acronym for remembering these essential ingredients in effective feedback and the core of the philosophy.

It is profoundly more effective and inspiring to frame feedback based on the experiences or results we want to move toward (the

gain

) instead of focusing only on what we want to move away from (the

pain

).

We can often do this in just a few short sentences. Say an engineering lead is frustrated with a PMâ€™s sloppy handoffs and the resulting delays. The engineering leadâ€™s feedback to the PM might sound like: â€œI want us to reduce handoff friction so we can ship faster. The last handoff meeting we had, you didnâ€™t share any background customer research, which I failed to flag at the time. Lacking that context led to multiple rounds of engineers pinging you with questions instead of making empowered decisions on their own. Can you include that next time?â€

The challenge is that feedback doesnâ€™t typically show up in our brains in such a crisply articulated, ready-for-prime-time state. It might start as â€œProduct just threw this over the wall; they donâ€™t care about our time.â€ Or â€œTheyâ€™re lazy; they always do this!â€ It takes some translating to get our raw reactions into a form thatâ€™s more likely to have the desired impact (the â€œgainâ€). That form must include:

What both parties ultimately care about (the

G

oal)

What needs to change to get there (the

A

ctions)

Why (the

I

mpacts)

Who

will do

what

by

when

to start making progress (

N

ext actions)

Thinking through the GAIN elements is a powerful way to understand and channel those raw reactions constructively. This often starts by using your feelings as a clarifying signal pointing to what you really care about, which should inform your Goal for the conversation. In cases where your feelings are particularly strong or unwieldy, preparation thatâ€™s even more focused on your â€œinner gameâ€ (a topic for a different post) becomes an essential step in leveraging the GAIN framework.

Especially with complex or sensitive topics, including each of these GAIN elements in the conversationâ€”often, but not always, in orderâ€”leads to greater receptivity and, consequently, more behavior change and more satisfying results.

Once my clients understand how GAIN works, they see the ways they themselves have been unintentionally eliciting defensiveness and resistance and how, very practically, they can flip that on its head.

One founder said, â€œGAIN helped me have structured conversations that have proven to be successful especially with â€˜fragileâ€™ team members. Iâ€™ve done this successfully with more than five people in just the past two weeks.â€

Another client, startup veteran

Brian Martin

, described his experience this way: â€œAfter I gave feedback once or twice in this new way and saw people actually listen and change and drive the business results or the behavior that I was trying to help them toward, I was like, â€˜This is great. I can do this.â€™ It created a positive feedback loop, and now itâ€™s something I just do that I think people really appreciate about me.â€

As Brian notes, people start to realize that the function of feedback isnâ€™t just for behavior change. When shared with skill and engaged with vulnerability, a substantive feedback conversation builds immense trust, meaningful connection, and deep gratitude. (For example,

many

guests

on Lennyâ€™s Podcast and beyond rave about that exact impact that Sheryl Sandbergâ€™s feedback had on them and their relationship with her.)

This post will help you build confidence, fluidity, and effectiveness in giving feedback that people end up thanking you for.

Iâ€™ll walk through a real scenario from a client who applied the framework within a challenging interpersonal context (with a micromanaging founder), explaining each â€œmoveâ€ she makes. Then Iâ€™ll review phrases and principles you can use, and wrap up with three ways AI can help you prepare and master feedback.

But first, where did GAIN come from?

Why GAIN? The evidence behind the framework

GAIN represents the convergence of principles that I encountered in research across over a dozen diverse domains, ranging from couples therapy to decision-making to workplace collaboration to teaching, all focused on how to inspire enduring behavior change.

They all orient to the same thesis: Communicating what we all want more of (the gain) is more effective than communicating what we want less of (the pain). They donâ€™t say that speaking about pain is bad, just that itâ€™s helpful to contextualize it as the gap between where we are and where we want to be.

First, letâ€™s look at communication in the most sensitive domain: marriage. Watching just three minutes of a conversation, marriage researcher

John Gottman

was able to predict with 93% accuracy which couples would be happy, distressed, or divorced six years down the road; most people, even couples therapists, averaged

50% accuracy

. How does Gottman distinguish the relationship masters from the disasters? His meticulous research, filming couplesâ€™ conflicts and tracking the couples over years, shows that couples with long-term happy, stable relationships open conflict conversations with more positive and less negative emotions than doomed ones, especially

focusing on sharing the â€œdream within the conflict.â€ Instead of just complaining, they articulate what it is they want more of and why (the

Goal

in GAIN)

. Hearing what you both want more of makes it easier to feel motivated and excited about the future rather than frustrated about whatâ€™s not working in the present. When

65% of startup failures

are attributed to problems

within

management team dynamics, Gottmanâ€™s relationship research is also worth paying attention to at work.

The GAIN framing not only leads to behavior change, it can also elicit better decisions.

One study

by Amos Tversky and colleagues showed that doctors given data with an

avoid

framing (â€œThe mortality rate is 10%â€) made the most effective treatment decision only 50% of the time, whereas those with the same inputs but using an

approach

framing (â€œThe survival rate is 90%â€) made the most effective decision 84% of the time. The meaning of the framing is identical (10% rate of mortality is the same as 90% rate of survival), but the

avoid

framing and its threat mindset distort our ability to think and process information clearly. This is precisely the ability we want people to be engaging when receiving feedback and considering a change in behavior.

This dynamic plays out elsewhere in the workplace as well (and in a way I keep thinking about as leaders fret over AI adoption strategies). Amy Edmonsonâ€™s

research

compared teams in 16 different organizations implementing the same new technology. She showed that teams whose leaders pitched it using an â€œaspirationalâ€ framing (e.g. â€œThis will help patients recuperate fasterâ€) were far more effective than those using a â€œdefensiveâ€ framing (e.g. â€œThe reason for doing this . . . is to avoid being blindsided by competitive pressures in the futureâ€). The successful leaders not only emphasized what to move toward, they also framed getting there as a team effort.

Our brains are engaged in a constant calculus: approach or avoid, friend or foe? When we frame a conversation with the potential benefits of change and acknowledge what we need to change ourselves, it sends a strong approach signalâ€”a powerful â€œweâ€™re on the same teamâ€ messageâ€”to the other party. They are then more likely to hear it as â€œHey, this information can be really useful to me, and I want to help this person too.â€

To explore what all this theory looks like in a real feedback conversation, see the case of one of my clients below.

GAIN in the real world: Giving feedback to a micromanaging founder

Laura is a VP of Business Development at a high-growth startup, where she reports to a founder who practiced micromanagement like it was her religion. Unsurprisingly, this was quite frustrating for Laura. It would have been tempting to approach this challenge by telling the founder how deflating this experience was and asking her to change (or by just avoiding the conversation altogether, growing resentful over time, and eventually looking for another job). Instead, Laura processed her own raw reaction first, using her emotions to uncover what she actually wanted to get out of a feedback conversation: the founder to give her more space. Laura understood Gottmanâ€™s â€œdream within a conflictâ€ principle, so when the founder was complaining to her yet again about other teamsâ€”this time the sales teamâ€”Laura seized the moment.

When she began the conversation, she named the founderâ€™s

actions

and the

impacts

that mattered

to the founder

, and empathized with her on those points. Then, instead of dwelling on the founderâ€™s pain or going into her own, Laura turned toward the possible

goal

for the founder

, emphasizing the benefits for the founder in a way that aligned with her own goal. (GAIN conversations donâ€™t have to present the goal first to be effective, as long as they are in the spirit of GAIN and the goal is clarified soon after.) Finally, she guided the conversation toward

next actions

that would accomplish

both

of their goals.

Hereâ€™s how she masterfully led the conversation, as she reported it to me:

Actions and Impacts:

â€œRight now, I can see you are on every sales call, email, in all of the Slack channels across every team, and I can only imagine how exhausting that must be.

I imagine that you feel like you have to do it because you feel the job wonâ€™t get done unless you do.â€

Goal:

â€œI think there might be another approach here that could create more time for you to focus on the areas where your impact is greatest and, at the same time, help the team step up, do better work, and feel more motivated. It could even reduce some of the stress and burnout you were talking about.â€

The founder responded honestly,

â€œI know, I know, and I want to fix it. But I just donâ€™t see any other way of doing things, and I donâ€™t see the sales team getting the results.â€

A feedback opening line is an invitation, and Lauraâ€™s was so inviting, the founder accepted and immediately engaged with it. The response, though brief, revealed lots of useful information: the founderâ€™s desire to change, the obstacle to that change, and what matters most to herâ€”getting results. Laura can leverage this to move the dialogue forward toward her desired outcomes.

Next, Laura took her founderâ€™s concern seriously and engaged her further, asking more questions that invite the founder to reflect more deeply on the obstacle sheâ€™s raising while emphasizing the importance of trust:

â€œIf thatâ€™s the case, do you have the right people? If you canâ€™t trust people to do the work, are they the right people?â€

â€œYeah,â€

the founder said,

â€œI think thatâ€™s the bigger questionâ€”the thing I need to think about.â€

Having built a sense of â€œIâ€™m on your teamâ€ in the conversation, Laura continued and challenged her:

â€œThe interesting thing is, though, I know you trust

me

a lotâ€”youâ€™ve mentioned it, and Iâ€™ve even observed itâ€”but you still sometimes do it with me.â€

Here, Laura doesnâ€™t judge or criticize; she just references her observation of the founderâ€™s pattern of actions (participating in all calls, emails, Slack channels), as well as the exceptions to that pattern (â€œYouâ€™ve mentioned [your trust in me], and Iâ€™ve even observed itâ€).

The founder acknowledged,

â€œI know. Itâ€™s just my way; Iâ€™m so used to doing it. I get everything done myself. I push, push, push.â€

So Laura asked,

â€œWhat would be more helpful for you, so you donâ€™t have to do that?â€

Instead of solely focusing on what the founder can do differently, Lauraâ€™s question broadens the possible levers for change to include herself and others. Some people are wary that this approach might mean less accountability for the feedback recipient, letting them off the hook. Rather, more levers means more leverage to get what we want.

Next actions:

The founder replied,

â€œI really appreciate when you proactively send me updatesâ€”which I know you already do. But also, you can just tell me when Iâ€™m doing it.â€

Laura confirmed:

â€œSounds great. So moving forward, Iâ€™ll continue proactively sending you updates, youâ€™ll work on stepping backâ€”i.e. not jumping in or pinging me with status checksâ€”and Iâ€™ll name it in the moment if the old pattern rears its head.â€

These are clear, concrete actions that they can both take to change the feedback recipientâ€™s pattern of checking in unnecessarily. The founder conveyed her intent to stop while also acknowledging her imperfection and inviting Laura to help her make this changeâ€”which was particularly empowering given the power dynamic.

The improvement took time to sink in, though Laura felt full agency to help move it forward. A month later, she shared that the founder had reverted to micromanagement mode three times that week, but then stopped after Laura told her when she would send an update. Over time, the micromanaging pattern ceased almost completely, and not only that; Laura told me that she felt newly confident in her ability to initiate challenging conversations.

Laura navigated a tough dynamic with someone holding greater organizational power and emerged with both the founderâ€™s commitment to change and an invitation to support that changeâ€”while building her own confidence and skills.

GAIN feedback principles and phrases

This example models both the practical steps of the GAIN framework and the spirit of it: care

and

challenge, offering empathy

and

expressing what the feedback giver wantsâ€”in that order.

Hereâ€™s a simple breakdown of Lauraâ€™s conversation through the lens of GAIN, though her goal comes in the third sentence, not the first:

You can download the GAIN template

here

, with prompts to guide you.

Letâ€™s dive into the nuances and some helpful phrases and principles you can consider, drawing on Lauraâ€™s example and others.

G: The Goal

The essence of the Goal section is naming the topic and pointing to the possible benefits of making a change in such a way that they feel youâ€™re on their team.

These benefits can be for you too, but they

must

be appealing to the feedback recipient. This is not what you want them to

do

differently; itâ€™s the impact created by them doing it. What will they gain from making a change?

If they understand that motivation up front, theyâ€™re hooked, and everything else flows from there.

What struck me most in the way Laura shared her feedback is that it didnâ€™t sound like a complaint or criticism at all. Instead, she shared it in a way that genuinely came across as a desirable opportunity for both the founder and her. She mentioned the current pain but oriented to the possible gain, â€œthe dream within the conflictâ€ that Gottmanâ€™s research showed to be so effective.

Here are a few ways to approach this.

If you can find even one instance of someone already using the desired behavior, it will help them recognize what that looks and feels like. Further, they will feel more confident in their ability to act in that way. The impact of that behavior serves as the Goal for the feedback.

â€œEven moreâ€ conveys that they both have some foundations to build on and that theyâ€™re not there yet.

Sometimes itâ€™s important to emphasize the â€œnot yet,â€ the gap between where performance is now and where it needs to be. In those cases, we must offer both challenge and support. These qualities are usually understood as opposites (â€œIâ€™m either extremely honest or extremely caringâ€), but that is a misconception.

One randomized, double-blind controlled

study

from Stanford tested the impact of integrating these two qualities using the sentence â€œIâ€™m giving you these comments because I have very high expectations [challenge] and I know that you can reach them [support].â€ Compared with a placeboâ€”â€œIâ€™m giving you these comments so that youâ€™ll have feedback on your paperâ€â€”the first sentence more than

quadrupled

the number of revisions to the original work that one group of students did, and the revisions themselves were on average 26% better.

Each of the phrases above can engage peopleâ€™s interest in the conversation, but the key is that it be an actual conversationâ€”a dialogue, not a monologue. One of the most common mistakes I see people make is

delivering

feedback instead of exploring it together.

The point is not you giving the feedback; itâ€™s them getting itâ€”and ideally, you getting some insights too. In this way, communication is like a game of catch: the other person needs to actually receive and hear the feedback in order for the game to be successful. GAIN largely focuses on throwing better so itâ€™s easier for them to catch whatever youâ€™re throwing, but ultimately, effective communication is an alternating rhythm of throwing and catching.

You can initiate this rhythm by pausing routinely and asking a question that elicits their reflections, reactions, and their own feedback for you. This creates two benefits: (1) it can reveal what message they are actually getting, giving you the opportunity to clarify immediately if needed, and (2), equally important, it can expand your understanding of root causes, revealing blind spots you may have (and keeping you from putting your foot in your mouth).

Especially if youâ€™re not clear yet on why they would care or what

their

goal is, listen before you talk.

A & I: Actions & Impacts

Itâ€™s not just the goal that determines receptivity or defensiveness. The most common move I see that leads people to put up their shields is hearing critical judgments. Theyâ€™re an almost guaranteed way to trigger people and derail the conversation.

Feedback is effective when itâ€™s actionable. To make it actionable instead of objectionable, feedback masters share

observations

of peopleâ€™s actions and their impacts, not judgments. â€œResponse times to client emails averaged 4.2 days last monthâ€ lands very differently than â€œYouâ€™re being unprofessional.â€

Even if this seems obvious, it becomes challenging when weâ€™re annoyed and feeling justified in our annoyance (â€œProduct just threw this over the wall; they donâ€™t care about our time!â€).

Our judgments feed our emotions

, which then amplify our judgments.

And what seems straightforward in theory can be deceptively difficult in the face of real-world situations. In over 50 feedback workshops across dozens of tech companies, Iâ€™ve asked people to share real-world feedback examples, and there hasnâ€™t been a single workshop in which everyone agreed on whether every example we talked through was a judgment or observation.

Fortunately, recognizing and translating our judgments is a skill easily developed with practice, one even more easily practiced now with AI. To do so, you can use this

custom GPT

or just input this prompt:

Give me examples to practice identifying whether something is an observation or a judgment. Explain why Iâ€™m right or wrong after I answer, then give another example, making them increasingly ambiguous each time.

Thereâ€™s a strong correlation here: the more clearly we see what is impacting progress, the more quickly we can identify new strategies to move toward our Goal.

Laura never called it â€œmicromanagementâ€ to the founder. She didnâ€™t even use the word â€œfeedbackâ€; she just described the actions and their effects as she observed them.

This gets to the essence of feedback as I define it: creating visibility into peopleâ€™s actions and their impacts

. Simple cause and effect is a line of sight that the actor often lacks. Often, not only do we lack visibility into how we are impacting others; we lack the ability to step back from and see our own behavior.

Avoid flattering judgments too

Even for well-practiced feedback masters, it can be tempting to deploy critical judgmentâ€™s close cousin, flattering judgment. Well-intentioned people might offer praise and compliments, like â€œYouâ€™re a rockstar PMâ€ or â€œYouâ€™re so brilliant. You have such a gift,â€ to motivate or thank colleagues. But just as critical judgments can trigger defensiveness, flattering ones can destroy growth mindset, autonomy, and productivity, and foster people-pleasing.

The â€œrockstar PMâ€ worries about maintaining that image if she fails, turning challenges into threats rather than opportunities to learn and grow. Carol Dweckâ€™s

mindset research

explains why:

If the cause of my success is my brilliance, what does that imply about the cause of my potential failures?

This is especially pernicious in contexts that require experimentation, where failureâ€”however temporaryâ€”is inevitable. This dynamic is exacerbated when the flattering judgments come from a manager or executive.

That rockstar PMâ€™s focus reorients from iteration and impact to seeking approval.

She could spend a lot of energy trying to get an â€œExceeds Expectationsâ€ on her next report cardâ€”I mean performance reviewâ€”instead of focusing on influencing user behavior or being more effective in her work relationships.

By contrast, observations of peopleâ€™s actions and impacts point them to what is within their controlâ€”the actions they can change or keep doing and the potential impacts of those actions. They see how to increase their own ability to create their desired outcomes. This means that giving this kind of actionable feedback can actually

evoke

feedback recipientsâ€™ growth mindsets, or

as Satya Nadella frames it

, help them shift from know-it-alls to learn-it-alls. If you want greater openness and motivation, share impacts, not judgments!

Hereâ€™s how

.

Acknowledge your own actions and their impacts

Judgments arenâ€™t the only trigger for defensiveness. When you as the feedback giver have contributed to a situation but you arenâ€™t noting that, the recipientâ€™s attention can latch onto this gap. â€œYeah, but what about

your

actions?â€ they wonder. You can preempt this reaction by simply and directly acknowledging your contribution.

As a social species, humans unconsciously mirror the behavior we see in others around us. The crowd waits at a light, and so do we; we start jaywalking, and others are more likely to as well. This plays out interpersonally too. Far too often, we approach feedback conversations with the implicit attitude of â€œYou are the one who needs to change.â€ From this angle, itâ€™s no surprise that what we get back is the other person looking at us thinking, â€œNo,

you

are the one who needs to change.â€ We tend to get what we give.

But this human instinct is actually good news: if you want to lead a conversation and a relationship in a specific direction, just start moving in that direction yourself. The message switches to â€œI think we can both shift some things that will make it easier to change and move toward that Goal.â€

This is one of most disarming feedback approaches Iâ€™ve ever encounteredâ€”and one of the least-used. Once we have acknowledged our contribution and shown that we are monitoring our own behavior, the other person can release their focus on our actions, freeing up their attention to concentrate on their own actions.

Thereâ€™s an added benefit here for the feedback giver: shifts that I make in one relationship often apply to other relationships too. When I was frustrated with one client constantly rescheduling, I realized how I was contributing to and even encouraging the problem by always saying yes to requests. Quick check-ins with all my clients around minimizing changes dropped rescheduling requests to zero across the board. More levers, more leverage.

N: Next actions

What do we do with those levers we identified? Once feedback has been shared and received, the final step in an effective feedback conversation is defining next actions. These could be actions for the feedback recipient, the feedback giver, or both.

The clearer and more concrete our action-and-impact observations are, the more obvious the potential next actions become.

Sometimes itâ€™s a very direct link: â€œHey, when you donâ€™t schedule interviews in the time frame you said you would, it slows down our hiring process. Can you schedule those this week and let me know if there are any blockers?â€

Other times, arriving at next actions is more complex and uncomfortable. Our insides are screaming, â€œThis conversation is awkward. Get me out of here already!â€ But even an otherwise-great feedback conversation will often amount to nothing if we donâ€™t close it clearly and concretely.

There are a few ways of doing this that increase buy-in and follow-through.

First, ask for their ideas

In an often-maddening phenomenon, people will resist change that they themselves want if it appears to come at the expense of their autonomy. We may want to change, but we want to choose it.

One simple way of supporting someoneâ€™s sense of autonomy is to ask for their ideas of what to change before offering our own. When the other person is really on board with the Goal and open to hearing what weâ€™ve shared, given even a few moments of brainstorming, I have found that they regularly come up with useful options I had not considered. They also tend to be more motivated to pursue ideas they have generated themselves.

For feedback that is difficult to hear regardless of how itâ€™s shared, a feedback conversation might actually be a series of conversations, so itâ€™s often best to sleep on it and schedule a future time to return and commit to next actions (not just â€œlaterâ€).

Co-create through brainstorming together

If you are itching to share your own idea but also want to involve them, this option opens up the opportunity for both. It also lets you build on each otherâ€™s ideas.

Make specific requestsâ€”and make it easy to say no

Sometimes asking for the other personâ€™s ideas can feel disingenuous when all you want is to propose your own. In that case, share your idea, and offer it as a suggestion or a request in a way that still honors their autonomyâ€”in other words, not as a demand.

To support this feeling, make it easy for them to say no to or edit the request. A

meta-analysis

of 52 studies showed that â€œFeel free to say noâ€ doubled the percentage of time that people said

yes

to a request. Supporting autonomy conveys real respect, which preempts resistance, engenders goodwill, and helps people think clearly about the suggestion on its own merits. If they actually say no, you will get to learn whatâ€™s blocking them and how to incorporate that in the next actions. (And if youâ€™re feeling attached to your idea and actually want to demand it, remember that itâ€™s the Goal you care about, not the way they get there.)

Itâ€™s also important to ensure that your specific request includes what you want more of, not just less of. As psychologist Marshall Rosenberg, the author of

Nonviolent Communication

, said, â€œYou canâ€™t do a donâ€™t.â€ Otherwise, when they find themselves in the same situation again, theyâ€™ll be unclear about what alternative behavior to engage in.

Frame it as an experiment

When someone is considering a new behavior, that means thereâ€™s an element of uncertainty. Understandably, there can be some hesitance about committing to do something theyâ€™ve never done before, no matter how much they follow the logic. Instead of committing to something new â€œforever,â€ explicitly acknowledge it as an experiment for a set time period.

WWW:

Who

will do

What

by

When

?

This question moves growth, progress, and whole companies forward faster than any other one I know.

â€œLetâ€™sâ€ and â€œshouldâ€ are insidious impostors when it comes to action items.

Letâ€™s

: Is that you or me?

Should

: Is that an idea weâ€™re considering or a commitment? When you hear these words, use them as an automatic signal to pause and clarify who will do what by when.

Schedule a check-in

If you donâ€™t set a clear check-in, you run the risk of not seeing the change you were hoping for and having to initiate a follow-up feedback conversation if you still want to close the gap. The discomfort of this is higher, so itâ€™s easier to avoid, and the frustration festers into resentment and helplessness. If instead you establish a time and date to check in, you set up three more desirable possibilities:

An agreed-upon check-in time often increases the accountability and expectation of change.

The behavior may have changed, and you have a built-in moment to appreciate the impacts youâ€™re seeing and the progress toward the Goal, encouraging even more development. This reinforces a growth mindset.

If the behavior hasnâ€™t changed or hasnâ€™t had the desired impact, you will recognize this sooner and be able to troubleshoot and course-correct faster.

AI in feedback

Thatâ€™s a lot to digest and consider when preparing and leading a feedback conversation. Fortunately, youâ€™re not on your own anymore.

While many people fear AI will erode human connection, I see the opposite happening with clients and in

our workshops

:

People are using AI as a tool for expanding interpersonal skills and deepening human relationships.

Here are three ways AI can support you in using the GAIN approach and having much more effective, connective feedback conversations:",https://www.lennysnewsletter.com
https://www.lennysnewsletter.com/p/how-to-get-the-most-out-of-your-product,"How to get the most out of your product pass, part 1",Tips for taking full advantage of the 15+ free premium products you have access to as a paid subscriber,2026-01-02T16:54:06.110997,Lenny Rachitsky,"ğŸ‘‹  Each week, I tackle reader questions about building product, driving growth, and accelerating your career. For more:

Lennybot

|

Lennyâ€™s Podcast

|

How I AI

|

Lennyâ€™s Reads

|

Courses

Subscribe

After

introducing the product pass

a couple of months ago, the most common question I getâ€”other than â€œWait, is this for real??â€â€”is how I and others use these tools. So Iâ€™ve put together a short 2-part series about how I use each tool, what I love about it, and what it can help you with.

This week Iâ€™ll cover

Replit

,

Warp

,

Linear

,

Wispr Flow

,

Gamma

,

Magic Patterns

,

Descript

, and

Mobbin

. Next month Iâ€™ll cover

Lovable

,

Bolt

,

n8n

,

Granola

,

Superhuman

,

Raycast

,

Perplexity

, and

ChatPRD

.

My goal here isnâ€™t to sell you the products (

you already get them for free

!) but to help you get the most possible value out of your paid subscription.

If you already use any of these products heavily and have a pro tip to offer, please share in the comments!

Leave a comment

1.

Replit

What it is

A leading vibe-coding platform that takes a plain English description of what you want to create and builds a production-ready app. It includes a native database, user authentication, security scan, and hosting. It also supports most programming languages. They launched

Agent 3

, which can run for more than 200 minutes autonomously, building, testing, and fixing your app on the fly.

Why I love it

Itâ€™s a powerful full-stack vibe-coding platform that lets you build highly complex web and mobile apps. Unlike other vibe-coding platforms, you can single-shot fairly sophisticated applications. It includes a fully featured (web-based) IDE and automatically tests the apps it builds using its internal browser (i.e. checking buttons, forms, APIs) and then auto-fixes the issues it finds.

What I use it for

The entire (surprisingly sophisticated)

product pass workflow

is hosted on Replit.

How other people are using it

Coming up with startup ideas

Vibe-coding automations

Building a social networking platform (in 193 minutes)

Building a fully featured website for Saastr

Taking on Upwork projects

Learn more

Check out their

website

and their

PM-specific use cases

.

2.

Warp

What it is

An agentic development environment thatâ€™s essentially a drop-in replacement for your Mac/Windows/Linux terminal. It was originally built for engineers but is increasingly used by PMs, designers, data scientists, and even recruiting teams.

Why I love it

Itâ€™s truly a magical experience, and unlike anything else Iâ€™ve used. It makes you feel like a superhero. Anytime I run into a problem working in my Mac terminal, AI solves it for me. How did we ever work with computers without something like this?

What I use it for

Iâ€™ve used it to download YouTube videos to convert them to MP3s (for my sonâ€™s

Yoto

), analyze all of my locally-saved podcast transcripts to find the most-mentioned books, and install gnarly packages with a bunch of weird dependencies. I just tell it what I want to do, and it figures it out. Hereâ€™s me asking Warp to download the Linear YouTube video below:

How other people are using it

Building a Strava clone

Converting MRI imaging files to jpegs

Dad teaching kids how to build websites

Learn more

Visit their

website

or check out their many tutorials at

warp.dev/university

.

3.

Linear

What it is

A beautifully designed tool for tracking your teamâ€™s tasks, organizing projects, and building your roadmap. It even has deep agent integrations, so you can delegate your tickets to an agent to take a first pass at a task. The future is now.

Why I love it

In my mind, thereâ€™s no question that every startup and modern product team should be using Linear. And at this point, most are.

What I use it for

OK, so I donâ€™t actually use Linear, because itâ€™s just me. But if I had a team, I wouldnâ€™t even consider another tool.

How other people are using it

Linear using Linear to manage their own customer feedback

Claire at ChatPRD building an agent in Linear

Commure building dashboards to monitor their teamâ€™s progress

Learn more

Check out their

website

and this

library

of how-to videos, and hereâ€™s

my podcast conversation with Nan Yu

(their head of product).

4.

Wispr Flow

What it is

Magical voice-dictation that actually works the way youâ€™ve always expected this to work. Press the function key on your laptop, talk, and it instantly (and accurately) transcribes everything youâ€™ve said. Works across Mac, iPhone, and Windows, and with every vibe-coding tool and IDE.

Why I love it

Itâ€™s so fast, and itâ€™s wildly accurate. It understands the context of the conversation,  filters out filler words, and learns your unique acronyms, terms, and phrases. If youâ€™ve tried the native mic transcription on your phone, this is a whole different ballgame.

What I use it for

Iâ€™ve always preferred typing over talking, so Iâ€™ve never gotten into transcription apps, but Wispr Flow changed how I work. Since itâ€™s so good, Iâ€™ve found myself using it constantly, especially on my

phone

. For example, my wife is a big texter, and instead of having to type out long messages all day, I press a button, say it, and send it.

How other people are using it

Talking to chatbots

Vibe coding

Getting â€œvoicepilledâ€

Doctors transcribing their notes

Slow typers using it to speed up their workflows

Learn more

Check out their

website

.

5.

Gamma

What it is

An AI-powered document creation tool that generates polished presentations, landing pages, and documents from a simple prompt. Describe what you want, upload the raw data, and get back a compelling deck/doc/website. And just last week,

they launched Gamma 3.0

, which adds a bunch of really cool new powers.

Why I love it

If Cursor is making engineers faster, Gamma is doing the same for PMs. Creating decks and docs is one of the most time-consuming and annoying parts of most buildersâ€™ jobs. I also just love

the story of Gamma

â€”small team, little funding, profitable, and AI-native from day one.

What I use it for

Most recently, Iâ€™ve been experimenting with using Gamma to create shareable summaries of my podcast episodes. Hereâ€™s

Ethan Smithâ€™s episode on AEO

,

Ben Horowitzâ€™s episode on leadership

,

Brendan Foodyâ€™s story of Mercor

, and

a peek at an upcoming episode on evals

. I copied and pasted the transcripts, clicked a few buttons, and . . . magic.

How other people are using it

Generating custom decks for each sales outreach

Creating landing pages

Turning meeting notes into a deck and follow-up email

(

hereâ€™s how to do this with Zapier

)

Creating a unique resume

Turning PRDs into decks

Learn more

Check out

their website

.

6.

Magic Patterns

What it is

Magic Patterns is an AI prototyping tool built specifically for PMs and designers. Itâ€™s not trying to get you to build production-ready apps or launch startups. Itâ€™s designed instead to be the best tool in the world at prototyping your ideas using your existing productsâ€™ styles, getting feedback from customers, and then helping you build a real product in your regular development environment.

Why I love it

Iâ€™ve been playing with all of the popular prototyping tools, and I find that Magic Patterns most often produces the highest-quality, most useful, and most correct results. Itâ€™s especially great at quick iterations and ideation because it only produces frontend code, so you donâ€™t have to worry about random database/infra architecture limitations. I encourage you to do your own side-by-side comparison to see for yourself. I also love that itâ€™s not trying to be everything to everyoneâ€”itâ€™s designed specifically to solve pain points for product teams at large companies, with features like

using your own component library

.

What I use it for

Iâ€™ve used it to create

this thumbnail preview tool

, and other internal tools Iâ€™m tinkering with that Iâ€™m not ready to show the world ğŸ‘€

How other people are using it

Copy-and-pasting PRDs, creating a Magic Patterns prototype, and then

sharing with engineering

Creating personalized customer demos

Skipping Figma entirely

when prototyping

More examples here

and

tutorial videos here

Learn more

Check out their

website

, and build something!

7.

Descript

What it is

An AI-powered editor that gives everyone (including product managers!) the power to create and edit professional videos. Use it to make launch videos, design walk-throughs, for LinkedIn posts, etc. You edit videos like you edit Google Docs (wild!), and it now includes an agentic video editor, called Underlord, that makes it even easier to make tricky changes (

see example

).

Why I love it

I have zero video editing skills and Iâ€™m constantly making and editing video and audio. I wouldnâ€™t be able to do this without Descript. Itâ€™s been my tool of choice for over five years now, and Iâ€™ve never thought about using anything else.

What I use it for

I use Descript basically every day. I use it to record my podcast ads and intros to each episode, and my production team uses it exclusively to take my raw podcast recordings and turn them into the videos you see on

YouTube

.

How other people are using it

Creating YouTube shorts

Adding big juicy text to videos

Creating rough cuts with AI

Creating avatar-based demos

Creating ads

Learn more

Check out their

website

.

8.

Mobbin

What it is

The worldâ€™s largest mobile and web design reference library. Use it to unblock your design thinking, benchmark your product flows against best-in-class apps, and stay up to date on the latest UX trends.

Why I love it

I wish this existed when I was a full-time PM. I would have saved my team and myself hundreds of hours of screenshotting competitorsâ€™ flows looking for inspiration and a lay of the land.

What I use it for

I point people to it when they are looking for ideas for optimizing a flow in their product.

How other people are using it

Getting design inspiration

Copy-and-pasting best-in-class flows into your Figma

Designing a viral app

Elevating your design taste

Studying the best

Learn more

Check out their

website

.

â€”

Part 2 is coming next month.

Subscribe

if you havenâ€™t yet to get access to all of these tools!

Have a fulfilling and productive week ğŸ™

If youâ€™re finding this newsletter valuable, share it with a friend, and consider subscribing if you havenâ€™t already. There are

group discounts

,

gift options

, and

referral bonuses

available.

Subscribe

Sincerely,

Lenny ğŸ‘‹",https://www.lennysnewsletter.com
https://newsletter.pragmaticengineer.com/p/the-pragmatic-engineer-in-2025,The Pragmatic Engineer in 2025,"The most-read articles of the year, plus some personal favorites, and a look back at a busy year in tech",2026-01-02T16:54:08.047380,Gergely Orosz,"This holiday season marks the end of the fourth year of The Pragmatic Engineer as my full-time focus, following more than a decade of working as a software engineer and engineering manager. Throughout 2025, you received 134 newsletters: a mix of in-depth

deepdives

, tech news in

The Pulse

, and conversations on software engineering in

The Pragmatic Engineer Podcast

.

As of today, the newsletter has 1,073,929 readers, of whom more than 200,000 have joined in the past year alone. Special thanks to paying subscribers, who get full access to all deepdives, issues of

The Pulse

, as well as

extra resources

for career growth, and content for engineering leaders. Iâ€™d like to thank everyone who reads this publication; your support is truly valued.

In this final article of 2025, I look back on the year and suggest some articles and pod episodes worth revisiting â€“ or checking out for the first time.

Today, we cover:

Most popular articles.

Five of the most-read, and five of my personal favorites

Podcast.

Memorable episodes to check out

Tech industry in 2025.

AI dominated the conversation and trends, the job market was weird, RTO accelerated, and much else

The Software Engineerâ€™s Guidebook.

Four more translations, a hardcover edition, and a best-seller in Japan!

See also annual reviews in

2024

,

2023

,

2022

, and

2021

.

1. Most popular articles

Over the course of the year, the articles below are the ones which were read by the most subscribers.

How Claude Code is built.

Claude Code took the industry by storm in 2025, and we sat down with two of its founding engineers. They revealed how they helped make command line interfaces (CLIs) surprisingly relevant in such a short space of time.

State of the software engineering job market in 2025

. Itâ€™s been a bizarre 12 months in the job market, when job seekers struggled to hear back about their applications, and employers found it hard to hire solid engineers. We delved into the state of things in a three-part series on data (

part 1

), what hiring managers saw (

part 2

), and job seekersâ€™ personal stories (

part 3

).

Real-world engineering challenges: building Cursor

.

Founded in 2023, Cursor became one of the most â€“ if not

the

most popular â€“ dev tools this year. A deepdive into its tech stack, engineering decisions, and the database migrations which took place behind the scenes.

MCP Protocol: a new AI dev-tooling building block.

The Model Context Protocol that extends AI capabilities for both IDEs and AI agents. An in-depth look at the technology with its co-creator, David Soria Parra.

AI fakers exposed in dev recruitment: postmortem

.

In March, a full-remote startup nearly hired a backend engineer who didnâ€™t exist, after they used an AI filter and a fake resume to try and hoodwink recruiters. Itâ€™s also possible the scammer was a North Korean agent. Indeed, just a few days ago, Amazon

caught

one posing as a US contractor. Incidents like these have probably helped contribute to a decline in the number of remote jobs, and made it harder to get hired.

My personal favorites:

AI Engineering in the real world

. What does AI engineering look like in practice? Hands-on examples and learnings from software engineers turned â€œAI engineersâ€ at seven companies â€“ with inspiration for how itâ€™s relatively easy for a software engineer to become an â€œAI engineer.â€

Inside Googleâ€™s Engineering Culture

.

A broad, deep dive into how Google works from the perspective of SWEs and eng managers.

What are Forward Deployed Engineers, and why are they so in demand?

Startups and scaleups went on a hiring spree for a software engineering role pioneered by Palantir. A deepdive into the role, and why FDEs became so popular this year.

Cross-platform mobile development.

Cross-platform mobile development was on the rise in 2025. An in-depth look into the most popular frameworks: React Native, Flutter, native-first, and web-based technologies, and how to pick the right approach

The 10x â€œoveremployedâ€ engineer

.

One improbable story this year was that of a clearly talented software engineer who tricked more than a dozen Silicon Valley startups. He aced the interviews and got hired â€“ then proceeded to do almost zero work, all the while collecting paychecks and seeking out more job interviews. The music stopped for this blagger when one frustrated founder whoâ€™d hired and fired him went public.

See all deepdives of 2025

here

.

2. The podcast

I launched

The Pragmatic Engineer podcast

18 months ago because I was having interesting conversations with tech folks during research for the written articles, and it felt like a bit of a waste to not share these talks with readers in more detail. Overall, Iâ€™ve been very happy with the decision to experiment with the podcast format.

This has been the podâ€™s first full year, and the feedback has been positive. When I meet people at conferences and events, around a third say they discovered The Pragmatic Engineer via the podcast. At the end of this year, the podcast has crossed 10M downloads across YouTube, Spotify, Apple Podcasts, and other apps. In 2026, Iâ€™ll prioritize quality over quantity even more, so expect two or three episodes per month on Wednesdays.

Here are some highlights from the podcast in 2025, which are good places to start if youâ€™re diving into the show for the first time:

How Linux is built with Greg Kroah-Hartman

: the longtime Linux kernel maintainer breaks down the inner workings of Linux development; from its unique trust model, to the benefits of open-source contribution. This is the episode that was most popular with listeners this year, judging by the feedback.

Netflixâ€™s Engineering Culture

.

A very rare peek into how engineering works at the streaming giant, with CTO Elizabeth Stone.

The Philosophy of Software Design â€“ with John Ousterhout

: why thoughtful software design matters more than ever, as AI transforms coding practices and developer workflows.

From Software Engineer to AI Engineer â€“ with Janvi Kalra

.

Janvi impressed tech leaders after teaching herself AI engineering. She discusses the tactics and focus that helped her get into OpenAI.

How Swift and Mojo were created with Chris Lattner

. A conversation about how better language and compiler design can open the door to faster, more accessible AI development, with one of the most productive software engineers around.

How AI will change software engineering â€“ with Martin Fowler

.

Thoughtworksâ€™s Chief Architect and author of the bestselling books Refactoring and Patterns of Enterprise Application Architecture, provides a level-headed take on what AI really means for the tech biz.

Measuring the impact of AI on software engineering â€“ with Laura Tacho

. Findings from 180+ companies on how AI really affects devsâ€™ productivity â€“ and what many teams get wrong.

Code Complete with Steve McConnell.

Code Complete is one of the all-time most popular books on software engineering. Author Steve McConnell gives a rare interview and shares some career principles that every engineer should know.

Building Figma Slides with Noah Finer and Jonathan Kaufman.

Behind the scenes on how a hackathon project turned into a polished product at Figma. Key engineering decisions, challenges, and practices.

Design-first software engineering: Craft â€“ with Balint Orosz.

You may not know I have a brother who also works in tech. Balint is the founder of the award-winning notes app, Craft Docs. An interview about what itâ€™s like to be a design-first software engineer.

See all podcast episodes

here

.

If you have any recommendations for interesting guests for me to invite onto the podcast in 2026, you can let me know by replying to this email or

via this form.

3. Tech industry in 2025

The main theme of 2025 has, of course, been AI, and how itâ€™s changing software engineering â€“ but we also covered numerous other matters and emerging trends.

The AI coding tools explosion.

Claude Code

rapidly became a favorite of devs,

Cursor

grew to be the most popular AI coding tool, the term â€œ

vibe coding

â€ went mainstream (and was even

pronounced

â€˜Word of the Yearâ€™ by Collins Dictionary â€“ despite being two words), the

MCP protocol

won wide adoption, and coding with agents slowly overtook using AI for â€œjustâ€ autocomplete.

Weird job market stays tight.

As we covered in the

state of the software engineering job market in 2025

series, 2025 saw many professionals experience more struggle than usual to land the next job â€“ except AI engineers and senior+ engineers with eye-catching resumes.

RTO acceleration.

Amazon

mandated

a 5-day return to office in January 2025, and Instagram

will do so

from February 2026. Meanwhile, the share of full-remote jobs

kept falling

.

Decline of StackOverflow.

This started before ChatGPT launched, but LLMs seem to have accelerated

the decline

. New generations of developers may have no idea what StackOverflow is, nor what made it so important in the 2010s.

AI-fueled cheating crisis.

Remote algorithmic interviews provide little signal these days, now that invisible tools help candidates ace them without much preparation. In response, some companies allow AI tools in interviews and set more ambitious problems, while others restore in-person interviews. More on this in

Ban or embrace AI in tech interviews?

and

How AI is changing tech interviews.

OpenAIâ€™s aggressive expansion.

The leading AI lab didnâ€™t slow down in 2025, and is

valued

at a jaw-dropping $500B. It

became

a for-profit,

launched a browser

to compete with Chrome,

bought

experimentation platform Statsig for $1B,

probably spent

more than $170M this year on Datadog, and

is making

plans to go public.

AI surprises.

Senior engineers use AI coding tools

more efficiently than juniors

, but equally, juniors are

more â€œprofitableâ€ than before

due to faster onboarding. AI seems to amplify coding knowledge for seniors, and could help juniors become seniors faster. Even though engineers at AI startups can usually use AI tools without any kind of budget limits â€“ it is also AI startups which set

extreme working hours cultures

in their bid to outrun competition.

Weekly editions of

The Pulse

on Thursdays provided the latest news from around tech. Check out

previous issues.

4. The Software Engineerâ€™s Guidebook

Itâ€™s been two years since I

published

The Software Engineerâ€™s Guidebook for navigating senior, tech lead, staff, and principal positions at established tech companies and startups. In November, I

shared a recap

to mark two years since publication, including learnings from taking the self-publishing route.

The book has now sold more than 40,000 copies, proving it was worth spending a very lengthy four years on writing it! This year, it was translated into four more languages, and is

available

in Japanese, Simplified and Traditional Chinese editions, Mongolian, German, and Korean.

In Mongolia, a 30-person startup did the

translation

, and this summer I flew over to visit them. It was there that I learned that the company which hosted me â€“ Nasha Tech â€“ also builds the Uber Eats of Mongolia.

A delightful moment was learning that the Japanese edition was a bestseller of the week at Tokyoâ€™s largest bookstore:

Best-seller in Kinokuniya Shinjuku, in the Shinjuku district of Tokyo

As a fun fact, the Japanese edition is printed in a special way: it reads vertically (top to bottom), and right to left. The publisher â€“ Oâ€™Reilly Japan â€“ told me itâ€™s an unusual but purposeful choice for a technical book:

â€œGenerally, novels and typical non-fiction books use vertical writing, which requires right binding (right-to-left reading). Technical books, due to code snippets, are usually written horizontally with left binding.

However, for The Software Engineerâ€™s Guidebook, we intentionally chose vertical writing with right-side binding so that readers can take their time with the content. Itâ€™s quite an unusual choice for an Oâ€™Reilly Japan bookâ€.

In November, a hardcover edition was published, which I decided to do after a reader asked to have their paperback signed, and I saw it was in poor condition due to heavy usage, with lots of notes and highlights. Softcovers donâ€™t withstand lots of heavy use, and seeing that beaten-up copy was the final prompt to create the hardcover.

Hardcover edition: Purchase as a gift

here

As a reader of this newsletter, you can get access to the bonus section of the book: 100 pages with ten online-only chapters.

Get them here.

Happy Holidays!

With that, the newsletter and podcast head off for a winter break. I wish you and your loved ones a very happy holiday, with time to rest. If you end up being oncall during this period, fingers crossed for a completely uneventful one. If your company has

code freezes

in place for this period, this could help with that.

Thank you for reading this newsletter and listening to the podcast in 2025; thereâ€™s more to come in 2026.

Have a good one â€“ see you in January!",https://newsletter.pragmaticengineer.com
https://newsletter.pragmaticengineer.com/p/the-pulse-157-internal-dev-tooling,The Pulse #157: Internal dev tooling at Meta & the â€œtrajectoriesâ€ feature,"Also: GitHub upsets devs by charging for self-hosted CI/CD, Warsaw could become the EUâ€™s new â€œtech capitalâ€, hiring juniors is profitable now, and more",2026-01-02T16:54:09.907250,Gergely Orosz,"Before we start, two things:

Check out our new MCP report!

Elin and I have done something experimental: create a more detailed report than even our MCP deepdive was. If youâ€™d like to get deeper into the MCP ecosystem,

check out the report

. Feel free to give feedback, as weâ€™d like to do more of these if thereâ€™s enough interest to justify the intensive research involved.

This is the most detailed report on this topic that we know about.

The Pragmatic Summit

speaker list

is nearly finalized.

Speakers confirmed in the past few weeks include some names you may know as guests on the Pragmatic Engineer podcast:

Chip Huyen

,

Martin Fowler

,

Nicole Forsgren

, and

Kent Beck

. Iâ€™ll share the detailed agenda in early January. The event takes place on 11 February, in San Francisco.

Apply here

to secure a spot.

The Pulse is a series covering events, insights, and trends within Big Tech and startups.

Today, we cover:

Internal dev tooling at Meta & â€œtrajectories.â€

An overview of three different internal AI-coding tools which devs at Meta use. Also, the company has started sharing the prompts which devs make when generating code â€“ and itâ€™s pretty controversial.

GitHub upsets devs by charging for self-hosted CI/CD.

GitHub Actions is notoriously slow and unreliable. But instead of improving the service, GitHub sought to charge by the minute for using third-party CI/CD solutions. That didnâ€™t go down well with devs.

Industry Pulse.

Warsaw on track to become the â€œtech capital of the EU,â€ hiring juniors could actually be â€œprofitableâ€ thanks to AI tools, GitHub is working on stacked diffs, non-devs use LLMs for more coding tasks, Cursor migrates CMS to Markdown, OpenAI gets rid of 6-month vesting cliff, and more.

â€œApple Taxâ€ alive in Japan.

A US court rejected Appleâ€™s appeal, and the iPhone maker can no longer ban alternative in-app payments or impose junk fees on third-party paymentsâ€¦ in the US. Meanwhile in Japan, Apple is pulling the same trick that got it in trouble in the US.

1. Internal dev tooling at Meta & â€œtrajectoriesâ€

An interesting detail about Meta that I learnt from chatting with two current software engineers there: last week, the social media giant rolled out a feature allowing devs to see the AI prompt history on diffs (internal lingo for pull requests) in review. And this new feature launch is causing some controversy in the workplace!",https://newsletter.pragmaticengineer.com
https://newsletter.pragmaticengineer.com/p/the-history-of-servers-the-cloud,No Title,,2026-01-02T16:54:11.035404,Gergely Orosz,"Stream the latest episode

Listen and watch now on

YouTube

,

Spotify

, and

Apple

.

See the episode transcript at the top of this page, and timestamps for the episode at the bottom.

Brought to You by

â€¢â 

Statsig

â  â€” â  The unified platform for flags, analytics, experiments, and more. Companies like Meta and Google had to build their own infrastructure for safer deployments and experimentation. Statsig makes this advanced tooling accessible to everyone. They have a generous free tier, a $50K startup program, and affordable enterprise plans.

Check it out.

â€¢â 

Linear

â  â€” â  The system for modern product development. We know that AI will be part of the software stack â€” in fact, it already is, today. To support AI agents, Linear they built an open API and SDK that lets any agent plug into your issue tracker. You can also connect popular agents like ike Cursor, GitHub Copilot, OpenAI Codex, and others.

Take a look at how Linear works with agents

.

In this episode

How have servers and the cloud evolved in the last 30 years, and what might be next? Bryan Cantrill was a distinguished engineer at Sun Microsystems during both the Dotcom Boom and the Dotcom Bust. Today, he is the co-founder and CTO of Oxide Computer, where he works on modern server infrastructure.

In this episode of

The Pragmatic Engineer

, Bryan joins me to break down how modern computing infrastructure evolved. We discuss why the Dotcom Bust produced deeper innovation than the Boom, how constraints shape better systems, and what the rise of the cloud changed and did not change about building reliable infrastructure.

Our conversation covers early web infrastructure at Sun, the emergence of AWS, Kubernetes and cloud neutrality, and the tradeoffs between renting cloud space and building your own. We also touch on the complexity of server-side software updates, experimenting with AI, the limits of large language models, and how engineering organizations scale without losing their values.

If you want a systems-level perspective on computing that connects past cycles to todayâ€™s engineering decisions, this episode offers a rare long-range view.

Interesting sections from the episode

How the â€œDotcom Bustâ€ brought more tech creativity than the â€œBoom.â€

As Bryan recalled (

starting at 14:53

), memories from the Dotcom Boom and Bust:

â€œA boom can get you to care about things that you actually donâ€™t care about. This is because in a boom, everyone is so financially driven that itâ€™s hard not to become financially driven!

But [the financial side] is actually not why I got into this. So during the bust, I was definitely able to put a meal on my table and a roof over my head.  But [the Bust] was a reminder about whatâ€™s important.

We did better technical work in the Bust than in the Boom.

I think thatâ€™s because in the Bust it was like: okay, now we

really

have to focus. We have fewer resources. These fewer resources actually force more creativity!

So all of the things that we did, certainly speaking at Sun and system software, so

ZFS

filesystem] and

DTrace

[dynamic tracing framework] and

Service Manager Facility

[unified service management framework in Solaris 10], all these things that were really revolutionary for the operating system. All of these [innovations] happened in this same kind of post-Bust period of time, from 2001 to 2005.â€

How Jeff Bezos tricked everyone with AWS pricing.

Bryan was running a small competitor to AWS called Joyent, and so he knew something about AWS pricing that the rest of the world was yet to catch up to (

starting at 26:07

)

â€œJeff Bezos is the apex predator of camitalism.

The thing that was so impressive is they were able to give people the idea that this [AWS] was a terrible business.

In particular, they did not break out their financials. So everyoneâ€™s like: 'oh my God, what an awful business! Theyâ€™re [AWS is] cutting the price every year. This is a classic â€˜red oceanâ€™. Itâ€™s bloody, you donâ€™t want to compete.

At Joyent, we were actually competing head to head with AWS. We had a public cloud. And unlike AWS, we took the software that we had used to run the public cloud and made it available for people that wanted to run a cloud on-prem on their own hardware. So people that would buy Dell or HP or Super Micro, they would buy [Joyentâ€™s] software and they would run it and get a cloud.

So we ran a public cloud and we knew what the economics of a public cloud were, namely

pretty

good. Margins

were

good! And so what we knew that Amazon wasnâ€™t volunteering [the price cuts.]

We knew is that AWS S3 was underwriting a war on big box retail, and that S3 was paying for your Prime shipping!

So it was a genius move [by Bezos.]â€

Why Kubernetes might have become really popular, in part thanks to AWS.

Bryan,

starting at 27:42

:

â€œThere was a period of time when it felt like in order to be in the cloud [business] you have to implement every AWS API [as a cloud provider]. So thereâ€™s this idea that you had to be [fully] compatible with EC2. One reason it was thought that GCP and Azure could never compete with AWS was because they could never be API compatible.

Then, what starts in 2015 is Kubernetes. Part of that initial attraction to Kubernetes is that people wanted to get some optionality around their cloud â€” and they felt locked into AWS. Theyâ€™re like â€˜Iâ€™m not using all this stuff [on AWS]. Iâ€™m not using

Elastic Beanstalk

[PaaS for web applications]. Iâ€™m not using

IoT Greenglass

[edge runtime and cloud for IoT services]. Iâ€™m not using

Redshift

[SQL-based cloud data warehouse]. What I actually want is this kind of basic infrastructure.

Kubernetes now gives me this layer upon which I can deploy and get some sort of

true

cloud neutrality.

The early momentum behind Kubernetes [felt like] this around this idea of I need to have some optionality [from AWS].â€

Interesting: Oxide found it hard to hire hardware engineers who are principles-first

, and folks who can build hardware without a â€œreference designâ€ available.

Turns out, building a new type of computer like Oxide means creating hardware from scratch, instead of using â€œreference designs.â€ Many hardware engineers are simply not used to doing this:

Bryan: â€œIn computer [hardware] design in particular, the high-speed designs are so hard that [electrical engineers] got very accustomed to taking the reference designs. And it was harder to find folks who were willing to take a clean sheet of paper. Ultimately, we found them, and weâ€™ve got an EE (electrical engineering) team that is extraordinary and absolutely fearless.

[The EE engineers at Oxide] didnâ€™t spend their careers at Dell and HPE. Theyâ€™re coming from [the likes of] GE Medical where they worked on CT systems.

Gergely: â€œIt feels like such a different field. I would naively assume that if youâ€™re building a computer, youâ€™ll try to get electronics engineers who have built computers.

Bryan: â€œYou would think! That was our [initial] thought as well. Then we discovered that we were not getting along with those engineers very well.

We were just finding thereâ€™s a lot of friction because there wasnâ€™t a real first-principles approach.

When you get to talk to folks that have been at Dell for a generation, for any design, theyâ€™re used to calling whatâ€™s called the FAE. FAE stands for the Field Applications Engineer for the voltage regulator. Itâ€™s like: â€˜well the FAE gives me the design.â€™ So we ask: â€˜Alright, well how do you know that itâ€™s the

right

design?â€™ So we went,  alright, so letâ€™s go hire that person [who understands the actual design.]â€

Bryan initially struggled to find the â€œrightâ€ electrical engineers â€” until Bryan

shared in an article

that they paid everyone at Oxide a $175,000 base salary in 2021 (today, this number

is $235,000 today

.) This article got shared inside hardware engineering circles, and suddenly they got standout electrical engineers applying â€” who often have not built computers before, but have been designing electrical equipmen and hardware from zero in other fields.

The Pragmatic Engineer deepdives relevant for this episode

Startups on hard mode: Oxide. Part 1: Hardware

Startups on hard mode: Oxide, Part 2: Software & Culture

Three cloud providers, three outages: three different responses

Inside Uberâ€™s move to the Cloud

Inside Agodaâ€™s private Cloud

Timestamps

(

00:00

) Intro

(

01:26

) Computer science in the 1990s

(

03:01

) Sun and Ciscoâ€™s web dominance

(

05:41

) The Dotcom Boom

(

10:26

) From Boom to Bust

(

15:32

) The innovations of the Bust

(

17:50

) The open source shift

(

22:00

) Oracle moves into Sunâ€™s orbit

(

24:54

) AWS dominance (2010â€“2014)

(

28:15

) How Kubernetes and cloud neutrality

(

30:58

) Custom infrastructure

(

36:10

) Renting the cloud vs. buying hardware

(

45:28

) Designing a computer from first principles

(

50:02

) Why everyone is paid the same salary at Oxide

(

54:14

) Oxideâ€™s software stack

(

58:33

) The evolution of software updates

(

1:02:55

) How Oxide uses AI

(

1:06:05

) The limitations of LLMs

(

1:11:44

) AI use and experimentation at Oxide

(

1:17:45

) Oxideâ€™s diverse teams

(

1:22:44

) Remote work at Oxide

(

1:24:11

) Scaling company values

(

1:27:36

) AIâ€™s impact on the future of engineering

(

1:31:04

) Bryanâ€™s advice for junior engineers

(

1:34:01

) Book recommendations

References

Where to find Bryan Cantrill:

â€¢ X:

https://x.com/bcantrill

â€¢ LinkedIn:

https://www.linkedin.com/in/bryan-cantrill-b6a1

â€¢ Website:

https://bcantrill.dtrace.org

Mentions during the episode:

â€¢ Sun Microsystems:

https://en.wikipedia.org/wiki/Sun_Microsystems

â€¢ Oxide Computer:

https://oxide.computer

â€¢ Linux:

https://www.linux.org

â€¢ Haiku:

https://www.haiku-os.org

â€¢ Gnu Hurd:

https://en.wikipedia.org/wiki/GNU_Hurd

â€¢ Duke Nukem:

https://en.wikipedia.org/wiki/Duke_Nukem

â€¢ Greg Papadopoulos:

https://en.wikipedia.org/wiki/Greg_Papadopoulos

â€¢ Dave Pachecoâ€™s talk, as linked to in Bryanâ€™s blog entry:

https://oxide.computer/blog/systems-software-in-the-large

â€¢ Gilded Age:

https://en.wikipedia.org/wiki/Gilded_Age

â€¢ 1950 ChÃ¢teau dâ€™Yquem:

https://www.wine-searcher.com/find/d+yquem+sauternes+bordeaux+france/1950

â€¢ Pets.com:

https://en.wikipedia.org/wiki/Pets.com

â€¢ Webvan:

https://en.wikipedia.org/wiki/Webvan

â€¢ Jeff Bonwick on LinkedIn:

https://www.linkedin.com/in/jeff-bonwick-80b4b51

â€¢ CFS:

https://en.wikipedia.org/wiki/Clustered_file_system

â€¢ SPARC:

https://en.wikipedia.org/wiki/SPARC

â€¢ X86:

https://en.wikipedia.org/wiki/X86

â€¢ LISA11 - Fork Yeah! The Rise and Development of illumos:

â€¢ Larry Ellison:

https://en.wikipedia.org/wiki/Larry_Ellison

â€¢ Oxideâ€™s blog entry on their $100M Series B:

https://oxide.computer/blog/our-100m-series-b

â€¢ Bryan Cantrill from Joyent on Manta: internet-facing object storage facility that features compute:

â€¢ Jeff Bezos on X:

https://x.com/JeffBezos

â€¢ Kubernetes:

https://kubernetes.io

â€¢ How Kubernetes is Built with Kat Cosgrove:

https://newsletter.pragmaticengineer.com/p/how-kubernetes-is-built-with-kat

â€¢ Craig McLuckie on LinkedIn:

https://www.linkedin.com/in/craigmcluckie

â€¢ Cloud Native Computing Foundation:

https://www.cncf.io

â€¢

The Datacenter as a Computer: Designing Warehouse-Scale Machines

:

https://www.amazon.com/Datacenter-Computer-Designing-Warehouse-Scale-Architecture/dp/303100633X

â€¢ Startups on hard mode: Oxide. Part 1: Hardware:

https://newsletter.pragmaticengineer.com/p/oxide

â€¢ Oxideâ€™s Compensation Model: How is it Going?:

https://oxide.computer/blog/oxides-compensation-model-how-is-it-going

â€¢ Gumroad:

https://gumroad.com

â€¢ Oxide on Github:

https://github.com/oxidecomputer

â€¢ Omicron:

https://github.com/oxidecomputer/omicron

â€¢ OxCon 2025: Update on Update:

â€¢ Intelligence is not Enough | Bryan Cantrill | Monktoberfest 2023:

â€¢ Richard Suttonâ€™s on LLMs as a dead end:

â€¢ Liron reacts to â€œIntelligence Is Not Enoughâ€ by Bryan Cantrill:

â€¢ Smoke test:

https://en.wikipedia.org/wiki/Smoke_testing_(software)

â€¢ Python, Go, Rust, TypeScript and AI with Armin Ronacher:

https://newsletter.pragmaticengineer.com/p/python-go-rust-typescript-and-ai

â€¢ Richard Sutton on X:

https://x.com/RichardSSutton

â€¢ Rust:

https://rust-lang.org

â€¢ Open Source LLMs with Simon Willison:

https://oxide-and-friends.transistor.fm/episodes/open-source-llms-with-simon-willison

â€¢ Founder Mode:

https://paulgraham.com/foundermode.html

â€¢

Oxide and Friends

episode, Reflecting on Founder Mode:

https://oxide-and-friends.transistor.fm/episodes/reflecting-on-founder-mode

â€¢

The Soul of a New Machine

:

https://www.amazon.com/Soul-New-Machine-Tracy-Kidder/dp/0316491977

â€¢

Skunk Works: A Personal Memoir of My Years at Lockheed

:

https://www.amazon.com/Skunk-Works-Personal-Memoir-Lockheed/dp/0316743003

â€¢

Steve Jobs & the Next Big Thing

:

https://www.amazon.com/Steve-Jobs-Next-Big-Thing/dp/0689121350

â€”

Production and marketing by

Pen Name

.",https://newsletter.pragmaticengineer.com
https://newsletter.pragmaticengineer.com/p/how-aws-deals-with-a-major-outage,How AWS deals with a major outage,"What happens when thereâ€™s a massive outage at AWS? A member of AWSâ€™s Incident Response team lifts the lid, after playing a key role in resolving the leading cloud providerâ€™s most recent major outage",2026-01-02T16:54:16.954667,Gergely Orosz,"In October, the largest Amazon Web Services (AWS) region in the world suffered an outage lasting 15 hours, which created a global impact as thousands of sites and apps crashed or degraded â€“ including Amazon.com, Signal, Snapchat, and

others

.

AWS released an incident summary three days later, revealing the outage in us-east-1 was

started by

a failure inside DynamoDBâ€™s DNS system, which then spread to Amazon EC2 and to AWSâ€™s Network Load Balancer. The incident summary overlooked questions such as why it took so long to resolve, and some media coverage sought to fill the gap.

The Register

claimed

that an â€œAmazon brain drain finally sent AWS down the spoutâ€, because some AWS staff who knew the systems inside out had quit the company, and their institutional knowledge was sorely missed.

For more clarity and detail, I went to an internal source at Amazon: Senior Principal Engineer,

Gavin McCullagh

, who was part of the crew which resolved this outage from start to finish. In this article, Gavin shares his insider perspective and some new details about what happened, and we find out how incident response works at the company.

This article is based on Gavinâ€™s account of the incident to me. We cover:

Incident Response team at AWS.

An overview of how global incident response works at the leading cloud provider, and a summary of Gavinâ€™s varied background at AWS.

Mitigating the outage (part 1).

Rapid triage, two simultaneous problems, and extra details on how the DynamoDB outage was eventually resolved.

What caused the outage?

An unlucky, unexpected lock contention across the three DNS enactors started it all. Also, a clever usage of Route 53 as an optimistic locking mechanism.

Oncall tooling & outage coordination.

Amazonâ€™s outage severity scale, tooling used for paging and incident management, and why 3+ parallel calls are often run during a single outage.

Mitigating the outage (part 2).

After the DynamoDB outage was mitigated, the EC2 and Network Load Balancer (NLB) had issues that took hours to resolve.

Post-incident.

The typical ops review process at AWS, and how improvements after a previous major outage in 2023 helped to contain this one.

Improvements and learnings.

Changes that AWS is making to its services, and how the team continues to learn how to handle metastable failures better. Also, thereâ€™s a plan to use formal methods for verification, even for systems like DynamoDBâ€™s DNS services.

Spoiler alert: this outage was

not

caused by a brain drain. In fact, many engineers who originally built the service, DNS Enactor (responsible for updating routes in Amazonâ€™s DNS service) ~3 years ago, are very much still at AWS, and five of them hopped onto the outage call in the dead of night, which likely helped to resolve the outage more quickly.

As it turns out â€“ and as readers of this newsletter likely already know! â€“ operating distributed systems is simply hard, and itâ€™s even harder when several things go wrong at once.

Note: if you work at Amazon, get full access to The Pragmatic Engineer with your corporate email

here

. It includes deepdives like

Inside Amazonâ€™s Engineering Culture

, ones on

Meta

,

Stripe

, and

Google

, and a wide variety of

engineering culture deepdives

.

1. Incident Response team at AWS

Amazon is made up of many different businesses of which the best known are:

Amazon

Retail

: operates the shopping websites

Amazon.com

, and 21 regional versions including

Amazon.co.uk

,

Amazon.de

, and others.

AWS

: everything to do with Amazon Web Services. Retail is a major customer of AWS.

These organizations operate pretty much independently with their own processes and set ups. Processes are similar but not identical, and both groups evolve how they work over time, separately. In this deepdive, our sole focus is AWS, but it could be assumed Retail has some similar functions, like separate Incident Response teams.

Regions and AZs

: AWS is made up of 38 cloud regions. Each one consists of at least 3 Availability Zones (AZs), which are at least one independent data center, connected via a low-latency network.

Note:

â€œ

Regionâ€ and â€œAZâ€ mean slightly different things among cloud providers, as covered in

Three cloud providers, three outages, three different responses.

AWS is built up of regions, and each region has at least 3 AZs. More in

this deepdive

Incident Response

is a dedicated team at AWS, staffed by experienced support and infrastructure engineers, who do the following:

Monitor

all 38 AWS regions, continuously. Health KPIs (Key Performance Indicators) are set up, so itâ€™s easy to tell if a service is healthy or not. These KPIs vary by service, but typical ones include monitoring API success rates and response latencies, as well as synthetic canaries.

Get alerted

when a region becomes unhealthy, or KPIs suggest it might be.

Coordinate

incident calls when an outage happens within a region or AZ.

Oncall rotation 24/7

: someone is always available to respond to an alert.

Interesting detail: the oncall team for Incident Response is distributed across Seattle, Dublin, and Sydney, which is a â€œfollow the sunâ€ rotation, meaning nobody is oncall when itâ€™s nighttime where they are.

Locations of oncall teams in AWSâ€™s Incident Response team

Oncall team members are a mix of senior+ engineers and relatively junior engineers onboarded after training in how to handle alerts and run calls. For significant events like this latest outage, an oncall â€œAWS Call Leaderâ€ is paged automatically. These are more tenured folks, usually principal up to distinguished engineers.

All AWS Service teams and products run their own independent oncall teams.

The AWS Incident Response group is a â€œsafety netâ€ which coordinates large-scale events.

AWS has a â€œyou build it, you run itâ€ mentality: teams have autonomy to decide which service they build, the technologies used, and how they structure it. In return, they are accountable for the service meeting the uptime target; for production services, this means having an active oncall.

We cover more about oncall practices in

Healthy oncall practices,

and detailed compensation philosophies in the

Oncall compensation

article.

Gavinâ€™s background at AWS

For this article, Gavin McCullagh shared the story of how the incident unfolded from his point of view. Gavin joined Amazonâ€™s Dublin office in 2011 and worked for many years as a DNS and Load Balancing Specialist, including working on Route 53 Public DNS, and Virtual Private Cloud (VPC) DNS Resolver. Nowadays, Gavin works in a team focused on Incident Response and Resilience for AWS services and customer applications. This includes the AWS Incident Response team, and services like Application Recovery Controller and Fault Injection Service.

Since his early days there, Gavin has been a regular in large-scale incident calls as his first team â€“ Load Balancing â€“ was regularly paged into major incidents because it is central to AWS, and telemetry from load balancers can greatly help with debugging.

2. Mitigating the outage (part 1)

All times below are in Pacific time.

19 Oct, 11:48 PM:

the incident begins and regional health indicators degrade, triggering an alert. Minutes later, AWS Incident Response oncall is paged. The AWS call leader also joins the call. Gavin joins despite not actually being oncall because he figures he might be able to help â€“ which turned out to be a good hunch.

Rapid triage

Triage starts immediately. At this point, 100+ services were showing problems within us-east-1 due to the broad nature of the outage. From experience, the Incident Response Team has learned that it pays to start systematically debugging common layers of the stack. For such a broad outage, itâ€™s usually a core building block with an issue.

Below is a rough checklist of what the team works through:

Systematic investigations begin at the bottom of the stack

The Incident Response team begins by going through a checklist in roughly this order:

Investigate whether there are power failures or significant network failures

Check if the software-defined network and load balancing layers are healthy

Look for issues within DNS and authentication/ authorization services

Confirm that core services are healthy which most AWS products build on; like KMS, DynamoDB and S3

Two problems at once

As the team went through the list, two coincidental issues were uncovered within a minute of each other.

Sunday, 19 Oct, 11:48 PM: a networking failure event starts.

Network packet loss is detected in the â€œborderâ€ network which connects the us-east-1-az1 availability zone to the wider AWS backbone and internet. A Network Border Group is a unique group from which AWS advertises public IP addresses.

11:48 PM: DynamoDB degradation starts.

As packet loss is being detected, DynamoDB also starts to degrade.

11:52 PM:

AWS Incident Response automatically detects an impact upon services in us-east-1, and pages the incident response oncall network engineers, and affected service oncalls.

11:54 PM: Automated Triage suggests Networking.

Incident responseâ€™s automated metric triage system completes, noting a) a change in network performance (increase in packet loss) correlated with the time of the event start, and b) that DynamoDBâ€™s metrics have changed at the same time.

Typically

, this would suggest a network issue is the cause of the DynamoDB issue.

Monday, 20 Oct, 00:08 AM: after failover to a secondary conference system (more later), thereâ€™s an investigation into the networking event.

As the network event is â€œlower in the stackâ€ and initially appears to explain the impact on other services (including DynamoDB), the team focuses on fixing the networking issue.

00:26 AM: Red herring.

While the Incident Response team is busy resolving the networking failure in the AZ, someone posts in the incident Slack channel:

â€œHang on, why is DynamoDB not resolving in DNS, packet loss would not cause that?â€

This is when the Incident Response team realizes that while the networking event has

some

impact, something

bigger

is also happening thatâ€™s not caused by the outage. Whatever it is, itâ€™s impacting DynamoDB.

This smaller network outage is lower in the stack and started a few seconds before the first DynamoDB alerts, which led the experienced team off track into mitigating the smaller outage.

20 Oct, 00:27am: the call is divided.

The DynamoDB investigation is split into its own call, so two technical calls run in parallel:

DynamoDB outage: the focus of the dedicated call

Networking outage: resolving the â€œsmallerâ€ outage continues

Resolving the DynamoDB outage

To bring more senior leaders into the call, Gavin messages three longtime DynamoDB frontend engineers, and asks Incident Response to page them, too.

00:31am: realization that itâ€™s a DNS issue.

Using the ubiquitous â€œdigâ€ tool,

the team sees that something is wrong with DynamoDBâ€™s DNS records.

00:35-00:50am: engineers who designed and authored the DNS Enactor service join.

The root cause of the issue with DynamoDB ended up being identified as a race condition within a DNS Enactor service, as

previously covered

.

A race condition in DNS Enactor services ended up causing the outage

The quorum (collective) of seven DynamoDB engineers includes folks who built and authored a good part of the DNS Enactor and related services, and who have a combined Amazon tenure of over 50 years. With domain experts on the call, the incident responders let them figure out what has gone wrong with DNS Enactors, while looking for ways to mitigate faster.

As the team will later learn, the cause of the event is an edge case bug in how the three Enactors interact, as covered below.

Hitting the â€œautomation paradox.â€

The DynamoDB team has built a system with DNS Enactors that automatically does a lot of complicated work to keep their DNS updated. But now that itâ€™s gone wrong, itâ€™s time to manually fix the DNS records.

Hereâ€™s where the automation paradox kicked in: the team had never had to manually overwrite the DNS zone files before, as they had a system that could reliably do this! So, it took some time to package up and deploy the change to DNS records via manual intervention.

01:03 AM: a parallel mitigation, before the â€œfull fix.â€

The problem is that the DNS records for DynamoDB are empty due to the race condition. The first order of business is to restore

something that works and relieves customers

â€“ even before understanding exactly how things went wrong.

The incident response team prepares a â€œquickâ€ partial mitigation: within the internal private network, call us-east-1 â€œ

Prod

â€ (yes, this really is the name!) and apply an Response Policy Zone (RPZ) override on the DNS resolvers to force the DynamoDB IPs into place, for only this internal network. As it hosts many AWS services, this will help restore services like Identity and Access Management (

IAM

) and Secure Token Service (

STS

).

01:15 AM: mitigation.

The incident team performs an override to force DynamoDB IPs into the us-east-1 Prod DNS on DynamoDBâ€™s most common domain name. Then, the incident team performs a second override of DynamoDB IPs on DynamoDBâ€™s other domain names. At this point, services like IAM, STS, and SQS in us-east-1 recover.

02:15 AM: fix for public DynamoDB DNS found and applied.

The team figures out how to fix the public DNS records for DynamoDB, and applies this to restore DynamoDB for all customers.

The engineers look through the zone and realize the top-level alias record for the DynamoDB domain is pointing at a non-existent tree of records (the old â€œplanâ€ was deleted). They also note that a set of backup trees which the system maintains are still present. As a result, their task is to stop the automation system to prevent it from interfering, delete the broken alias record, and create a replacement that points at the rollback tree.

02:25 AM: DynamoDB recovered.

The first part of the incident is resolved. However, Amazon EC2 continues to have issues, and Network Load Balancer (NLB) would have problems later that night.

3. What actually caused the outage?

AWS runs three independent DNS Enactors, and each takes DNS plans and updates Route 53. DNS Enactors are

eventually consistent:

meaning that the DNS plans in any single Enactor will eventually be consistent with the source of truth. This is unlike strongly consistent systems where you can be certain that after writing something, the whole system is consistent.

Using an

optimistic locking

mechanism

is

a common solution to prevent multiple writers from changing the same record. The AWS team wanted to achieve this: have only one Enactor writing to Route 53 at any single given time.

You need to have some kind of key-value database to implement optimistic locking. For example, you could use DynamoDB for this:

DynamoDB could be an optimistic locking mechanism â€“ but itâ€™s not what the AWS team decided to do

The problem with this approach would have been that DynamoDB itself depends on DNS Enactors, so this would create a circular dependency:

Introducing circular dependencies is risky, and the AWS team chose not to do so

Instead, the AWS team solved the locking problem in a clever way by using Route 53 as the database for optimistic locking.

As Senior Principal Engineer Craig Howard

explains

:

â€œWe didnâ€™t want to add any other dependencies [to DNS Enactors]. Right now, all we depend on is the DNS Plan, and Route 53 (Amazonâ€™s DNS service.) Can we use Route 53 to create a lock? Well, you can!

You create a TXT record, and the existence of that TXT record indicates who the holder of the lock is. The value of the text record is the host name of the lock holder, and then you put the

epoch

[length of time] in seconds for how long this lock is alive for.â€

The AWS team used the transactional characteristic of Route 53: the Route 53 control plane ensures that â€œCREATEâ€ and â€œDELETEâ€ operations for TXT are transactional â€“ they either succeed or fail.

So, this is how a lock is created:

CREATE â€œlock.example.comâ€ TXT â€œ<hostname> <epoch>â€

As lock creation is transactional, whenever a DNS Enactor wants to create a

new

lock for its own use, it first deletes the old lock because it can be certain that lock creation /deletion is transactional. Then it creates the new lock:

DELETE â€œlock.example.comâ€ TXT â€œ<hostname> <old epoch>â€

CREATE â€œlock.example.comâ€ TXT â€œ<hostname> <new epoch>â€

// Insert updated Route 53 DNS records

Lock contention is what started the outage.

In this case, one Enactor was very unlucky and failed to gain a lock several times in succession. At that point, the plan it is trying to install is older than anyone had ever envisioned. By the time this â€œunluckyâ€ Enactor gains the lock, an up-to-date Enactor takes over and its â€œrecord clean-upâ€ workflow detects a

very

old plan and deletes it. This was an older plan than the team expected.

Unexpected lock contention: one unlucky Enactor failed to get the lock for ages. When it did, its DNS plan was further behind than expected. Source:

Craig Howard / AWS

Craig explains how this strange race condition played out

in this re:Invent talk

, shared earlier this week.

What started the outage?

As the team will later learn, the cause of the event is an edge case bug in how the three Enactors interact. They are designed to operate independently for resilience, with each Enactor optimistically taking a lock on the DNS zone, making their changes, and releasing the lock. If an Enactor fails to obtain a lock, it backs off and then tries again later.

4. Oncall tooling & outage coordination at Amazon

Letâ€™s hit pause on the incident mitigation to look into some unique parts of Amazonâ€™s oncall process.",https://newsletter.pragmaticengineer.com
https://newsletter.pragmaticengineer.com/p/frictionless-why-great-developer,Frictionless: why great developer experience can help teams win in the â€˜AI ageâ€™,"Exclusive excerpts from the newly-released book â€˜Frictionlessâ€™, by Nicole Forsgren and Abi Noda.",2026-01-02T16:54:19.699640,Gergely Orosz,"Hi, this is Gergely with a bonus, free issue of the Pragmatic Engineer Newsletter. In every issue, I cover Big Tech and startups through the lens of senior engineers and engineering leaders. If youâ€™ve been forwarded this email, you can

subscribe here

.

Begin researching developer productivity, and youâ€™re likely to soon encounter the work of Nicole Forsgren, as lead author of the best-selling book

Accelerate

, and creator of the widely-adopted

DORA

and

SPACE

developer productivity frameworks. And, beyond SPACE is the

DevEx framework

for productivity measurement, created by Abi Noda in collaboration with Nicole.

When I heard that the pairâ€™s work together has continued in a new book about developer experience, I wanted to know more, and had the chance to sit down for a conversation about this big project of theirs.

Frictionless

was released two weeks ago, and Nicole and Noda have been nice enough to share an excerpt from their new title with readers of the Pragmatic Engineer, below.

My copy of

Frictionless

The back cover of

Frictionless

reads:

â€œ

AI can generate code in minutes â€” so why does shipping software still take forever?

The answer is friction: the invisible barriers that turn quick wins into endless delays. While your competitors ship daily updates, your developers burn out fighting broken tools instead of solving real problemsâ€.

Personally, the lack of impact by AI coding tools to date upon the speed of development and quality of software, is amusing to me. If I take the apps I use, websites I visit, and services I access via software, I donâ€™t see signs of faster iteration, higher quality, and fewer bugs since AI started shaking up the tech industry. An exception might

be AI labs that release new features at eye-catching speed, often faster than much nimbler startups, but it could be argued those begin as experimental features.

What I like about

Frictionless

is that it gives a step-by-step approach for

developers

to smooth their own workflows and be more productive. If it helps teams and companies to move faster and produce more and better software â€“ this is a win for everyone!

This article begins with our conversation, followed by two excerpts from the new book. We cover:

How

Frictionless

relates to Accelerate, DORA, and SPACE.

An obvious question, but I had to ask.

Writing process.

Frictionless took more than 3 years to write â€“ including a â€œfull rewriteâ€

Making the business case to reduce developer friction.

Excerpt from the new book, chapter 9.

How AI changes (and doesnâ€™t change) the metrics you need.

Chapter 59 of the book.

To hear more about developer productivity from Nicole, see

this podcast episode with her in The Pragmatic Engineer Podcast.

My usual disclaimer: As with all my recommendations, I was not paid to recommend this book, and none of the links are affiliates. See

my ethics statement

for more details.

1. How

Frictionless

relates to Accelerate, DORA, and SPACE

When developer productivity is discussed, your book

Accelerate

often gets a mention. How does Frictionless relate to it â€“ and what makes it different?

Nicole:

â€œGreat question! The two are complementary but different in their foundations.

Accelerate

distills years of research connecting software delivery performance to business outcomesâ€”it tells you what matters and why, but leaves the how to the reader.

Frictionless

is built from years of hands-on work advising teams on DevEx improvements, and itâ€™s designed to help you actually do the work.

Where

Accelerate

is research-backed insights,

Frictionless

is a practitionerâ€™s guide. It gives you a seven-step process you can start wherever you are, plus organizational change frameworks, metrics guidance, and 100+ pages of workbooks. Many of the practices are informed by research, but the book is ultimately about what weâ€™ve seen work in the real world when teams try to improve developer experience.

So, if

Accelerate

is the â€˜why,â€™

Frictionless

is the â€˜howâ€™â€.

DORA, SPACE and this book

To what extent does the new book build upon the DORA and SPACE frameworks?

Nicole:

â€œTheyâ€™re all deeply connected, Frictionless is really the natural evolution of this work.

DORA and

Accelerate

was about improving software delivery through technical, cultural, and process changes. Yes, we focused on business outcomes, but a core goal was always to make work better for engineers.

Frictionless

extends that same idea: start by identifying inefficiencies in the system, talk to developers about where their work is difficult, and you can improve both business outcomes and developer experience.

SPACE gave us a framework for measuring complex work like software development. Measurement is critical to any DevEx initiative, and we dedicate a full chapter to how AI is changing what we need to measure and how we should think about metrics differently.

But hereâ€™s the key difference:

Frictionless

goes beyond measurement. Metrics alone donâ€™t create change. You also need to sell the initiative to leadership, advocate for change across your organization, plan communications, manage organizational changeâ€”all the messy, human work of actually making improvements happen. Thatâ€™s what this book is designed to help withâ€.

2. Writing Process

Itâ€™s always interesting to learn what goes into writing a book. What made you both decide to take on this project?

Nicole:

â€œThis is a great question, since I didnâ€™t think I would write

another

book. However, I realized a couple of things after talking to teams and leaders across many companies and industries: there was increased attention upon, and a push to improve, developer experience. Also, many questions and challenges were very similar: how do I start, how do I prioritize, and how do I convince the business that DevEx is important?â€

Abi:

â€œOver the past several years, Iâ€™ve worked hands-on with Developer Productivity (DevProd) leaders at hundreds of organizations. The same questions and challenges came up repeatedly, so a book felt inevitable. When Nicole reached out about teaming up in 2022, it was a no-brainer to finally make it happen. As a fun fact, I bought the domain developerexperiencebook.com in February 2021 â€“ so about a year before we started writing the book!â€

How long did the writing take?

Nicole:

â€œThe whole process took about 3.5 years, but it wasnâ€™t linear. I had a false start where Iâ€™d written several thousand words and realized it was way too pedantic; basically a research book with chapters explaining methods, but nothing immediately actionable. I scrapped it and started over.

This was the point when I reached out to Abi. Weâ€™d written a couple of papers together before, and his work at DX was giving him incredible insights into the key challenges teams face, and what actually works to improve DevEx. The combination is perfect: I brought the frameworks and experience from working with teams, and he brought the frontline practitioner perspective and patterns he was seeing across hundreds of companies.

Together, we built something practical: the step-by-step process, the workbooks, the real-world examples. Much better than my first attempt!â€

Where did you get inspiration for the practices shared in the book?

Nicole:

â€œHonestly, it all comes from actual work: collaborating or advising actual teams and actual DevEx improvement projectsâ€.

Abi:

â€œAs Nicole mentioned, we didnâ€™t go reaching for ideas or conduct research specifically for the book. Everything we put into it comes directly from our existing research and practical experienceâ€.

What are your favorite parts of the book, and do any details stick out in particular?

Nicole:

â€œI love the section on treating metrics like a product. But if I had to pick one story, itâ€™s how Dave Anderson was able to drive improvements at Amazon. What made it stick with me was how he could influence real change without authority by sharing a dashboardâ€.

Note from Gergely: longtime newsletter readers and podcast listeners may find Dave Andersonâ€™s name familiar: he authored the popular guest post,

A Day in the Life of a Senior Manager at Amazon

, and has been a guest on the

Pragmatic Engineer podcast

.

Abi:

â€œTranslating developer experience into business terms (Chapter 9) is a topic thatâ€™s especially meaningful to me. Developers â€“ and most engineering leaders â€“ instinctively understand how essential a strong developer experience is. But helping non-technical leaders see its strategic importance can be challenging.

In the chapter, we highlight the story of Mike Fisher, former CTO of Etsy, who led a transformative developer-experience initiative that not only accelerated engineering teams, but was backed by hard, quantifiable ROIâ€.

What has the feedback about the book been like, so far?

Nicole:

â€œItâ€™s been really positive, overall. A lot of folks are finding ways to get unstuck, or engage with people at work in new ways. Iâ€™ve had a few folks insist the AI transformation will change everything, but the core of the book is really about how we build software, how to spot inefficiencies, and how to effect change.

And, as many of us are seeing, AI amplifies everything. Sometimes, itâ€™s how much we can prototype and experiment, and other times, itâ€™s our friction points and bottlenecks. So, while workflows may change, the fundamental importance of reducing friction doesnâ€™tâ€.

Abi:

â€œLike shipping a product, putting a book out into the world is exciting and also nerve wracking! Iâ€™ve been grateful for all the support weâ€™ve been receiving, as well as feedback.

I think this book lands at a critical moment for the industry. AI is creating enormous excitement and rapid change, and that makes the fundamentals of measurement and developer experience more important than everâ€.

Frictionless

is organized into five parts:

Part I: Understanding DevEx

Part II: The three essential parts of DevEx (feedback loops, flow state, cognitive load)

Part III: Making the business case [to invest in DevEx]

Part IV: Improving DevEx: a 7-step process

Part V: Evolving and sustaining DevEx

With that, letâ€™s jump into two full chapters of the book, from parts III and V.

The excerpts below are from Frictionless, by Nicole Forsgren and Abi Noda. Copyright Â© 2025 by Nicole Forsgren, Abi Noda. Published by Shift Key Press. Used with permission.

3. Making the business case to reduce developer friction

Chapter 9 from Part III: Making the Business Case.

Understanding developer experience and its business impact is just the beginning. The real challenge is securing the resources and organizational support needed to make meaningful improvements. Most DevEx initiatives fail not because of poor technical execution, but because they canâ€™t effectively communicate their value to decision-makers.

In this part, we explore how successful teams have translated developer pain points into compelling business cases that resonate with executives. Youâ€™ll learn how to connect DevEx improvements to financial outcomes, align with existing strategic priorities, and position your work as a solution to leadershipâ€™s most pressing concerns. These approaches are designed to be durableâ€”while the specific tools and technologies you advocate for will evolve with AI and other innovations, the fundamental methods for investigating problems, gathering data, and communicating business value remain constant. Whether youâ€™re seeking initial funding for DevEx improvements or defending existing investments, these chapters will show you proven approaches for building leadership support.

Translate developer experience into business value

C-suite executives speak the language of business outcomes.

While â€œimproving developer satisfactionâ€ or reducing lead timesâ€ might resonate with engineering leaders, executive decision-makers need to see clear connections to financial impact. Business leaders are evaluated on metrics like revenue growth, profitability, and market shareâ€”so your DevEx proposals must connect directly to these financial outcomes.

This doesnâ€™t mean executives donâ€™t care about developer well-being. Rather, they need to understand how developer experience investments translate to business results. This financial framing is crucial when selling your strategy (Step 5) and demonstrating value (Step 7).

Talk about

recovering time

: Convert developer hours into dollar value.

Developer time has clear financial value. You can:

Measure hours spent on unnecessary toil or waiting for builds/tests.

Multiply this time by the fully loaded hourly cost of developers.

Present these savings as â€œrecaptured productivity dollarsâ€ or â€œfree headcount.â€

For example, if your team of 50 developers each wastes 5 hours weekly on build-related delays at a fully loaded cost of $100/hour, thatâ€™s $25,000 in weekly productivity lossesâ€”or $1.3 million annually.

Alternatively, express this as â€œfree headcount,â€ for example, 50 developers Ã— 5 hours lost weekly = 250 total hours wasted. Based on a 40-hour workweek, thatâ€™s equivalent to gaining 6.25 additional developers without increasing your budget. This framing resonates particularly well when hiring freezes are in place or during tight labor markets. To safeguard your credibility, be sure to actually measure real, credible time savings.

The financial impact is substantial. When teams improve developer experience, those benefits compound. A $250,000 developer who recovers just 10 percent of their time by avoiding tooling friction represents $25,000 in reclaimed value. Multiply that across an entire engineering organization, and itâ€™s equivalent to dozens or hundreds of â€œfreeâ€ developers.

But the benefits donâ€™t stop thereâ€”faster toolchains enable organizations to build, experiment, and iterate more rapidly, creating business value that can climb into the millions. And thatâ€™s before accounting for the compounding effects on delivery timelines, team morale, and customer satisfaction.

Etsyâ€™s $2.5M DevEx investment: A business case in action

As Etsyâ€™s engineering team rapidly grew from 250 to nearly 1,000 developers, leadership recognized that productivity would suffer without investment in developer experience.

CTO Mike Fisherâ€™s approach to securing buy-in was straightforward: Translate improvements into business language. Instead of explaining technical changes to deployment processes, he told executives the work would save 50 percent of engineering hours spent waiting for deploymentsâ€”equivalent to a quarter of a full-time engineerâ€™s salary. The result was an initiative that dedicated 20 percent of engineering capacity to improve DevEx in four key areas: Crafting products, developing and deploying, building with data, and reducing toil.

The initiative

, which took 18 months, was so successful that it led to ongoing investment in developer-experience work, integrated into Etsyâ€™s standard operations. Their key insight: Scale people, process, and technology together, or productivity suffers regardless of growth.

Talk about

saving money

: Quantify cost savings

. Improved developer experience delivers tangible cost savings that directly impact your financial statements. By enhancing automation, streamlining workflows, and eliminating redundancies, you can substantially reduce operational expenses.

For example, strategic DevEx investments can:

Decrease vendor costs through process optimization and tool consolidation.

Reduce cloud computing expenses by improving test efficiency (eliminating redundant tests).

Lower infrastructure costs through optimized build processes that require less processing power.

Minimize expensive production incidents through better quality controls.

When making your business case, focus on these hard cost savings with specific projections whenever possible. This includes both the immediate savings from reduced delivery costsâ€”as developers ship features faster with less frictionâ€”and the long-term savings from improved talent retention. Executives respond strongly to initiatives that demonstrate clear paths to expense reduction while maintaining or improving output quality.

Strategic standardization: How Block delivered millions in savings

Block is a leading fintech company made up of brands like Square, CashApp, and Tidal. Rather than pursuing standardization for its own sake, they centered their DevEx strategy on consolidating tools to create well-supported golden paths for developers.

When making their business case, Blockâ€™s team could have focused on abstract benefits such as developer satisfaction or code throughput. Instead, they focused on the dollar-impact that engineering friction and avoidable reliability incidents were having on the business..

The business-focused approach paid off. Their developer experience initiative was funded and delivered impressive results in 12 months. Thanks to investments in better developer environments, golden paths, and AI tooling, they saw millions of dollars in documented savings and double-digit improvements in developer satisfaction scores.

Talk about

making money

: Accelerating revenue through DevEx.

While developer experience initiatives arenâ€™t direct revenue generators, they significantly accelerate your existing revenue streams. For software-driven companies, revenue growth depends on five key factors:

Feature velocity (how quickly new capabilities reach customers).

Feature quality (how well new capabilities meet customer needs).

Experimentation speed (how quickly you can test and iterate on new ideas).

Service reliability (how consistently your products perform).

Security posture (how well you protect customer data and trust).

All five factors depend on effective developer workflows. Consider the AI transformation, where companies racing to integrate AI capabilities into their products are pursuing increased revenue, competitive advantage, and expanded market share. However, these AI features must be:

Designed and coded.

Reviewed for quality.

Built and tested.

Integrated into existing systems and products.

Securely deployed.

Maintained and improved.

When developers wrestle with suboptimal tools and fragmented processes, these technical bottlenecks become business problems. Your company faces delayed market entry, compromised quality, and security vulnerabilitiesâ€”all directly hitting revenue.

Consider this: If competitors release AI features quarterly while your team needs six months due to workflow inefficiencies, youâ€™re not just accumulating technical debt, youâ€™re experiencing actual revenue losses and diminishing market relevance. Smart DevEx investments create a multiplier effect where your existing teams deliver more value faster, enhancing revenue streams and capturing market opportunities before competitors do.

Accelerating revenue growth: Capital Oneâ€™s DevEx transformation

Capital One dramatically transformed their market position by strategically investing in developer experience and cloud infrastructure.

Before their DevEx overhaul, launching new financial products took 6-9 months, limiting the companyâ€™s competitive agility. By standardizing development environments, implementing modern CI/CD pipelines, and embracing cloud-native technologies, Capital One slashed delivery times from months to weeks or days.

The business impact was fast and measurable. Faster deployment cycles meant Capital One could capture market opportunities before competitors, respond quickly to customer needs, and optimize revenue-generating features through rapid iteration.

Capital One secured executive buy-in by directly connecting their DevEx investments to accelerated time-to-market and customer acquisitionâ€”metrics that translate directly to revenue growth. Their story demonstrates how improving developer workflows isnâ€™t just a technical winâ€”itâ€™s a powerful revenue accelerator.

Talk about

proven correlations

: Link technical metrics to business outcomes

. Sometimes the most powerful business cases come from identifying correlations between technical metrics and business outcomes.

In

How to Measure Anything

, Douglas Hubbard writes, â€œIf you can correlate two things to each other, and then if you can correlate one of them to money, then you can express them both in terms of money.â€

At eBay, a breakthrough analysis showed that every 10 millisecond improvement in page speed translated to millions of dollars in increased web purchases. This correlation transformed how the business viewed page speedâ€”from a technical concern to a critical revenue driver deserving investment.

A specific example of this approach is the developer experience index (DXI), developed by DX. The DXI score is the average of 14 survey questions that ask developers about different aspects of their development process. By correlating this score against self-reported time loss, DX has found that each one-point gain in DXI score translates to saving 13 minutes per week per developer, equivalent to

10 hours annually

. This correlation allows organizations to predict financial impact from DevEx improvements.

You can apply this same approach to developer experience:

Correlate developer satisfaction scores with reduced time-to-market.

Link build system reliability to fewer production incidents.

Connect onboarding experience improvements to faster new-hire productivity.

However, be cautious of trying to tie a developer productivity metric straight to revenueâ€”a common mistake in this space. DevEx leaders attempt this, they donâ€™t know how to get started, and then they give up trying to convince their management of the value of their work.

Storytelling matters more than the exact numbers

When building your DevEx business case, perfect is the enemy of good. Many leaders get stuck trying to calculate perfectly precise ROI figures or collect perfect data. This perfectionism is rarely necessary.

In most situations, reasonably credible data is sufficientâ€”what really matters is how you weave that data into a compelling narrative. While data points make good headlines or proof points, they need a clear, concise story to be meaningful to decision-makers.

Remember: Even the most impressive numbers rarely speak for themselves. Your presentation skills, narrative structure, and understanding of your audienceâ€™s priorities will ultimately determine whether your business case succeeds. We discuss this more in Step 5: Sell Your Strategy.

4. How AI changes (and doesnâ€™t change) the metrics you need

The excerpt below is Chapter 59 from Part V: Evolving and Sustaining DevEx

AI changes what developers do every day, so your metrics need to change too.

The good news? The measurement fundamentals you just learned still apply. As AI tools become woven into daily development work, youâ€™re probably wondering: â€œDo I need to throw out everything I know about measuring developer experience?â€ The short answer is noâ€”but the longer answer is more interesting.

The fundamentals still matter.

The SPACE framework and DORA metrics remain relevant because developers still need fast builds, reliable deployments, good documentation, and strong collaborationâ€”and AI often amplifies the pain of bad foundations rather than masking them. Flow and friction havenâ€™t gone anywhere: Developers still need focus, and slow pipelines, flaky tests, and unclear ownership still break it. And while AI shifts what developers do, it doesnâ€™t change their need for clarity, autonomy, mastery, and purpose.

Applying SPACE to AI-Augmented Development

In Chapter 58, we introduced the SPACE framework as a way to balance your metrics across five dimensions. That framework remains just as valuable when measuring AI-augmented workflowsâ€”you just need to ask slightly different questions:

Satisfaction

: How do developers feel about AI tools? Do the tools improve or frustrate their work experience? Are they satisfied with the quality of AI suggestions?

Performance

: Are outcomes improving with AI assistance? Track defect rates, feature completion times, and time-to-resolution for incidents.

Activity

: What are the AI usage patterns? Monitor suggestion acceptance rates, what types of work get delegated to AI, and where AI gets used most (tests, docs, debugging).

Communication

: How does AI affect collaboration? Are code reviews shifting from syntax to architecture? Are developers asking AI instead of peers? Is documentation improving or declining?

Efficiency

: Where does AI save time versus create overhead? Measure workflow changes, time saved on routine tasks, and friction from context gaps or validation effort.

Consider adding a sixth dimension: Trust. How much do developers trust AI-generated code, comments, and recommendations? This affects both adoption patterns and the value teams ultimately derive from AI tools.

AI doesnâ€™t just speed up existing workâ€”it changes workflows

. Developers arenâ€™t just writing code; theyâ€™re reviewing, prompting, and steering AI. And hereâ€™s what most people miss: Most AI usage involves reasoning about information rather than generating it. Developers spend more time using AI to write summaries, compare alternatives, conduct smart searches, and synthesize research than generating code. This means existing metrics that capture time spent coding or lines of code written miss the bulk of AI-augmented workâ€”the time spent crafting prompts, reviewing suggestions, and making decisions based on AI synthesis.

New metrics can help answer new, emerging questions. Track prompting efficiencyâ€”how many attempts developers need before getting a useful AI suggestion. Measure validation effortâ€”the time spent reviewing and correcting AI-generated code, which reveals whether AI truly saves time or just shifts where developers spend it. And monitor trust calibrationâ€”when developers trust AI too much (shipping bugs) versus too little (wasting time double-checking correct code). These metrics reveal friction that traditional measures like commit frequency or lines changed simply canâ€™t capture.

AI usage telemetry can reveal important patterns and changes in workflows. Capture what types of work developers do themselves versus what they hand off to AI agentsâ€”this shows where AI is valuable and may reveal new system blockers. Deep insights also come from watching how AI reshapes team dynamics: Are engineers asking AI instead of peers for help? Are code reviews shifting from syntax nitpicks to architecture discussions? Are new developers getting up to speed faster because AI explains your internal codebase?

Most importantly, collect this data thoughtfully. AI instrumentation typically involves much more detail than other methods. Aggregate at the team level rather than tracking individuals, be transparent about what youâ€™re measuring and why, and work with HR and legal partners early to establish clear privacy boundaries. The goal is to improve developer experience, not create surveillance.

Your metrics product also needs to adapt. Create segmented views that compare AI and non-AI performance. Build feedback loops that reveal where AI adds the most value. And introduce visualizations that track how AI affects collaboration patterns.

Use AI to build your AI metrics

Thereâ€™s a helpful irony here: While AI changes what you need to measure, it can also help you build that measurement infrastructure faster.

Use AI coding assistants to:

Generate data pipeline code for collecting AI telemetry

Create dashboard prototypes and visualizations

Write scripts to analyze prompt patterns and acceptance rates

Build data transformation logic for aggregating team-level metrics

What once required weeks of custom development can now be scaffolded in hours. Just remember to validate AI-generated code carefullyâ€”especially for instrumentation that feeds critical decisions. And leverage AI for the scaffolding and boilerplate, while keeping human expertise focused on the measurement strategy and interpretation.

Mixed methods become even more important with AI

. Logs tell you what AI suggested and what got accepted, but only surveys and interviews tell you why. Your telemetry might show developers rejecting 80% of AI suggestions for authentication codeâ€”but you need to talk to developers to learn whether thatâ€™s because AI hallucinates security vulnerabilities, doesnâ€™t understand your internal frameworks, or because developers donâ€™t trust it for security-critical work. The â€œwhatâ€ reveals patterns; the â€œwhyâ€ reveals the root cause and tells you what to fix.

The shift from output to outcomes becomes critical.

AI makes it painfully obvious that lines of code was always a bad metricâ€”but more importantly, it forces us to connect our metrics to what actually matters. Developer productivity isnâ€™t the goal; faster customer value delivery is. That means measuring problem-solving speed (time from idea to working solution), breadth of exploration (how many approaches developers can test), and cognitive load (whether AI helps or overwhelms). These outcome-focused metrics tell you what actually moves the needle on business value, not just code volume.

Behind the scenes, your infrastructure needs to evolveâ€”and not just for developer workflows.

Traditional telemetry doesnâ€™t capture AI interaction loops, so you need new instrumentation for prompting workflows, completions, and accept/reject events. These real-time metrics reveal patterns and friction that retrospective analysis can miss.

But donâ€™t stop thereâ€”instrument agentic workflows too. Tracking what agents do reveals which tasks are best suited for automation and which require human judgment. It also shows how delegation to agents affects developersâ€™ mental models of the system: Does offloading routine tasks free developers to think architecturally, or does it create blind spots where they lose understanding of how things work?

Start by auditing your existing metrics through an AI lens.

Ask: â€œDoes this metric still capture what we care about?â€ and â€œWhat important tasks, workflows, and feedback loops does our data miss?â€ Donâ€™t throw everything out and donâ€™t start over from scratchâ€”add data that fills important gaps and helps you answer new questions.

Your metrics strategy should also anticipate questions stakeholders will ask about AI adoption: â€œWhere is AI providing the most value?â€ â€œAre we seeing quality trade-offs?â€ â€œWhich teams are benefiting most?â€ Your metrics should answer these, not just operational velocity questions. Having these insights ready builds confidence in your AI implementation strategy while helping teams make better decisions about where and how to leverage these tools.

The bottom line: AI changes what you measure, but not why you measure it.

Youâ€™re still trying to understand and improve developer experienceâ€”the SPACE framework still applies, as shown in the callout above. The fundamentals remain sound; you just need new instruments to capture the new reality.

End of excerpt.

Takeaways

Gergely, again.

Thanks to Nicole and Abi for talking with me, and sharing excerpts from

Frictionless

. You can buy the book

here

, and

download a free DevEx workbook

that accompanies the title.

Get the book

Before AI tools started to spread, larger tech companies invested in developer experience. Amazon calls this area â€œbuilder experienceâ€, Spotify uses â€œR&D experienceâ€, and other places use terms like â€œengineering experienceâ€, or â€œmaker experienceâ€ as per Frictionless.

With AI coding tools in use, there are expectations that productivity should increase for us devs â€“ basically; assuming these AI tools are any good, products should be built faster, and iteration speed should improve. But we seem to be going back to basics in figuring out why this happens at some companies, but not at others.

Engineering leaders finally have a case for the resources of time and money to reduce/remove processes, improve tooling, and make developer experience better at both small and large teams.

I like Frictionless because it doesnâ€™t talk about how to use AI differently or â€œbetterâ€: instead, it provides suggestions about removing annoying friction from devsâ€™ day-to-day lives.

Thanks, Nicole and Abi for writing this book; hopefully it makes the case easier for reducing developer friction at your workplace, too!

For more reading on developer productivity, check out these deepdives:

How tech companies measure the impact of AI on software development

Measuring developer productivity: 17 real-world examples

. Examples from Google, LinkedIn, Peloton and other companies

How Uber measures developer productivity

Measuring engineering efficiency at LinkedIn

DevEx framework: an overview",https://newsletter.pragmaticengineer.com
https://newsletter.pragmaticengineer.com/p/mcp-deepdive,Building MCP servers in the real world,"How engineers and teams use MCP servers: from debugging to working with legacy systems, & giving non-devs more access. Details from 40+ devs â€“ with some surprises",2026-01-02T16:54:20.808865,Gergely Orosz,"The

Model Context Protocol (MCP)

was released almost exactly a year ago by Anthropic, and today, MCP is enjoying quite a moment, with strong growth in the numbers of devs building MCP servers. That might be related to MCP servers being a great way to give agents like Claude Code, Cursor Agent, and other LLMs new capabilities to use services, query documentation, and be more efficient. Adoption is widespread and diverse, across cutting-edge startups and regulated industries like aerospace alike.

One year on, how are engineering teams using this technology, and what does that teach us? To find out, we collected input from 46 software engineers who build and use MCP servers at work, and talked with

Jeremiah Lowin

, CEO of Prefect and creator of FastMCP, the leading MCP framework for Python, and

Den Delimarsky

, core MCP maintainer and Principal Engineer at Microsoft.

Thanks to everyone who shared their experience of building with MCP.

Today, we cover:

MCP fundamentals.

Brief recap of the protocol.

Usage realities.

Internal MCP server usage outpaces its public usage, business stakeholders are heavy MCP users, and other details.

How teams use MCP.

Based on a dozen use cases, there are varied ways of using it.

Popular public MCP servers.

Stats from widely-used public MCP servers operated by Sentry and Linear, plus an odd conjunction of thousands of DAUs and millions of daily sessions.

Security considerations.

Securityâ€™s still the Achilles heel of MCPs and LLMs. There are some sensible security practices for treading carefully in the space.

Learnings from building MCPs.

Start small and local, choose the development language carefully, design primitives for

agents

and not humans â€“ & more.

Useful tools for building MCP servers.

FastMCP, MCP Inspector, and Cloudflareâ€™s remote MCP guide among the top mentions.

Our look into MCP usage suggests that using, building, and maintaining MCP servers are on the way to becoming part of the software engineering toolset; perhaps they already are. Meantime, best practices are still taking shape. Letâ€™s get into it:

Full subscribers have access to the 35-page, even

more detailed MCP report

, which accompanies this deepdive.

Get it here.

1. MCP fundamentals

The MCP protocol

was released

in November 2024 and was developed by two software engineers at Anthropic, David Soria Parra, and Justin Spahr-Summers, who started work on it that July.

The protocol aims to be the â€œUSB-Câ€ layer for AI applications. Itâ€™s a standardized protocol to connect Clients (chatbots, IDEs, AI applications) to Servers (data, files, and tools). Hereâ€™s how the protocol works, at a high level:

MCP protocol

With MCP, you can do handy things like:

Implement a ticket.

Tell the agent inside Cursor to implement TICKET-123 in Linear.

Interactive debugging.

Tell Claude Code to verify that the feature implemented in TICKET-123 in tools like Linear/JIRA works as expected, or by inspecting logs inside Sentry, Statsig, and other production systems.

Query databases.

Ask your agent running in the IDE: â€œHow many new signups were there last week?â€ With a database MCP connector, the agent goes and writes the right query and returns the result.

For devs, a very practical use case is to add MCP servers for preferred agents, then instruct the agent to do more complicated workflows involving one of the added tools. Thanks to having access to tools via MCP â€“

boom!

â€“ the agent becomes a lot more capable!

We cover the origin of MCP and how it works in depth, in the deepdive

MCP Protocol: a new AI dev tools building block

.

2. Usage realities

When it comes to MCP adoption, thereâ€™s a view that there are many times more MCP servers than users, as per a popular meme in the space:

MCP: Absence of users?

But the survey paints a nuanced picture, as shown by three unexpected details about the adoption:

#1: internal MCP server usage >> public MCP server usage.

FastMCP creator Jeremiah Lowin told us:

â€œOne of the most interesting things that we [at Prefect] have observed is that we expected to see every company launch an MCP server, and that their customers would begin interacting with them in that way. But that is not what is happening. Many companies are

launching

MCP servers, but not publicly.

There are perhaps 10 MCP servers that are heavily used, from major developer-facing companies. Then, thereâ€™s this massive long tail of public servers with close to zero users, built for personal use.â€

The public-facing server stats tally with what Prefect was seeing, of far higher adoption. They figured out that the reason was that most MCP servers stay internal-only and not in public.

#2: Business stakeholders are heavy MCP users.

Again, from Jeremiah:

â€œTurns out, MCP is being used especially heavily by internal data and platform teams to give internal users access to systems. These are systems that these users perhaps already had access to, but it was either too complex or too broad, or needed a lot of documentation or special skills to use.

We also came across interesting cases where users didnâ€™t have access to a system because the team owning that service didnâ€™t have a trusted way to build a workflow to make those tools available.

Almost every team that we talk to now is an internal team serving internal stakeholders, including non-engineers, and using MCP as the technology to do it.

My colleague recently said that he thinks that MCP will replace â€˜Self-service Business Intelligence.â€™

This replacement is especially likely once thereâ€™s a really established way to just build dashboards from what comes out of it. Itâ€™s almost like the promise of

The Semantic Layer

in the data world is actually being realized through MCP in a way that never quite got out of the barnâ€.

#3: Median MCP user.

This user might be different from some expectations;

my initial idea of a dev using MCP is someone who connects a Linear or Figma MCP server to their agent to get things done faster. But Jeremiah says not:

â€œThe median MCP user is someone who says something like: â€˜I want to access my companyâ€™s own data warehouse through an MCP serverâ€™, and uses an internal MCP that they connect to the agent theyâ€™re using.â€

This observation lines up with many MCP users not being developers; theyâ€™re business folks who want to generate reports and dashboards, and use MCP servers as connectors to internal systems.

So, whatâ€™s with the â€œmore builders than usersâ€ opinion?

Jeremiah answered:

â€œIf all I saw was external-facing tooling, then I would agree with the observation of there being more builders than consumers. In that case, I would see a sea of people promoting junk MCP servers to an audience of zero, and I would probably draw the same conclusion. But empirically, I disagree [with the meme above], based on the usage data I see: there are lots of internal MCP servers built, which are built to be used.â€

This is probably it: the observation that most public MCP servers have few to zero users is correct, but itâ€™s not the case that there are more MCP builders than users. Actually, most users are inside companies which makes them invisible.

A more accurate meme?

Why internal usage makes sense

MCP has many limitations currently which work against public-facing usage, such as:

Controlling both server and client.

When an MCP server is built internally, you know what the client will be, and build for that client. It might be an internal LLM application, or a tool the company uses. By controlling both the client and the server, you can build useful capabilities into the server. But when only controlling the server, the MCP server might need to be â€œdumbed downâ€ to serve simple MCP clients.

Security

. Still an open-ended question â€“ as covered below. For a protocol with security question marks, itâ€™s safer to keep usage within the company firewall.

Elicitation

is a good example of when internal MCP servers make more sense than external ones. Jeremiah explains:

â€œElicitation is like a confirmation in the middle of a tool call, to ask you to provide some structured input, like a â€˜trueâ€™ or â€˜falseâ€™ or â€˜confirmâ€™ or â€˜cancelâ€™, etc. And thatâ€™s great, and you can use that in your server. But if the client being used doesnâ€™t support it, and most clients donâ€™t support it, then you brick the entire conversation. So what do you do if youâ€™re PayPal, at that scale? You choose not to use that feature, which you would think is the obvious thing to put into a confirmation of moving funds, or something. So, all of a sudden, you find your external use cases being boiled down to this lowest common denominator of client abilityâ€.

3. How teams use MCP

There are an abundance of ways that teams are using MCP. A roundup of examples reported to us:

Better dev workflows

Connecting AI agents to ticketing systems and CI/CD systems is a common use case which devs shared with us:

GitHub MCP

is frequently mentioned for:

Interacting with a repo: create PRs, trigger Agent Copilot, search for release history, etc.

Working across several repos: at companies where projects are split across repositories, creating PRs that span many is easier with GitHub MCP.

AI agent + ticketing system.

A backend developer at a small startup shared with us: â€œThe integration with Atlassian Rovo allows me to commit, push, and create a merge request in a single command with Claude Code.â€

Substitute Atlassian with any ticketing system, or Claude Code with any agentic dev tool.

Unsurprisingly, internal MCP servers are a popular means for using dev tools. Hereâ€™s a software engineer on how they do it:

â€œWeâ€™ve only used internally built MCPs due to how much we tailor our API usages.

Weâ€™ve built our own MCP servers for GitHub, JIRA, Datadog, Buildkite and many others.

We are still engaging with them in new ways, but itâ€™s reshaped how we work with agentic AI and integration with crafting sandboxes.â€

MCP servers reduce context switching in these cases; previously, devs needed to context switch between the browser and IDE/CLI, whereas now they can do the same work by interacting with an agent. Iâ€™d say that the time and effort savings are minor, but us devs can be selectively lazy; so, if you can do it with a prompt, why switch between two or more tools!

From a ticket to a v1 version

Bootstrapping work is another common usage of MCP servers. Devs use the MCP server of their ticketing system and kick off a coding agent:

Linear MCP

:

â€œThe Linear MCP allows us to simply paste in a link to an issue and ask an AI like Cursor or Claude Code to complete the ticket.â€

â€“ software engineer.

Confluence MCP:

â€œWe use it to search for relevant documentation, update JIRA tasks, and read descriptions to bootstrap work.â€

â€“

Lukas Kurucz

, React Native engineer at language learning platform Preply.

Atlassian + GitHub + Confluence MCP:

also from Lukas, a case of MCPs working together: â€œI use Atlassian MCP to create new documentation for changes based on PR on GitHub. Copilot will read code changes via GitHub MCP and create a doc on the changes in Confluenceâ€.

Figma MCP

is a popular mention among devs in our research, with several using it to turn Figma designs into code. Figma offers

a hosted remote server

, and

a local server

that can be used with the Figma desktop app:

â€œThe Figma MCP server has been handy for implementing Figma designs into React Native UI code. Cursor Agent mode, using the tools available on this server often does an excellent job of creating a first draft of code that can be improved in future agent loops. The official MCP from Figma has been consistent for me since I started with it, and uses the existing authentication from the Figma desktop appâ€. â€” Joshua Yoes, Staff Engineer at Infinite Red.

â€œFigma MCP is a good one â€” it works well with Jira Atlassian MCP to bootstrap work for new JIRA tasksâ€. â€“ Lukas Kurucz.

Debugging

Connecting error tracking and observability tools is another popular use case:

Observability MCP server:

â€œWe built an MCP server to access observability data. We are now pulling this observability data into Claude Code so that it can directly cross-reference errors with code changes and potential causes. Also, to analyze slowdowns / expensive database queries. That way, Claude Code has access to performance data and full schemas.â€ â€“ Ben Blackmore, CTO, observability startup, Dash0.

Sentry MCP + Cursor:

â€œSentryâ€™s MCP server is fairly useful; Iâ€™ll give Cursor a link to a Sentry issue and ask it to troubleshoot and create a plan for fixing the issue.â€ â€” Principal engineer at a late-stage startup.

In-house Rollbar MCP

: â€œWe paste in a link to a Rollbar issue (Rollbar is an error tracking service) and the AI is then able to read the stack trace of the error, find the exact line of code in our codebase, fix it, and push a PR. We built our own Rollbar MCP server because there wasnâ€™t an official one at the timeâ€. â€“ Software engineer at a startup.

Testing

Running browser automations with MCPs

: using the MCP servers of browser automation frameworks like Playwright and Puppeteer is a no-brainer for some frontend devs:

â€œBrowser automation via Playwrightâ€™s accessibility tree. Interact with and test web apps deterministically via MCP.â€ â€” Software engineer at a self-driving startup

â€œIâ€™ve started using the Playwright MCP for giving the assistant context on what the frontend implementation looks like.â€ â€” Developer at a consultancy

â€œPuppeteer is helpful for UI changes; Iâ€™ll tell Cursor what local URL to visit and what it should see or what issues to look for.â€ â€” Andrew Minion, platform engineering manager at travel tech company iVisa

Making specialized testing options available to all devs:

a systems development engineer at a mid-sized company said:

â€œChrome DevTools has powerful

automation capabilities

like recording and replaying user flows, getting performance insights, and doing advanced browser debugging like analyzing network requests on the fly. However, until recently, only a small portion of people familiar with UI automation could utilize this.

Thanks to

chrome-devtools-mcp

, now backend developers and even product managers can quickly experience its capabilitiesâ€.

New way of testing features?

Software engineer,

Theo Windebank

, described an interesting, new type of testing workflow at customer ops AI startup, Gradient Labs. It involves using a Notion database, Claude Code, and the Linear MCP:

â€œA flow that worked really well for us:

We built a new feature and got lots of people at the company to test it.

They added rows to a Notion database with their testing results and feedback.

I used Claude Code to create a new database with aggregated/categorized test results in it, grouped by underlying issue.

I iterated on that a bit manually in Notion, then used the Linear MCP to create a new project and filled it with tickets based on the rows in the aggregated database.

This all worked quite well: the agent often gets the parameters wrong, but then retries with corrections, which is kind of cool to watchâ€.

Mobile QA with the iOS Simulator MCP

: an interesting use case was shared by Staff Engineer Joshua Yoes, using the popular

iOS Simulator MCP

he created:

â€œI have found that if I give clear instructions for navigating a UI flow that I know works, the iOS Simulator MCP can be handy for automating the taking of screenshots of key screens and screen recordings of flows. Often, I need to include these in pull request descriptions in order for a reviewer to approve my code, so this has been a handy use case to speed it up. For some more complicated changes, I was able to successfully get 5 different screen recordings in one conversation with the agentâ€.

External documentation

Giving agents access to external documentation (like AWS docs) is a good trick for getting better coding results and fewer hallucinations.

Context7 MCP: a popular way to access external documentation.

Context7 MCP

was mentioned by several devs as the way they give agents access to up-to-date documentation:

â€œContext7 is a great tool for allowing the model to quickly fetch documentation about a toolâ€. â€” Federico Saravia, Head of Engineering at

letsmake.com

.

â€œOn my team we use Context7 for up-to-date documentation, and see fewer hallucinated APIs and functions as a resultâ€. â€” Wynand Pieters, consultant dev.

â€œWe usually use Context7 to give external documentation access to agentsâ€. â€“ Andrew Eacott, Principal Engineer at Miro.

AWS Knowledge MCP server

is also handy, according to a software engineer at an autonomous driving startup:

â€œWe use the Remote AWS Knowledge MCP via

Streamable HTTP

, proxied to standard input/output (stdio) for editor compatibilityâ€.

Internal documentation

Opening up access to internal docs is also a common use case:

â€œThe Notion MCP has been useful to open up internal docs

. We use these documents as context when creating product requirement documents to implement larger features in a React Native codebase. Sometimes, clients I work with will create style guides or tutorials for developers. Having Cursor use this Notion MCP is great for securely accessing internal Notion documents, and doing so without copy/pasting the entire contents of the article in an agent chat.â€ â€” Joshua Yoes, Staff Engineer at Infinite Red.

Building custom MCPs to access internal docs is common:

â€œWe built an internal tool that serves DevEx knowledge (how-to guides, internal library documentation) to engineers working in agentic coding IDEs. It was very straightforward to build; we used

Spring AI

. The challenge is in optimising content served, not the MCP integration itself.â€ â€” Andrew Eacott, Principal Engineer at Miro

â€œIâ€™ve built an MCP sharing information about our internal libraries: it suggests what internal library to use, lists all libraries, provides documentation of a library, and provides examples of use of a libraryâ€. â€” software engineer at a startup

Better context on legacy codebases.

A clever use case was shared by a Tech Lead at a SaaS company working in governmental technology:

â€œI developed my own MCP server for a project to give better context on legacy codebases to agents by providing an interface to search for similar implementation patterns, bugs, etc. Itâ€™s very helpful, so farâ€.

Internal service discovery

An MCP for a â€œservices lookupâ€ is a smart use case.

At GetYourGuide, the team built one to get service catalog information. Engineering Manager Harshal Shah shared:

â€œOne of the internal MCP servers we have developed talks to our service catalog and answers questions such as â€˜who owns a given serviceâ€™, â€˜which team does an individual belong toâ€™, and so onâ€.

Working with legacy systems & complex libraries

A software engineer at an electric aviation company built an MCP server to pull data from complicated, legacy

Siemens Polarion ALM

software:

â€œWe are building an MCP to surface documents from Siemens Polarion ALM for tracking aircraft requirements and airborne software development.

Accessing this legacy system with very poor integration points has always been a challenge, especially for AI tools.

But itâ€™s also been a challenge to get documents out from this system to share with external users. Even now, access control due to the sensitive nature of the data is a really tough situation with the current MCP spec. Running the MCP server inside a trusted boundary works though â€“ at least for now.â€

Suggesting which internal libraries that agents should use

. Another engineer is working on an MCP specifically to provide more context on internal libraries:

â€œIâ€™ve built an MCP server that shares information about our internal libraries: it suggests what internal library to use, lists all libraries, provides documentation of a library, and provides examples on how to use this library. Itâ€™s a handy MCP serverâ€. â€” software engineer at a startup.

Giving non-devs more access

The GetYourGuide team is experimenting with surfacing more information to customer agents. From Harshal Shah, Engineering Manager at the company:

â€œOne MCP implementation is over our GraphQL gateway. This server allows our customer care agents to get information more easily.  After initial testing, we found it useful and are rolling it out. This MCP helps answer questions such as â€˜how many participants were in this booking?â€™ and â€˜Have there been previous refunds for booking ID ###?â€™ We will expand the scope further to take bigger, bolder workflows in the coming months.â€

Turning internal APIs into data sources for non-devs to use.

A Staff Geospatial Engineer at an aerospace company shared:

â€œOur MCPs help non-software engineers aggregate data from different services. These servers expose information from our API data services. We build National Catastrophe (NatCat) solutions (e.g. modeling the likelihood of the likes of earthquakes, floods, wildfires, etc), and we find it useful to have a GPT client feeding on our data so our non-developers can aggregate more data sources, more efficiently. So far, itâ€™s working great!â€

Allowing non-devs to contribute code

Kamlesh Chandnani is Director of Frontend at payment solutions provider, Razorpay. He shared how theyâ€™ve

open-sourced

their design system called â€˜Bladeâ€™, and built

an MCP server

that can implement frontend UIs using this design system. The MCP server is also open source:

Components from

Razorpayâ€™s design system

Kamlesh explained how the team uses this MCP for a few purposes:

Figma to code, with great results.

â€œOur Blade MCP server converts Figma designs to code. Just grab the Figma frame, paste a link into the server, and then without prompting, you get production-ready code. A few stats:

70%

of our frontend engineers have used it to build features without writing any single line of code.

75%

accuracy on the first generation, without prompting (we measure this using

evals

). Devs iterate on the rest.

3x:

devs reported shipping three times faster when building UIs from Figma designs.

We keep our Figma designs and code matching. For the release cycle of the design system, we only release a new component in Figma once the code for the component is released. We built tooling and scripts to validate the property structure on Figma. All this work seems to be paying off with accurate code generation.

Migrating existing code/UI to the new design system:

Blade MCP made it easier and faster. Weâ€™re now building an agentic workflow to make migration autonomous via agentsâ€.

Something that got my attention is how Razorpay has opened up â€œvibe codingâ€ to non-devs. Kamlesh, again:

â€œ

This MCP acts as our â€˜internal Lovableâ€™ tool for PMs/designers/anyone else.

It enables all of these non-engineers to vibe code their ideas, using the design language of Razorpay, and take it to production.

Our issue with existing vibe coding tools was that they could not generate minimum viable products(MVPs) that follow the internal design guidelines and coding practices. But with our Blade MCP, we can scaffold a base template with our coding practices, and then the UI can be built using our design language, tested using Playwright MCP, and deployed using our DevOps MCP, so now itâ€™s a lot easier to take apps prototyped like this to productionâ€.

Internal MCP servers that keep evolving

A principal engineer at a startup shared an interesting emergent model of experimentation with MCP:

â€œI started off a project of setting up an internal agent with the expectation that it would just be used to triage tech support tickets. However, after I showed it to a few colleagues, everyone wanted to use it, and they all started using it for their own things.

The way that I see things progressing is that you share your internal server, and then it starts to shape up

. So, like me, you set up an agent with all of these MCP tools in various internal systems. At first, itâ€™s a free-for-all: anyone can come and use it. But as people start using it more, repeatable workflows and effective prompts will begin to emerge, and we will extract and formalize those.

Some things will definitely stay loose and ad-hoc, of course. For example, one of the biggest use cases so far is just keeping ad-hoc data queries off of the plates of our data team. Before, they would interrupt us with things like â€œwhat do usage patterns for this new feature look like?â€. Now, they can simply ask this of the agent that has our MCP connected to itâ€.

Many ways to use MCPs

The list of examples goes on, evidently. I reckon a lot of innovation is happening inside teams, with MCPs allowing devs to do things more easily, and for non-devs to get access to data, systems, and functionality.

External MCP servers and their relatively small usages donâ€™t tell the whole story of whatâ€™s going on: within trusted boundaries where security is not a worry, theyâ€™re widely-used powerful components for working with agents.

4. Popular public MCP servers

We know thereâ€™s a â€œpower lawâ€ at play in which a few MCP servers get outsized public usage. I reached out to Sentry and Linear about what theyâ€™re seeing.",https://newsletter.pragmaticengineer.com
https://newsletter.pragmaticengineer.com/p/the-pulse-155,The Pulse #155: 5-day RTO looms for those at Big Tech?,"Also: senior engineers might be more efficient with AI tools than juniors, Anthropic and Claude Code on a roll, and more.",2026-01-02T16:54:22.262097,Gergely Orosz,"The Pulse is a series covering events, insights, and trends within Big Tech and startups. Notice an interesting event or trend? Hit reply and share it with me.

Today, we cover:

Could a 5-day RTO be around the corner in Big Tech?

From next February, workers at Instagram must be in the office, five days a week. This makes Meta the second tech giant after Amazon to mandate a 5-day RTO. Will more big companies do the same?

Are senior engineers better with AI than juniors are?

Data from more than 40,000 users of Cursor suggests that experienced devs get better results from AI agents. It suggests the tool amplifies coding skills â€“ not replace them.

Industry Pulse.

OpenAI dumps Mixpanel after security breach, Stripe buys usage metering startup, OpenAIâ€™s â€œCode Red,â€ a high-risk security issue at a $3B legal startup, and more.

Anthropic and Claude Code on a roll.

Claude Code just surpassed $1B in annual run rate revenue after 6 months, Anthropic buys JavaScript runtime Bun, and might plan to go public as soon as next year.

1. Could a 5-day RTO be around the corner for Big Tech?",https://newsletter.pragmaticengineer.com
https://newsletter.pragmaticengineer.com/p/being-a-founding-engineer-at-an-ai,No Title,,2026-01-02T16:54:23.842529,Gergely Orosz,"Stream the latest episode

Listen and watch now on

YouTube

,

Spotify

, and

Apple

.

See the episode transcript at the top of this page, and timestamps for the episode at the bottom.

Brought to You by

â€¢â 

Statsig

â  â€” â  The unified platform for flags, analytics, experiments, and more. Together with Statsig, Iâ€™m hosting The Pragmatic Summit on 11 February, San Francisco.

Get tickets and join us there.

â€¢â 

Linear

â  â€” â  The system for modern product development. The founders of Linear saw how companies like Airbnb, Uber and Coinbase started to slow down as they grew quickly â€” and Linear was built to give clarity and coordination, without the overhead.

Try it yourself.

In this episode

Michelle Lim

joined Warp as engineer number one and is now building her own startup,

Flint

. She brings a strong product-first mindset shaped by her time at Facebook, Slack, Robinhood, and Warp. Michelle shares why she chose Warp over safer offers, how she evaluates early-stage opportunities, and what she believes distinguishes great founding engineers.

Together, we cover how product-first engineers create value, why negotiating equity at early-stage startups requires a different approach, and why asking founders for references is a smart move. Michelle also shares lessons from building consumer and infrastructure products, how she thinks about tech stack choices, and how engineers can increase their impact by taking on work outside their job descriptions.

If you want to understand what founders look for in early engineers or how to grow into a founding-engineer role, this episode is full of practical advice backed by real examples.

An interesting quote from the episode

Michelleâ€™s advice to be a standout founding engineer at an AI company:

Gergely: What would your advice be to software engineers who would love to join as a founding engineer, perhaps an AI startup  these days?

Michelle:

Itâ€™s about showing that youâ€™ve built in AI before, because that skill is very much in high demand and itâ€™s very new.

Very few people â€” relatively speaking â€” have ever built an AI product before. So just spending some time over the weekends knowing how to build an AI product already helps you stand out above many people. Build anything that scratches your itch that uses any of the models, or the completion APIs.

To excel in the role: it starts off with picking the right founder.

Once you do join, itâ€™s all about volunteering to do the things that no one wants to do, but itâ€™s the most important thing for the business.

So I wrote  blog posts, published them on Hacker News, and I answered all the questions on Hacker News. I went out there and I created our company Twitter, and I was writing tweets for the company. I then started a YouTube channel for the company before any developer tool companies really thought about doing YouTube. I started a Discord channel, filed all feedback, and did things outside of engineering that the business really needed.

You still have to make sure that youâ€™re doing your number one job, which is software engineering.

That needs to still stay the main focus, and you should only volunteer for other stuff if you are already doing well in your main job.

The benefit of doing a lot of these things and learning how to do a lot of these things is that then you get to learn what businesses

need

.

The Pragmatic Engineer deepdives relevant for this episode

Thriving as a founding engineer: lessons from the trenches

From software engineer to AI engineer

AI Engineering in the real world

The AI Engineering stack

Timestamps

(

00:00

) Intro

(

01:32

) How Michelle got into software engineering

(

03:30

) Michelleâ€™s internships

(

06:19

) Learnings from Slack

(

08:48

) Product learnings at Robinhood

(

12:47

) Joining Warp as engineer #1

(

22:01

) Negotiating equity

(

26:04

) Asking founders for references

(

27:36

) The top reference questions to ask

(

32:53

) The evolution of Warpâ€™s tech stack

(

35:38

) Product-first engineering vs. code-first

(

38:27

) Hiring product-first engineers

(

41:49

) Different types of founding engineers

(

44:42

) How Flint uses AI tools

(

45:31

) Avoiding getting burned in founder exits

(

49:26

) Hiring top talent

(

50:15

) An overview of Flint

(

56:08

) Advice for aspiring founding engineers

(

1:01:05

) Rapid fire round

References

Where to find Michelle Lim:

â€¢ X:

https://x.com/michlimlim

â€¢ LinkedIn:

https://www.linkedin.com/in/michlimlim

â€¢ Website:

https://michellelim.dev

Mentions during the episode:

â€¢ Warp:

https://www.warp.dev

â€¢ Flint:

https://www.tryflint.com

â€¢ Meta:

https://www.meta.com

â€¢ Slack:

https://slack.com

â€¢ Robinhood:

https://robinhood.com

â€¢ Kafka:

https://en.wikipedia.org/wiki/Apache_Kafka

â€¢ Bloomberg:

https://www.bloomberg.com

â€¢ Zach Lloyd on LinkedIn:

https://www.linkedin.com/in/zachlloyd

â€¢ TypeScript:

https://www.typescriptlang.org

â€¢ Rust:

https://rust-lang.org

â€¢

Programming Rust: Fast, Safe Systems Development

:

https://www.amazon.com/Programming-Rust-Fast-Systems-Development/dp/1492052590

â€¢ Nathan Sobo on LinkedIn:

https://www.linkedin.com/in/nathan-sobo-92b46720

â€¢ Atom:

https://github.com/atom/atom

â€¢ Stop just using â€œFrontendâ€ or â€œBackendâ€ to describe the Engineering you like:

https://michellelim.dev/writing/stop-using-frontend-backend

â€¢ Claude Code:

https://www.claude.com/product/claude-code

â€¢ Cursor:

https://cursor.com

â€¢ Sohan Choudhury on LinkedIn:

https://www.linkedin.com/in/sohan-choudhury

â€¢ Nuro:

https://www.nuro.ai

â€¢ Windsurf:

https://windsurf.com

â€¢ Weapons:

https://www.imdb.com/title/tt26581740

â€”

Production and marketing by

Pen Name

.",https://newsletter.pragmaticengineer.com
https://newsletter.pragmaticengineer.com/p/evals,A pragmatic guide to LLM evals for devs,Evals are a new toolset for any and all AI engineers â€“ and software engineers should also know about them. Move from guesswork to a systematic engineering process for improving AI quality.,2026-01-02T16:54:25.182036,Gergely Orosz,"One word that keeps cropping up when I talk with software engineers who build large language model (LLM)-based solutions is â€œ

evals

â€. They use evaluations to verify that LLM solutions work well enough because LLMs are non-deterministic, meaning thereâ€™s no guarantee theyâ€™ll provide the same answer to the same question twice. This makes it more complicated to verify that things work according to spec than it does with other software, for which automated tests are available.

Evals feel like they are becoming a core part of the AI engineering toolset. And because they are also becoming part of CI/CD pipelines, we, software engineers, should understand them better â€” especially because we might need to use them sooner rather than later! So, what do good evals look like, and how should this non-deterministic-testing space be approached?

For directions, I turned to an expert on the topic,

Hamel Husain

. Heâ€™s worked as a Machine Learning engineer at companies including Airbnb and GitHub, and teaches the online course

AI Evals For Engineers & PMs

â€” the upcoming cohort

starts in January

. Hamel is currently writing a book,

Evals for AI Engineers

, to be published by Oâ€™Reilly next year.

In todayâ€™s issue, we cover:

Vibe-check development trap.

An agent appears to work well, but as soon as it is modified, it canâ€™t be established that itâ€™s working correctly.

Core workflow: error analysis

. Error analysis has been a key part of machine learning for decades and is useful for building LLM applications.

Building evals: the right tools for the job.

Use code-based evals for deterministic failures, and an LLM-as-judge for subjective cases.

Building an LLM-as-judge.

Avoid your LLM judge memorizing answers by partitioning your data and measuring how well the judge generalizes to unfamiliar data.

Align the judge, keep trust.

The LLM judgeâ€™s expertise needs to be validated against human expertise. Consider metrics like True Positive Rate (TPR) and True Negative Rate (TNR).

Evals in practice: from CI/CD to production monitoring.

Use evals in the CI/CD pipeline, but use production data to continuously validate that they work as expected, too.

Flywheel of improvement.

Analyze â†’ measure â†’ Improve â†’ automate â†’ start again

With that, itâ€™s over to Hamel:

1. Vibe-check development trap

Organizations are embedding LLMs into applications from customer service to content creation. Yet, unlike traditional software, LLM pipelines donâ€™t produce deterministic outputs; their responses are often subjective and context-dependent. A response might be factually accurate but have the wrong tone, or sound persuasive while being completely wrong. This ambiguity makes evaluation fundamentally different from conventional software testing. The core challenge is to systematically measure the quality of our AI systems and diagnose their failures.

I recently worked with

NurtureBoss

, an AI startup building a leasing assistant for apartment property managers. The assistant helps with tour scheduling, answers routine tenant questions, and inbound sales. Here is a screenshot of how the product appears to customers:

The AI leasing assistant of NurtureBoss. Source:

nurtureboss.io

They had built a sophisticated agent, but the development process felt like guesswork: theyâ€™d change a prompt, test a few inputs, and if it â€œlooked good to meâ€ (LGTM), theyâ€™d ship it. This is the â€œvibes-based developmentâ€ trap, and itâ€™s where many AI projects go off the rails.

To understand why this happens, it helps to think of LLM development as bridging three fundamental gaps, or â€œgulfsâ€:

Gulf of Comprehension:

The gap between a developer and a true understanding of their data and the modelâ€™s behavior at scale. Itâ€™s impossible to manually read every user query and inspect every AI response to grasp the subtle ways a system might fail.

Gulf of Specification:

The gap between what we

want

the LLM to do, and what our prompts

actually instruct

it to do. LLMs cannot read our minds; an underspecified prompt forces them to guess our intent, leading to inconsistent outputs.

Gulf of Generalization:

The gap between a well-written prompt and the modelâ€™s ability to apply those instructions reliably across all possible inputs. Even with perfect instructions, a model can still fail on new or unusual data.

The â€œThree Gulfsâ€ model of challenges in LLM pipeline development

Navigating these gulfs is the central task of an AI engineer. This is where a naive application of Test-Driven Development (TDD) often falls short. TDD works because for a given input, there is a single, deterministic, knowable, correct output to assert against. But with LLMs, thatâ€™s not true: ask an AI to draft an email, and there isnâ€™t one right answer, but thousands.

The challenge isnâ€™t just the â€œinfinite surface areaâ€ of inputs; itâ€™s the vast space of valid, subjective, and unpredictable outputs. Indeed, you canâ€™t test for correctness before youâ€™ve systematically observed the range of possible outputs and have defined what â€œgoodâ€ even means for your product.

In this article, we walk through the pragmatic workflow weâ€™ve applied at NurtureBoss, and at over 40 other companies, in order to move from guesswork to a repeatable engineering discipline. It is the same framework weâ€™ve taught to over 3,000 engineers in our

AI Evals course

. By the end of this article, weâ€™ll have gone through all the steps of what we call â€œthe flywheel of improvement.â€

A better alternative to vibes-based development of LLM apps, using evals

2. Core workflow: error analysis

To get past the LGTM trap, the NurtureBoss team adopted a new workflow, starting with a systematic review of their conversation traces. A trace is the complete record of an interaction: the initial user query, all intermediate LLM reasoning steps, any tool calls made, and the final user-facing response. Itâ€™s everything you need to reconstruct what actually happened. Below is a screenshot of what a trace might look like for NurtureBoss:

A trace rendered in

Arize Phoenix

, an open source LLM observability & eval tool

There are many ways to view traces, including with LLM-specific observability tools. Examples that I often come across in practice are

LangSmith

,

Arize

(pictured above), and

Braintrust

.

From raw notes to clear priorities

At first, it was an unglamorous process of manually using a trace viewer to review data. But as Jacob, the founder of NurtureBoss, reviewed traces, the friction became obvious. Off-the-shelf observability tools like LangSmith, Arize, Braintrust, and others offer decent ways to get started on reviewing data quickly, but are generic by design. For the NurtureBoss use case, it was cumbersome to view all necessary context, such as specific property data and client preferences on a single screen. This friction slowed them down.

Instead of fighting their tools, they invested a few hours in vibe coding a simple, custom data viewer using an AI assistant. It wasnâ€™t fancy, but solved their core problems by showing each conversation clearly on one screen, with collapsible sections for tool calls and a simple text box for adding notes. Below is a screenshot of one of the screens in their data viewer:

Data viewer built by NurtureBoss. Building internal tools is a good use case for vibe-coding

This simple tool was a game-changer. It unlocked a significant improvement in review speed, allowing the team to get through hundreds of traces efficiently. With this higher throughput, they began adding open-ended notes on any behavior that felt wrong, a process known as

open coding

. When open coding, it is important to avoid predefined checklists of errors like â€œhallucinationâ€ or â€œtoxicityâ€. Instead, let the data speak for itself, and jot down descriptive observations like:

â€œThe agent missed a clear opportunity to re-engage a price-sensitive user.â€

â€œIt asked to send a text confirmation twice in a row.â€

â€œOnce the user asked to be transferred to a human, the agent kept trying to solve the problem instead of just making the handoff.â€

After annotating dozens of conversations, NurtureBoss had amassed a rich, messy collection of notes. To find the signal amid the noise, they grouped these notes into themes in a step called

axial coding

. An LLM helped with the initial grouping of open codes into categories (aka the axial codes), which the team then reviewed and refined. A simple pivot table revealed that just three issues accounted for most problems: date handling, handoff failures, and conversation flow issues. This provided a clear, data-driven priority of failure modes in the application. Below is an illustration of what a partial view of this pivot table might look like, which is a count of failure categories:

Using axial coding to group open code into buckets

This bottom-up approach is the antidote to the problems of generic, off-the-shelf metrics. Many teams are tempted to grab a pre-built â€œhallucination scoreâ€ or â€œhelpfulnessâ€ eval, but in my experience, these metrics are often worse than useless. For instance, I recently worked with a mental health startup whose evaluation dashboard was filled with generic metrics like â€˜helpfulnessâ€™ and â€˜factuality,â€™ all rated on a 1-5 scale. While the scores looked impressive, they were unactionable; the team couldnâ€™t tell what made a response a â€˜3â€™ versus a â€˜4,â€™ and the metrics didnâ€™t correlate with what actually mattered to users.

They create a false sense of security, leading teams to optimize for scores that donâ€™t actually correlate with user satisfaction. In contrast, by letting failure modes emerge from your own data, you ensure your evaluation efforts are focused on

real

problems.

This process of discovering failures from data isnâ€™t a new trick invented for LLMs; itâ€™s a battle-tested discipline known as

error analysis

, which has been a cornerstone of machine learning for decades, and is adapted from rigorous qualitative research methods like

grounded theory

in the social sciences. Next, letâ€™s summarize this process into a step-by-step guide to apply to your problem.

Step-by-step guide to error analysis

This bottom-up process is the single highest-ROI activity in AI development. It ensures youâ€™re solving real problems, not chasing vanity metrics. Hereâ€™s how to apply it:

Build a simple data viewer.

This is your most important investment. A custom web app, tailored to your domain, allows you to show all necessary context in one place and makes capturing feedback trivial.

Open coding: bottom-up annotation.

Open coding

(not to be confused with software coding) is a technique of writing open-ended notes about observed problems. In the context of LLMs, you review at least 100 diverse traces and add open-ended notes on any undesirable behavior. When reviewing a complex trace, the most effective tactic is to identify and annotate only the first upstream failure. LLM pipelines are causal systems; a single error in an early step like misinterpreting user intent often creates a cascade of downstream issues. Focusing on the first observable error is more efficient by preventing you from getting bogged down in cataloging every symptom. Often, fixing that single issue resolves an entire chain of subsequent failures.

Axial coding: create a taxonomy.

Now you have open-ended notes from step 2, itâ€™s time to categorize them into buckets to understand the patterns. Group your open-ended notes into 5-10 themes. Use an LLM as an assistant to suggest initial clusters, but always have human review and refine the final categories.

Prioritize failures with data.

Use a simple pivot table or script to count the frequency of each failure mode. This transforms qualitative insights into a quantitative roadmap, revealing exactly where to focus engineering efforts.

A common question at this stage is: â€œwhat if thereâ€™s not enough real user data to analyze?â€ This is where

synthetic data

is a powerful tool. You can use a powerful LLM to generate a diverse set of realistic user queries that cover the scenarios and edge cases you want to test. This allows you to bootstrap the entire error analysis process before thereâ€™s a single user. The specifics of how to create high-quality, grounded synthetic data is a deep topic thatâ€™s beyond the scope of this article. Itâ€™s discussed in our

course

.

Below is a diagram illustrating the error analysis process:

The error analysis process visualized

3. Building evals: the right tool for the job

After error analysis, the NurtureBoss team had a clear, data-driven mandate to fix date handling and handoffs, based on the table of errors shown above. However, these things represent different kinds of failure: handling calendar dates is an objective outcome that can be measured against expected value, whereas the question of when to hand off a conversation to a human is more nuanced. This distinction is important because it determines the type of evaluator to build. We discuss the two types of evaluators you need to consider next: code-based assertions vs. LLM Judges.

For deterministic failures â†’ code-based evals

A user query like â€œcan I see the apartment on July 4th, 2026?â€ has one â€“ and only one â€“ correct interpretation. The AIâ€™s job is to extract that date and put it into the right format for a downstream tool. This is a deterministic, objective task thatâ€™s either right or wrong.

Code-based evals are the perfect tool for simple failures. To build one, the NurtureBoss team first assembled a â€œgolden datasetâ€ of test cases. They brainstormed the many different ways users might ask about dates, focusing on common patterns and tricky edge cases. The goal was to create a comprehensive test suite that could reliably catch regressions.

Hereâ€™s a simplified version of what their dataset looked like. Note: For relative dates, we are assuming the current date is 2025-08- 28.

â€œGolden datasetâ€ of test cases: The LLM is tested to ensure it returns the expected output

With this dataset, the evaluation process is straightforward and mirrors traditional unit testing. You loop through each row, pass the

User Query

to your AI system, and then run a simple function that asserts the AIâ€™s extracted date matches the

Expected Output.

A code snippet showing a simple python function for a code-based eval

Code-based evals are cheaper to create and maintain

than other kinds of evals. Since there is an expected output, you only have to run the LLM to generate the answer, followed by a simple assertion. This means you can run these more often than other kinds of evals (for example, on every commit to prevent regressions). If a failure can be verified with code, always use a code-based eval.

For subjective failures â†’ LLM-as-judge

But what about a more ambiguous problem, like knowing when to hand off a conversation to a human agent? If a user says, â€œIâ€™m confused,â€ should the AI hand off immediately, or try to clarify things first? Thereâ€™s no single right answer; itâ€™s a judgment call based on product philosophy. A code-based test canâ€™t evaluate this kind of nuance.

For subjective failures, we created another golden dataset. The domain expert, NurtureBossâ€™s founder, Jacob, reviewed traces where a handoff was potentially needed, and made judgment calls. In each case, he provided a binary PASS/FAIL score and, crucially, a detailed

critique

explaining his reasoning.

Here are a few examples from their dataset:

Example dataset for an LLM-as-judge eval.

Using a PASS/FAIL judgment works better than a points rating.

Youâ€™ll notice that in the example above, domain expert Jacob used a simple PASS/FAIL judgment for each trace, not a 1-5 points rating. This was a deliberate choice. Iâ€™ve learned that while itâ€™s tempting to use a

Likert scale

to capture nuance; in practice, it creates more problems than it solves. The distinction between a â€œ3â€ and a â€œ4â€ is often subjective and inconsistent across different reviewers, leading to noisy, unreliable data.

In contrast, binary decisions force clarity and compel a domain expert to define a clear line between acceptable and unacceptable, focusing on what truly matters for usersâ€™ success. Itâ€™s also far more actionable for engineers: a â€œfailâ€ is a clear signal to fix a bug, whereas a â€œ3â€ is an ambiguous signal: a signal to do what, exactly? By starting with pass/fail, you cut through the ambiguity and get a clear, actionable measure of quality, faster.

This dataset of traces, judgments, and critiques does more than just help the team understand the problem. As weâ€™ll see in the next section, these hand-labeled examples, especially the detailed critiques, become raw material for building a reliable LLM-as-judge.

4. Building an LLM-as-judge

The hand-labeled dataset of handoff failures is a necessary first step in measuring and solving handoff issues. But another issue is that manual review doesnâ€™t scale. At NurtureBoss, the next step was to automate the domain expertâ€™s expertise by building an LLM-as-judge: an AI evaluator that could apply his reasoning consistently to thousands of future traces.",https://newsletter.pragmaticengineer.com
https://newsletter.pragmaticengineer.com/p/code-security,No Title,,2026-01-02T16:54:31.873595,Gergely Orosz,"Stream the latest episode

Listen and watch now on

YouTube

,

Spotify

, and

Apple

.

See the episode transcript at the top of this page, and timestamps for the episode at the bottom.

Brought to You by

â€¢â 

Statsig

â  â€” â  The unified platform for flags, analytics, experiments, and more. Statsig are helping make the first-ever Pragmatic Summit a reality. Join me and 400 other top engineers and leaders on 11 February, in San Francisco for a special one-day event.

Reserve your spot here.

â€¢â 

Linear

â  â€” â  The system for modern product development. Engineering teams today move much faster, thanks to AI. Because of this, coordination increasingly becomes a problem. This is where Linear helps fast-moving teams stay focused.

Check out Linear.

â€”

In this episode

As software engineers, what should we know about writing secure code?

Johannes Dahse

is the VP of Code Security at Sonar and a security expert with 20 years of industry experience. In todayâ€™s episode of

The Pragmatic Engineer,

he joins me to talk about what security teams actually do, what developers should own, and where real-world risk enters modern codebases.

We cover dependency risk, software composition analysis, CVEs, dynamic testing, and how everyday development practices affect security outcomes. Johannes also explains where AI meaningfully helps, where it introduces new failure modes, and why understanding the code you write and ship remains the most reliable defense.

If you build and ship software, this episode is a practical guide to thinking about code security under real-world engineering constraints.

Interesting quotes from the episode:

How code quality and code security are connected:

Gergely: â€œHow does the quality of code relate to security?â€

Johannes: â€œThis is an underrated topic in the industry today. We talked about the null pointer exceptions or these slow regular expressions, right? That can lead to security issues. Thatâ€™s more of the obvious examples of bugs.

Think about unreadable code, not well-maintained code: thatâ€™s often like spaghetti code. At first, itâ€™s not so obvious how this is connected to security.

If you think about how code is not easy to comprehend, not easy to review. Then you do pair programming or code reviews in your development team â€” then in that spaghetti code, you will more likely overlook security problems of your colleague.

Now, maybe someone found an issue or and issue and reports it back to you. As a developer, you have to fix it. But if itâ€™s not maintainable code, you cannot fix the security problem easily. So quality suddenly becomes a security issue in how the attacker window stays open longer!

Your code quality is super related to code security, especially now with AI-generated code.

We typically see poor quality of code here. And that becomes a problem for security.â€

How AI is introducing new security issues:

Gergely: â€œHow do you think AI is changing code security?â€

Johannes: â€œWe see a change in how code and applications are built. Traditionally, you had this backend/frontend split. In the backend, you had a database. Remove that database, then you donâ€™t have a SQL injection risk anymore.

As soon as you add an LLM to the backend, you have to deal with prompt injection vulnerabilities.

The attacker can modify the system prompt or manage do some prompt engineering and then mess with the LLMâ€™s logic or the output. Itâ€™s taking time both for the attackers to adjust to this, and the industry to adjust in defending against it.â€

Gergely: â€œWhat about with coding assistance? Are you seeing things change in terms of how we think about code security?â€

Johannes: â€œ

The big problem [with AI coding assistance] in terms of security is that you produce code much more faster.

Writing code is not the challenge anymore. Suddenly, the new bottleneck is how are you verifying all that code, right? And if you donâ€™t verify it:that leads to security issues or quality issues. Quality issues, on the long run, lead to security problems.â€

On software composition analysis:

Gergely:

â€œWhat is

software composition analysis

(SCA)?â€

Johannes: â€œItâ€™s a technique where we look at manifest files and your list of dependencies, depending on the package manager you use. This list of dependencies is checked against a database of known security problems. Those are called the

CVEs

. Then you can - with software composition - map, for example, that this specific Log4j version of your library is vulnerable to the Log4 shell vulnerability that is known. And then it can warn you.â€

The Pragmatic Engineer deepdives relevant for this episode

What is Security Engineering?

Mishandled security vulnerability in

Next.js

Okta Schooled on Its Security Practices

Timestamps

(

00:00

) Intro

(

02:31

) What is penetration testing?

(

06:23

) Who owns code security: devs or security teams?

(

14:42

) What is code security?

(

17:10

) Code security basics for devs

(

21:35

) Advanced security challenges

(

24:36

) SCA testing

(

25:26

) The CVE Program

(

29:39

) The State of Code Security report

(

32:02

) Code quality vs security

(

35:20

) Dev machines as a security vulnerability

(

37:29

) Common security tools

(

42:50

) Dynamic security tools

(

45:01

) AI security reviews: what are the limits?

(

47:51

) AI-generated code risks

(

49:21

) More code: more vulnerabilities

(

51:44

) AIâ€™s impact on code security

(

58:32

) Common misconceptions of the security industry

(

1:03:05

) When is security â€œgood enough?â€

(

1:05:40

) Johannesâ€™s favorite programming language

References

Where to find Johannes Dahse:

â€¢ LinkedIn:

https://www.linkedin.com/in/johannes-dahse-112b3057

Mentions during the episode:

â€¢ Sonar:

https://www.sonarsource.com

â€¢ State of Code Security Report by Sonar

https://www.sonarsource.com/resources/the-state-of-code-security-report/

â€¢ OWASP Top Ten:

https://owasp.org/www-project-top-ten

â€¢ Software Composition Analysis:

https://en.wikipedia.org/wiki/Software_composition_analysis

â€¢ CVE Program:

https://www.cve.org

â€¢ SAST:

https://en.wikipedia.org/wiki/Static_application_security_testing

â€¢ What is DAST:

https://github.com/resources/articles/what-is-dast

â€¢ Stack Overflow AI survey:

https://survey.stackoverflow.co/2025/ai

â€¢ Go:

https://go.dev

â€¢ Java:

https://www.java.com

â€”

Production and marketing by

Pen Name

.",https://newsletter.pragmaticengineer.com
https://newsletter.pragmaticengineer.com/p/copy-holiday-gift-ideas-for-techies-2025,Holiday gift ideas for techies,Gift ideas and inspiration for the tech workers in your life (and maybe yourself) this holiday season,2026-01-02T16:54:34.035227,Gergely Orosz,"Itâ€™s that time of the year: the Black Friday and Cyber Monday sales are on, and the annual festive marketing blitz is just around the corner â€“ or already underway. It makes now a good time to start thinking about gifts, but techies can be a tough crowd for this, as we often already own the practical things we need.

In order to help you give gifts which are actually wanted this year, Elin, of this publication, and I have put together a list of ideas in this article. Alongside personal recommendations, weâ€™ve also crowdsourced recommendations from fellow techies on

X

,

Bluesky

, and

Threads

, covering:

Office accessories

Computer add-ons

Health and well-being

Gadgets

Gaming and games

Travel & wearables

Books and stationery

Kitchen goodies

Many products listed below are currently discounted in the sales, and the Pragmatic Engineer is also offering a very special

Black Friday / Cyber Monday deal

for annual subscriptions.

Claim it here

until Monday.

As always, none of the links below are affiliates (meaning I make no money from purchases), and Iâ€™ve not been paid to mention any product or category. See

my ethics statement

for more.

For more recommendations, see our holiday gift guide

from 2023

and book recommendations

from 2021

.

Programming note: this week, weâ€™ll have a podcast episode tomorrow (Wednesday), and no edition of The Pulse on Thursday. Regular programming resumes next week after Thanksgiving.

1. Office accessories

Ember Temperature Control Mug

â€“ keeps coffee or tea warm, even when itâ€™s forgotten about because youâ€™re focused on coding or other tasks. A hot drink stays drinkable for up to 1.5 hours, thanks to its built-in battery that charges on a nifty wireless-charging coaster.

Clever: built-in battery keeps drinks warm

Tumblers and coffee mugs

that keep drinks at the desired temperature can make it easier to stay hydrated throughout the day. They are insulated, made from stainless steel, and make great gifts for anyone who works at a desk. Popular brands include

Stanley

,

Yeti

, and

Thermoflask

. You can also

search for

stainless steel tumblers.

Tumblers: hydration at work or during a commute

CO2 monitor.

One of my personal favorite recent purchases is a carbon dioxide monitor for my office. When CO2 concentration is high (above 1,000 ppm), itâ€™s harder to focus and easier to become drowsy. A monitor detects when levels are raised, meaning you can make changes like open the window, or raise the matter of ventilation at work. In my opinion, you canâ€™t go wrong with almost

any CO2 monitor

.

One of many CO2 detectors: the

SwitchBot CO2 detector

Programmable mini-desk terminal

. These are always a hit for devs, and a popular option is the

Ulanzi TC001 Smart Pixel Clock 2882

. It comes by default as a clock, but can be custom programmed; for example, to show a pomodoro timer (25-minute countdown).

Customizing the Ulanzi Smart Pixel Clock to work as a Pomodoro timer

To program it â€“ and build your own custom matrix clocks â€“ you need to flash a custom firmware

like AWTRIX3

. See more details on the

Ulanzi blog

, and on the AWTRIX3

getting started page

.

Wired / wireless charging stations.

Almost all new phones and small devices are shipping with USB-C ports, so a USB-C-only charging station is increasingly practical. Thereâ€™s

no shortage of options

, and hereâ€™s a tasteful variant: the

Oakwood Dual Dock

Oakwood Dual Dock: a USB-C wired charging station

In his gift guide, Lenny Ratchisky

recommends

the

Belkin MagSafe 3-in-1 charger

.

2. Computer accessories

Techies unavoidably spend A LOT of time with our computers, so a premium accessory often makes for a good gift.

A comfortable mouse.

Given we use one all day, a mouse that feels good for your hand and that can be customized, should be a solid choice. The

Logitech MX Master 3

is recommended by software dev

Brian Cooley

. Iâ€™m personally a fan of

Logitech Mice

; they come in all shapes and sizes, with both wireless and wired versions.

Logitech MX Master 3

Quality noise-cancelling headphones.

These can offer a massive quality-of-work improvement for anyone in an open-plan office, coffee shops, or who likes to get in the zone with some music. My go-to pair is the

Bose QuietComfort Ultra

; I have one in the office, and one in my backpack at all times. Other popular options include the

Sony WH-1000XM5

, the

Apple AirPods Max

, and the

Sennheiser Momentum 4

.

The

Bose QuietComfort Ultra

Mechanical keyboard.

These provide superior typing and improved durability. A popular brand is Das

Keyboard

. Other choices include

Logitech mechanical keyboards

, the quiet

Razer Pro Type Ultra

, and

many others

.

Das Keyboard 6

Quality microphone.

For those who spend a lot of time on video calls, upgrading from the built-in laptop mic can be a nice improvement for those on the other end of a call.

The Shure MV7

is a popular choice. For a more compact microphone, the

Yeti Nano

can also work well.

Shure MV7

3. Health and well-being

Massage vouchers.

Most of us working in tech spend too much time staring at screens, so a massage voucher should be appreciated.

Experiences.

Of course, a great gift doesnâ€™t have to be an object, and a few less conventional ideas have been successes with techies, according to

Candy Evans

:

Walking with alpacas

Making French pastries

Whisky tasting

There is much more, like

high rope courses

,

indoor skydiving

, and even

bobsledding

. Get creative: it could make for something very memorable!

Founder Chris Evans

said

the best gift he has given was a bobsled experience. Pic: Olympic bobsledding for the public at

La Plagne, France

Escape rooms

are a fun activity, usually for groups of 2-6. You need to solve a series of puzzles, usually within an hour, and work together to succeed under pressure of the countdown.

An escape room I enjoyed with a group:

The Vault

, in Amsterdam

â€œEscape room in a boxâ€

is a companion concept, where you have an escape room experience at home, as a board game focused on puzzle solving.

WHOOP subscription

.

A return recommendation from last year: track biometrics such as skin temperature, blood oxygen level, and more. The battery lasts more than 2 weeks, and a subscription offers the option to upgrade to the latest models.

WHOOP band

Coffee, tea, matcha.

These are rarely a miss and many vendors offer subscription coffee or tea products. Here are two personal recommendations:

Terminal

: coffee subscription for devs, built by devs, accessible from your terminal. It has

an API as well

, and you can place an order using SSH. Ships to the US.

Keats&Co

â€“ coffee and tea, including subscription. All profits go to charity.

Coffee delivered by Terminal. Pic:

Andy Leclair

Apple Watch Ultra

.

I got mine this year and am very happy with it. It provides basic sleep tracking, workout tracking, and notifications â€“ and also supports payments (very convenient on vacation, or if you want to be phone-free.) It motivated me to take sleep and workouts more seriously. The battery lasts around 2 days â€“ much less than the Whoop, but I find it manageable.

Apple Watch Ultra 2. Image source:

Outdoor Gearlab

Sunbasin soap and shampoo bars

. These offer a lathery, soapy clean, and the shampoo comes in bar form, which is novel and makes it convenient for travel.

Sunbasin bars

Sunrise alarm clock.

Wake up easier with an alarm clock that simulates a sunrise. One popular edition is the

Philips SmartSleep Wake-up Light

. There is a large variety; just

search â€œsunrise alarm clocksâ€.

A few different Sunrise alarm clocks. Source:

No Sleepless Nights

Foldable walking pad.

I have one at home, and itâ€™s been a moderate success. I originally tried it as an under-the-desk walking pad â€“ which was

okay

, but I felt dizzy when walking at higher speeds for extended periods. Nonetheless, plenty of techies swear by its efficiency, and theyâ€™re increasingly affordable. Just

search â€œwalking padsâ€

for options.

The

WalkingPad A1 Pro

4. Gadgets

Flipper Zero.

When I ask techies about gifts theyâ€™ve enjoyed, the number one recommendation is still the Flipper Zero

from two years ago

. A multi-tool for security folks and those who enjoy hacking around with signals and hardware. Practical use cases include turning it into a universal infrared remote, a Bluetooth remote, and testing and debugging remote frequency (RF) gear like car key fobs with it. Itâ€™s also a great way to get hands-on with RF, NFC/RFID and embedded protocols, and basic firmware development.

The Flipper Zero

Boox Palma 2

. Phone-sized tablets with e-ink that also

kind of

work like a phone â€“ except without phone calls and SMS support. It runs open Android and has Google Play support. So, you can install and use Android apps on a minimalist device that also helps with being more mindful about screentime. It has integrated speakers, a camera, Bluetooth â€“ and the Palma Pro 2 model actually does support a SIM card for cellular data.

Boox Palma 2

Daylight DC-1

. An Android-based computer designed for deep focus and well-being. Uses a reflective â€œLive Paperâ€ LCD display with a blue-light-free experience. Itâ€™s designed to minimize eye strain and support better sleep health. Includes backlighting.

The Daylight DC-1

reMarkable Paper Tablet

â€“ touted as a replacement for notebooks and other books, which enables handwriting and annotating of PDFs. It has around two weeks of battery life, and does not include backlighting.

The reMarkable Paper Tablet

Supernote writing tablet

â€” a digital notebook, and frequently cited as an alternative to the reMarkable, offering similar functionality. Read a

comparison between the two

and more experiences shared on the

Supernote Reddit forum.

The Supernote

Small 3D printer.

Practical things techies can print include cable organizers, monitor risers, mouse trays, or tool holders for the desk. But then, thereâ€™s so much more you can do, as it can print pretty much any 3D model! A popular recommendation is the

Bambu Lab A1 Mini 3D Printer

.

The Bambu Lab A1 Mini 3D Printer

Pixel dice.

A nice gift for tabletop gamers. Edit how the dice behaves and lights up with a companion app. The

hardware engineering details behind the dice

are an interesting read.

How the Pixel Dice is constructed

YubiKey.

A hardware two-factor authentication passkey that is generally more secure than app-based 2FA, and also more resistant to phishing. If you plan to gift one, itâ€™s worth giving two, so the person setting up the Yubikey

has a backup key

â€“ with key services like email, banking, and others â€“ in case it gets lost.

YubiKey: small and supports both USB-C and Lightning

JetKVM

â€“ control your computer remotely, with a low latency of 30-60ms. KVM stands for Keyboard, Video, Mouse: this open source gadget lets you control your machine at home, over the internet.

Read a review of this gadget

.

JetKVM: Control your computer from anywhere

5. Gaming

Nintendo Switch 2

â€“ an upgrade on its predecessor with better performance, more storage, and an improved Joy-Con design. It has a wide range of games, and likely the biggest collection of family-friendly and collaborative games, like Mario Party.

Switch 2, played with a standalone controller. Source:

Gizmodo

ModRetro Chromatic

. A Game Boy-like device created by Palmer Luckey, the founder of Oculus and Anduril. Each game comes on a cartridge, just like with old handhelds.

ModRetro Chromatic: Game Boy vibes

The game catalog is smaller than ideal â€“ but in July this year, Luckey shared big plans,

writing

:

â€œWe have a huge slate of content coming from some of the best developers in the industry, starting with an all-new physical Tetris cartridge that will come bundled with each and every Chromatic, just as it did with every original Game BoyÂ® released in North America. We will also be launching all-physical re-releases and remasters of classic Game BoyÂ® titles, entirely new IP from incredible indie developers, first-time launches of Game BoyÂ® games that were canceled before release, and even some titles that were canceled before the public ever found out about them.â€

Playdate

. A tiny-handled homage to old-school gaming. The crank on the side brings a new type of gaming experience, and the platform is home to lots of clever indie games. This device has become a family favorite in our home, and was also a recommendation in the

2023 gift guide

.

The Playdate 2. Source:

Ars Technica

Board games.

Playing board games is a great group activity. A recent guest on the

Pragmatic Engineer Podcast

, Martin Fowler, is a big fan of them, so I asked which one he recommends. He said:

â€œItâ€™s a tricky one because itâ€™s a little bit like saying â€˜Iâ€™m really into watching movies. Which one would you recommend?â€™ There are so many different tastes.

If I wanted to pick one that is not too complicated to get into, but still has a lot of richness: at the moment I would pick the game Concordia. Fairly abstract in its nature, but itâ€™s easy to get into and itâ€™s got a good bit of decision making in the process.â€

Concordia

â€“ recommended by Martin Fowler

6. Travel & wearables

A light, sturdy backpack for commuting.

Tom Karlo, PM at Meta,

recommends

the

Dyneema Daypack

as light and durable.

The Dyneema Daypack

Cadence magnetic travel containers

. Leakproof travel-size jars for storing liquids like shampoo, body wash, and vitamins, or even jewelry. Both the jar and the lid are magnetic, making them easy to keep track of. Very useful when travelling, and also handy at home.

Cadence travel containers

Bellroy Tech Kit

â€“ a pouch for keeping cables, an adapter, and a mouse, in one place when traveling.

The Bellroy Tech Kit

Meta glasses

. Launched in September, these are the best smart glasses on the market. A review by The Verge

concluded

that they offer better battery life, but â€œstill have the tricky issue of being a camera, right on your face.â€

Meta Glasses with armband

Itâ€™s new technology, novel, and something many techies might not get for themselves, but which could definitely be interesting for AR enthusiasts. Software engineer Alvin Sng â€“ who works at Factory AI â€“

used his pair

to record a â€œday in the lifeâ€ video at Factory AI.

Recording a â€œday in the life at Factory AIâ€ with Meta Glasses.

Watch the full video

Airtags

. Always a useful gift for friends or family in the Apple ecosystem for tracking objects and also wandering kids. Obvious use cases include luggage, keys, autos, and more.

Airtags keep track of stuff

Car jump starter + power bank + flashlight.

I got one of these when my carâ€™s battery died, and itâ€™s been around ever since. Itâ€™s my go-to power bank when I need to plug something in, and serves as a reserve flashlight.

Helpful when thereâ€™s a power outage, or the car wonâ€™t start

7. Books and stationery

Books

are always a great gift for the holidays, when thereâ€™s usually more time to read. Specifically for tech, I previously shared

100+ recommendations

. Since that article, a few books have been published that Iâ€™ve enjoyed:

A selection from my bookshelf

From my collection of titles:

Tidy First

by Kent Beck. A short, impactful read. Check out an excerpt:

Dead code, getting untangled, and coupling versus decoupling

The Engineering Executiveâ€™s Primer

by Will Larson. Read an excerpt:

Getting an engineering executive job

.

Frictionless

by Nicole Forsgren and Abi Noda. AI can generate code in minutes, so why does shipping software still take forever? This book answers that question, and offers practical advice on how to reduce friction in dev teams. Released just last week.

Engineering management for the rest of us

by Sarah Drasner. An approachable, practical title about how to be a better engineering leader.

AI Engineering

by Chip Huyen. Probably the best book on AI Engineering. Read an excerpt:

The AI Engineering stack

.

The Software Engineerâ€™s Guidebook

by myself, is now available in hardcover and makes for a good gift.

More on why I wrote this book.

The Scaling Era

: An Oral History of AI, 2019â€“2025 by Dwarkesh Patel. An educational overview of the evolution of LLMs. Worth reading to understand more.

Find further recommendations on

Amazon

and

Goodreads

.

Stone paper notebooks.

Stone paper is a tea-proof and waterproof sheet â€“ which makes this format really practical. Some people love the feel of writing in them and others donâ€™t, but you wonâ€™t know until you try! Popular brands include

Moyu

(erasable notebooks),

Daily Bliss

, and

Karst Goods

.

Moyu notebooks

: with erasable paper

Analog TO-DO list

.

Analog is an offline TO-DO system that works like this:

Write 10 tasks on a â€œTodayâ€ card at the beginning of the day

Mark tasks as â€œin progressâ€, â€œdelegatedâ€ or â€œcompletedâ€

Move unfinished tasks either to a new â€œTodayâ€ card, or to a â€œNextâ€ or â€œSomedayâ€ card

With so many digital TO-DO lists, an offline version feels refreshing!

Analog cards. An offline twist on TO-DO lists

2026 journal / planner.

For those in your life who are organized and like to plan â€“ and also perhaps for those who do neither, but would like to be less chaotic.

Planners for those who like structure

â€“

or want some

Small, portable photo printers

. Turning your digital snaps into physical objects with a printer can be a lot of fun, and transforms them into permanent momentos.

Instax

,

Polaroid

, and

HP Sprocket

are popular portable options. Itâ€™s also easier than ever to

print photos

and create

photo books

online if you donâ€™t want to buy a printer.

Instax mini Link 3 and the Polaroid HiÂ·Print 2x3 Gen 2

Elin uses a

Canon Selphy CP1500

for postcard-sized and credit card-sized photos, and it comes with a nifty collage feature:

Elinâ€™s vacation memories in physical form

8. Kitchen goodies

One place to escape the screen is the kitchen, and, although itâ€™s not an obvious source of gift ideas for techies, there are many cool kitchen gadgets that are useful, day-to-day.

The Scizza

. If scissors and a pizza slicer had a baby, it would be the Scizza. We got one from a friend, and it has become a family favorite, and a source of amusement for guests â€“ followed by â€œwhere can I get one?â€

The Scizza

Soda machine.

Why buy soda when you can make your own and choose the flavours? Thereâ€™s a growing selection of domestic soda makers like

Sodastream

, and

Aarke

, which is more high-end and looks gorgeous.

The Aarke carbonator

Origami Dripper Coffee Brewer

.

An aesthetically pleasing way to brew coffee.

See a guide

on how itâ€™s done.

A different way to brew coffee

Airfryer.

One of the most useful kitchen tools I own; it makes cooking healthier (no oils), convenient, and fast. Popular brands include Ninja, Cosori, Philips, Instant (by Insignia), Tefal, and Breville.

A Philips air fryer

Useful kitchen tools

: These should be welcome items for people who donâ€™t yet own them.

The

Microplane cheese grater

, the

Zyliss Susi 3 Garlic Press

, and

Cook with Color food clips

The Chefâ€™s Press

.

For people who enjoy roasted food, this helps prepare delicious meals.

The Chefâ€™s Press

Takeaways

I hope this list of seasonal gift ideas is useful. For tech professionals, the perfect gift doesnâ€™t have to be a cool gadget; it could equally be something that pulls us away from the screen! And in keeping with the sentimental tone of the season, perhaps the best gift of all which we can give is spending time with those we care about most.

If you have more suggestions, add them in the comments!

Leave a comment",https://newsletter.pragmaticengineer.com
https://blog.bytebytego.com/p/message-brokers-101-storage-replication,"Message Brokers 101: Storage, Replication, and Delivery Guarantees",,2026-01-02T16:54:37.890992,ByteByteGo,"A message broker is a middleware system that facilitates asynchronous communication between applications and services using messages.

At its core, a broker decouples producers of information from consumers, allowing them to operate independently without direct knowledge of each other. This decoupling is foundational to modern distributed architectures, where services communicate through the broker rather than directly with one another, enabling them to evolve independently without tight coupling.

To understand this in practice, consider an order-processing service that places an â€œOrder Placedâ€ message on a broker. Downstream services such as inventory, billing, and shipping will get that message from the broker when they are ready to process it, rather than the order service calling each one synchronously. This approach eliminates the need for the order service to know about or wait for these downstream systems.

Message brokers are not merely pipes for data transmission. They are sophisticated distributed databases specialized for functionalities such as stream processing and task distribution. The fundamental value proposition of a message broker lies in its ability to introduce a temporal buffer between distinct systems. By allowing a producer to emit a message without waiting for a consumer to process it, the broker facilitates temporal decoupling. This ensures that a spike in traffic at the ingress point does not immediately overwhelm downstream services.

In this article, we will look at how message brokers work in detail and explore the various patterns they enable in distributed system design.

Fundamental Terms",https://blog.bytebytego.com
https://blog.bytebytego.com/p/openai-clip-the-model-that-learnt,OpenAI CLIP: The Model That Learnt Zero-Shot Image Recognition Using Text,,2026-01-02T16:54:39.232688,ByteByteGo,"If Your API Isnâ€™t Fresh, Your Agents Arenâ€™t Either. (Sponsored)

In the agentic era, outdated retrieval breaks workflows. This

API Benchmark Report from You.com

shows how each major search API performs to reveal which can best answer real-world, time-sensitive queries.

Whatâ€™s inside:

Head-to-head benchmarks comparing You.com, Google SerpAPI, Exa, and Tavily across accuracy, latency, and cost

Critical performance data to identify which APIs best handle time-sensitive queries

A data-driven analysis of the Latency vs. Accuracy trade-off to help you select the best retrieval layer for enterprise agents

Curious who performed best?

Get the 2025 API Benchmark Report

Disclaimer: The details in this post have been derived from the details shared online by the OpenAI Engineering Team. All credit for the technical details goes to the OpenAI Engineering Team.  The links to the original articles and sources are present in the references section at the end of the post. Weâ€™ve attempted to analyze the details and provide our input about them. If you find any inaccuracies or omissions, please leave a comment, and we will do our best to fix them.

Imagine teaching a computer to recognize objects not by showing it millions of labeled photos, but by letting it browse the internet and learn from how people naturally describe images. Thatâ€™s exactly what OpenAIâ€™s CLIP does, and it represents a fundamental shift in how we teach machines to understand visual content.

CLIP (Contrastive Language-Image Pre-training) is a neural network that connects vision and language. Released in January 2021, it can classify images into any categories you want without being specifically trained for that task. Just tell it what youâ€™re looking for in plain English, and it can recognize it. This â€œzero-shotâ€ capability makes CLIP different from almost every computer vision system that came before it.

In this article, we will look at how CLIP works and the problems it tries to solve.

The Problem CLIP Solves

Traditional computer vision followed a rigid formula. If you want a model to distinguish cats from dogs, you need thousands of labeled photos. For different car models, you need another expensive dataset. For reference, ImageNet, one of the most famous image datasets, required over 25,000 workers to label 14 million images.

This approach created three major problems:

First, datasets were expensive and time-consuming to build.

Second, models became narrow specialists. An ImageNet model could recognize 1,000 categories, but adapting it to new tasks required collecting more data and retraining.

Third, models could â€œcheatâ€ by optimizing for specific benchmarks.

For example, a model achieving 76% accuracy on ImageNet might drop to 37% on sketches of the same objects, or plummet to 2.7% on slightly modified images. Models learned ImageNetâ€™s quirks rather than truly understanding visual concepts.

CLIPâ€™s approach is radically different. Instead of training on carefully labeled datasets, it learns from 400 million image-text pairs collected from across the internet. These pairs are everywhere online: Instagram photos with captions, news articles with images, product listings with descriptions, and Wikipedia entries with pictures. People naturally write text that describes, explains, or comments on images, creating an enormous source of training data.

However, CLIP doesnâ€™t try to predict specific category labels. Instead, it learns to match images with their corresponding text descriptions. During training, CLIP sees an image and a huge batch of text snippets (32,768 at a time). Its job is to determine which text snippet best matches the image.

Think of it as a massive matching game. For example, we show the system a photo of a golden retriever playing in a park. Among 32,768 text options, only one is correct: maybe â€œa golden retriever playing fetch in the park.â€ The other 32,767 options might include â€œa black cat sleeping,â€ â€œa mountain landscape at sunset,â€ â€œa person eating pizza,â€ and thousands of other descriptions. To consistently pick the right match across millions of such examples, CLIP must learn what objects, scenes, actions, and attributes look like and how they correspond to language.

By solving this matching task over and over with incredibly diverse internet data, CLIP develops a deep understanding of visual concepts and their linguistic descriptions. For example, it might learn that furry, four-legged animals with wagging tails correspond to words like â€œdogâ€ and â€œpuppyâ€. It might learn that orange and pink skies over water relate to â€œsunsetâ€ and â€œbeach.â€ In other words, it builds a rich mental model connecting the visual and linguistic worlds.

ğŸ‘‹ Goodbye low test coverage and slow QA cycles (Sponsored)

Bugs sneak out when less than 80% of user flows are tested before shipping. However, getting that kind of coverage (and staying there) is hard and pricey for any team.

QA Wolfâ€™s

AI-native solution provides high-volume, high-speed test coverage for web and mobile apps, reducing your organizationâ€™s QA cycle to minutes.

They can get you:

80% automated E2E test coverage in weeksâ€”not years

Unlimited parallel test runs

24-hour maintenance and on-demand test creation

Zero flakes, guaranteed

The benefit? No more manual E2E testing. No more slow QA cycles. No more bugs reaching production.

With QA Wolf,

Drataâ€™s team of engineers

achieved 4x more test cases and

86% faster QA cycles.

â­ Rated 4.8/5 on G2

Schedule a demo to learn more

The Technical Foundation

Under the hood, CLIP uses two separate neural networks working in tandem: an image encoder and a text encoder.

The image encoder takes raw pixels and converts them into a numerical vector (called an embedding). The text encoder takes words and sentences and also outputs a vector. The key insight is that both encoders output vectors in the same dimensional space, making them directly comparable.

Initially, these encoders may produce completely random, meaningless vectors. For example, an image of a dog might become [0.2, -0.7, 0.3, ...] while the text â€œdogâ€ becomes [-0.5, 0.1, 0.9, ...]. These numbers have no relationship whatsoever. But hereâ€™s where training works its magic.

The training process uses whatâ€™s called a contrastive loss function. This is simply a mathematical way of measuring how wrong the model currently is. For correct image-text pairs (like a dog image with â€œdog playing fetchâ€), the loss function says these embeddings should be very similar. For incorrect pairs (like a dog image with â€œcat sleepingâ€), it says they should be very different. The loss function produces a single number representing the total error across all images and texts in a batch.

See the diagram below:

Source:

OpenAI Research Blog

Then comes backpropagation, the fundamental learning mechanism in neural networks. It calculates exactly how each weight in both encoders should change to reduce this error. The weights update slightly, and the process repeats millions of times with different batches of data. Gradually, both encoders learn to produce similar vectors for matching concepts. For example, images of dogs start producing vectors near where the text encoder puts the word â€œdogâ€.

In other words, through the continuous pressure to match correct pairs and separate incorrect ones across millions of diverse examples, the encoders evolve to speak the same language.

Zero-Shot Classification in Action

Once CLIP is trained, its zero-shot capabilities become evident. Suppose we want to classify images as containing either dogs or cats. We donâ€™t need to retrain CLIP or show it labeled examples.

Instead, we can simply take the image and pass it through the image encoder to get an embedding. Next, we can take the text â€œa photo of a dogâ€ and pass it through the text encoder to get another embedding. Further on, we can take the text â€œa photo of a catâ€ and get a third embedding. Compare which text embedding is closer to the image embedding, which would be the answer.

See the diagram below:

Source:

OpenAI Research Blog

CLIP is essentially asking: â€œBased on everything learned from the internet, would this image more likely appear with text about dogs or text about cats?â€

Since it learned from such diverse data, this approach works for nearly any classification task you can describe in words.

Want to classify types of food? Use â€œa photo of pizza,â€ â€œa photo of sushi,â€ â€œa photo of tacosâ€ as your categories. Need to analyze satellite imagery? Try â€œa satellite photo of a forest,â€ â€œa satellite photo of a city,â€ â€œa satellite photo of farmland.â€ Working with medical images? You could use â€œan X-ray showing pneumoniaâ€ versus â€œan X-ray of healthy lungs.â€ You just change the text descriptions. No retraining required.

This flexibility is transformative. Traditional models needed extensive labeled datasets for each new task. CLIP can tackle new tasks immediately, limited only by your ability to describe categories in natural language.

Design Choices That Made CLIP Possible

CLIPâ€™s success wasnâ€™t just about the core idea. OpenAI made two critical technical decisions that made training computationally feasible.

First, they chose contrastive learning over the more obvious approach of training the model to generate image captions. Early experiments tried teaching systems to look at images and produce full text descriptions word by word, similar to how language models generate text. While intuitive, this approach proved incredibly slow and computationally expensive. Generating entire sentences requires much more computation than simply learning to match images with text. Contrastive learning turned out to be 4 to 10 times more efficient for achieving good zero-shot performance.

Second, they adopted Vision Transformers for the image encoder. Transformers, the architecture behind GPT and BERT, had already revolutionized natural language processing. Applying them to images (treating image patches like words in a sentence) provided another 3x computational efficiency gain over traditional convolutional neural networks like ResNet.

Source:

OpenAI Research Blog

Combined, these choices meant CLIP could be trained on 256 GPUs for two weeks, similar to other large-scale vision models of the time, rather than requiring astronomically more compute.

Conclusion

OpenAI tested CLIP on over 30 different datasets covering diverse tasks: fine-grained classification, optical character recognition, action recognition, geographic localization, and satellite imagery analysis.

The results validated CLIPâ€™s approach. While matching ResNet-50â€™s 76.2% accuracy on standard ImageNet, CLIP outperformed the best publicly available ImageNet model on 20 out of 26 transfer learning benchmarks. More importantly, CLIP maintained strong performance on stress tests where traditional models collapsed. On ImageNet Sketch, CLIP achieved 60.2% versus ResNetâ€™s 25.2%. On adversarial examples, CLIP scored 77.1% compared to ResNetâ€™s 2.7%.

Source:

OpenAI Research Blog

However, the model still struggles with some things, such as:

Tasks requiring precise spatial reasoning or counting. It also has difficulty with very fine-grained distinctions, like differentiating between similar car models or aircraft variants where subtle details matter.

When tested on handwritten digits from the MNIST dataset (a task considered trivial in computer vision), CLIP achieved only 88% accuracy, well below the 99.75% human performance.

CLIP exhibits sensitivity to how you phrase your text prompts. Sometimes it requires trial and error (â€prompt engineeringâ€) to find wording that works well.

CLIP inherits biases from its internet training data. The way we phrase categories can dramatically influence model behavior in problematic ways.

However, despite the limitations, CLIP demonstrates that the approach powering recent breakthroughs in natural language processing (learning from massive amounts of internet text) can transfer to computer vision. Just as GPT models learned to perform diverse language tasks by training on internet text, CLIP learned diverse visual tasks by training on internet image-text pairs.

Since its release, CLIP has become foundational infrastructure across the AI industry. Itâ€™s fully open source, catalyzing widespread adoption. Modern text-to-image systems like Stable Diffusion and DALL-E use CLIP-like models to understand text prompts. Companies employ it for image search, content moderation, and recommendations.

References:

CLIP: Connecting Text and Images

What is ImageNet

SPONSOR US

Get your product in front of more than 1,000,000 tech professionals.

Our newsletter puts your products and services directly in front of an audience that matters - hundreds of thousands of engineering leaders and senior engineers - who have influence over significant tech decisions and big purchases.

Space Fills Up Fast - Reserve Today

Ad spots typically sell out about 4 weeks in advance. To ensure your ad reaches this influential audience, reserve your space now by emailing

sponsorship@bytebytego.com.",https://blog.bytebytego.com
https://blog.bytebytego.com/p/ep195-common-network-protocols-every,EP195: Common Network Protocols Every Engineer Should Know,,2026-01-02T16:54:40.334741,ByteByteGo,"The 5-Step Playbook for Unlocking the Value of AI (Sponsored)

Successful AI transformation starts with deeply understanding your organizationâ€™s most critical use cases.

This practical guide

from

You.com

walks through a proven framework to identify, prioritize, and document high-value AI opportunities.

In this AI Use Case Discovery Guide, youâ€™ll learn how to:

Map internal workflows and customer journeys to pinpoint where AI can drive measurable ROI

Ask the right questions when it comes to AI use cases

Align cross-functional teams and stakeholders for a unified, scalable approach

Get the Guide

This weekâ€™s system design refresher:

Common Network Protocols Every Engineer Should Know

ğŸš€ Learn AI in the New Year! Become an AI Engineer | Learn by Doing | Cohort 3

8 Popular Network Protocols

9 best practices for developing microservices

SPONSOR US

Common Network Protocols Every Engineer Should Know

Ever wonder what actually happens when you click ""Send"" on an email or join a video call? Every click, message, and API call on the internet relies on network protocols. They define how data moves, who can talk, and how securely it all happens.

At the foundation are transport protocols: TCP ensures reliable delivery, UDP prioritizes speed, and QUIC brings both worlds together over UDP.

On top of that, HTTP powers the web, TLS secures it, and DNS translates names into addresses.

Need remote access? Thatâ€™s SSH. File transfers? SFTP or SMB.

Real-time chat and media? WebSocket, WebRTC, and MQTT keep data flowing live.

For identity and access, OAuth and OpenID handle authorization and authentication.

In the backend, DHCP, NTP, ICMPv6, and LDAP quietly keep everything synchronized, addressed, and discoverable.

From simple emails (SMTP, IMAP) to encrypted VPNs (WireGuard, IPsec), these protocols form the invisible language that keeps the internet connected and secure.

Over to you: If one protocol suddenly stopped working worldwide, which one would break the internet first?

ğŸš€ Learn AI in the New Year! Become an AI Engineer | Learn by Doing | Cohort 3

After the amazing success of Cohorts 1 and 2 (with close to 1,000 engineers joining and building real AI skills), Iâ€™m excited to announce the launch of Cohort 3 of Become an AI Engineer!

This isnâ€™t just another course on AI tools and frameworks. Our mission is to equip engineers with the solid foundation and complete end-to-end skill set required to excel as AI engineers in todayâ€™s fast-moving world.

Hereâ€™s what sets this cohort apart:

Learn by doing: Build real-world AI applications hands-on, far beyond just watching videos.

Structured, systematic curriculum: Progress step by step from core fundamentals to advanced concepts in a carefully crafted learning path.

Live feedback and mentorship: Receive direct guidance and reviews from experienced instructors and peers.

Strong community support: Learning solo is tough â€” learning together with a motivated community makes it enjoyable and effective!

If you missed the previous cohorts and want to ğ¥ğğšğ«ğ§ ğ€ğˆ ğ¢ğ§ ğ­ğ¡ğ ğğğ° ğ˜ğğšğ«, this is your perfect opportunity to join Cohort 3 and level up your AI engineering career.

Check it out here

8 Popular Network Protocols

Network protocols are the key to transferring data between two systems in a network.

FTP (File Transfer Protocol)

Uses separate control and data channels to upload and download files between a client and server.

TCP (Transmission Control Protocol)

Establishes a reliable connection using a 3-way handshake (SYN, SYN+ACK, ACK) for accurate data delivery.

UDP (User Datagram Protocol)

Sends lightweight, connectionless packets (requests and responses) with minimal latency. Ideal for fast transmissions.

HTTP (HyperText Transfer Protocol)

Uses TCP to request and receive web resources (HTML, images) through HTTP requests and responses.

HTTP/3 (QUIC)

Built on top of UDP, it enables faster and more reliable connections by multiplexing data streams and reducing latency.

HTTPS (Secure HTTP)

Secures HTTP with encryption using public and session keys over a TCP connection, thereby protecting web data.

SMTP (Simple Mail Transfer Protocol)

Transfer emails from a sender to a recipient through an SMTP server. It is commonly used for email delivery.

WebSocket

Upgrades an HTTP connection to a full-duplex channel for real-time, bidirectional communication like live chats.

A picture is worth a thousand words: 9 best practices for developing microservices

When we develop microservices, we need to follow the following best practices:

Use separate data storage for each microservice

Keep code at a similar level of maturity

Separate build for each microservice

Assign each microservice with a single responsibility

Deploy into containers

Design stateless services

Adopt domain-driven design

Design micro frontend

Orchestrating microservices

SPONSOR US

Get your product in front of more than 1,000,000 tech professionals.

Our newsletter puts your products and services directly in front of an audience that matters - hundreds of thousands of engineering leaders and senior engineers - who have influence over significant tech decisions and big purchases.

Space Fills Up Fast - Reserve Today

Ad spots typically sell out about 4 weeks in advance. To ensure your ad reaches this influential audience, reserve your space now by emailing

sponsorship@bytebytego.com.",https://blog.bytebytego.com
https://blog.bytebytego.com/p/learn-ai-in-the-new-year-become-an,ğŸš€ Learn AI in the New Year: Become an AI Engineer Cohort 3 Now Open,,2026-01-02T16:54:42.477085,ByteByteGo,"After the amazing success of Cohorts 1 and 2 (with close to 1,000 engineers joining and building real AI skills), we are excited to announce the launch of Cohort 3 of Become an AI Engineer!

Check it out Here

Check it out Here

This is not just another course about AI frameworks and tools. Our goal is to help engineers build the foundation and end to end skill set needed to thrive as AI engineers.

Hereâ€™s what makes this cohort special:

â€¢ Learn by doing: Build real world AI applications, not just by watching videos.

â€¢ Structured, systematic learning path: Follow a carefully designed curriculum that takes you step by step, from fundamentals to advanced topics.

â€¢ Live feedback and mentorship: Get direct feedback from instructors and peers.

â€¢ Community driven: Learning alone is hard. Learning with a community is easy!

We are focused on skill building, not just theory or passive learning. Our goal is for every participant to walk away with a strong foundation for building AI systems.

If you want to start learning AI from scratch, this is the perfect time to begin.

Check it out Here",https://blog.bytebytego.com
https://blog.bytebytego.com/p/how-shopify-prepares-for-black-friday,How Shopify Prepares for Black Friday,,2026-01-02T16:54:44.055320,ByteByteGo,"Power real-time apps and AI agents with Redis (Sponsored)

Real-time isnâ€™t just about speed. Itâ€™s about instant, fresh, and reliable responses at scale.

This definitive Redis guide breaks down how to architect a real-time data layer that keeps user experiences snappy, AI agents responsive, and data up to date across your stack.

Inside, youâ€™ll learn:

How to get your apps from â€œfastâ€ to truly real-time

The role of Redis in low-latency caching, vector search, AI agent memory, and streaming workloads

Real-world patterns from companies using Redis to cut latency, reduce drop-offs, and keep users in flow

Download now

Note: This article is written in collaboration with the Shopify engineering team. Special thanks to the Shopify engineering team for sharing details with us about their Black Friday Cyber Monday preparation work and also for reviewing the final article before publication. All credit for the technical details shared in this article goes to the Shopify Engineering Team.

Black Friday Cyber Monday (BFCM) 2024 was massive for Shopify. The platform processed 57.3 petabytes of data, handled 10.5 trillion database queries, and peaked at 284 million requests per minute on its edge network. On app servers alone, they handled 80 million requests per minute while pushing 12 terabytes of data every minute on Black Friday.

Hereâ€™s the interesting part: this level of traffic is now the baseline for Shopify. And BFCM 2025  was even bigger, serving 90 petabytes of data, handling 1.75 trillion database writes with peak performance at 489 million requests per minute. This is why Shopify rebuilt its entire BFCM readiness program from scratch.

The preparation involved thousands of engineers working for nine months, running five major scale tests.

In this article, we will look at how Shopify prepared for success during the Super Bowl of commerce

The Three-Track Framework

Shopifyâ€™s BFCM preparation started in March with a multi-region strategy on Google Cloud.

The engineering team organized the work into three parallel tracks that run simultaneously and influence each other:

Capacity Planning involves modeling traffic patterns using historical data and merchant growth projections. The team submits these estimates to their cloud providers early so the providers can ensure they have enough physical infrastructure available. This planning defines how much computing power Shopify needs and where it needs to be located geographically.

The Infrastructure Roadmap is where the team reviews their technology stack, evaluates what architectural changes are needed, and identifies system upgrades required to hit their target capacity. This track helps sequence all the work ahead. Importantly, Shopify never uses BFCM as a release deadline. Every architectural change and migration happens months before the critical window.

Risk Assessments use â€œWhat Could Go Wrongâ€ exercises to document failure scenarios. The team sets escalation priorities and generates inputs for what they call Game Days. This intelligence helps them test and harden systems well in advance.

These three tracks constantly feed into each other. For example, risk findings might reveal capacity gaps the team didnâ€™t account for. Infrastructure changes might introduce new risks that need assessment. In other words, itâ€™s a continuous feedback loop.

Game Days

To assess risks properly, the Shopify engineering team runs Game Days. These are chaos engineering exercises that intentionally simulate production failures at the BFCM scale.

The team started hosting Game Days in early spring. This involves deliberately injecting faults into the systems to test how they respond under failure conditions. Think of it like a fire drill, but for software.

During these Game Days, the engineering team focuses extra attention on what they call â€œcritical journeysâ€. These are the most business-critical paths through their platform: checkout, payment processing, order creation, and fulfillment. If these break during BFCM, merchants lose sales immediately.

Critical Journey Game Days run cross-system disaster simulations. Here are some common aspects that are tested by the team:

The team tests search and pages endpoints while randomizing navigation to mimic real user behavior. They inject network faults and latency to see what happens when services canâ€™t communicate quickly.

They bust caches to create realistic load patterns instead of the artificially fast responses you get when everything is cached.

Frontend teams run bug bashes during these exercises. They identify regressions, test critical user flows, and validate that the user experience holds up under peak load conditions.

These exercises build muscle memory for incident response by exposing gaps in operational playbooks and monitoring tools.

Most importantly, Shopify closes those gaps well ahead of BFCM instead of discovering them when merchants need the platform most. All findings from Game Days feed into what Shopify calls the Resiliency Matrix. This is centralized documentation that tracks vulnerabilities, incident response procedures, and fixes across the entire platform.

The Resiliency Matrix includes five key components.

First is service status, showing the current operational state of all critical services.

Second is failure scenarios that document how things can break and what the impact would be.

Third is recovery procedures, listing expected recovery time objectives and detailed runbooks for fixing issues.

Fourth is operational playbooks with step-by-step incident response guides.

Fifth is on-call coverage showing team schedules and PagerDuty escalation paths.

The Matrix becomes the roadmap for system hardening before BFCM. Teams update it continuously throughout the year, documenting resilience improvements as they go.

Load Testing with Genghis and Toxiproxy

Game Days test components in isolation, but Shopify also needs to know if the entire platform can handle BFCM volumes. Thatâ€™s where load testing comes in.

The engineering team built a tool called Genghis that runs scripted workflows mimicking real user behavior. It simulates browsing, adding items to the cart, and going through checkout flows. The tool gradually ramps up traffic until something breaks, which helps the team find their actual capacity limits.

Tests run on production infrastructure simultaneously from three Google Cloud regions: us-central, us-east, and europe-west4. This simulates global traffic patterns accurately. Genghis also injects flash sale bursts on top of baseline load to test peak capacity scenarios.

Shopify pairs Genghis with Toxiproxy, an open-source framework they built for simulating network conditions. Toxiproxy injects network failures and partitions that prevent services from reaching each other. For reference, a network partition is when two parts of your system lose the ability to communicate, even though both are still running.

During tests, teams monitor dashboards in real time and are ready to abort if systems begin to degrade. Multiple teams coordinate to find and fix bottlenecks as they emerge.

When load testing reveals limits, teams have three options:

Horizontal scaling means adding more instances of the application.

Vertical scaling means giving each instance more resources, such as CPU and memory.

Optimizations mean making architecture-level changes that improve performance, ranging from better database queries to performance tuning across consuming layers up to the frontend.

These decisions set the final BFCM capacity and drive optimization work across Shopifyâ€™s entire stack. The key insight is that the team cannot wait until BFCM to discover the capacity limits. It takes months of preparation to scale infrastructure and optimize code.

The Analytics Platform Challenge

BFCM tests every system at Shopify, but 2025 presented a unique challenge. Part of their infrastructure had never experienced holiday traffic, which creates a problem: how do you prepare for peak load when you have no historical data to model from?

In 2024, Shopifyâ€™s engineering team rebuilt its entire analytics platform. They created new ETL pipelines. ETL stands for Extract, Transform, Load, which is the process of pulling data from various sources, processing it, and storing it somewhere useful. They also switched the persistence layer and replaced their legacy system with completely new APIs.

This created an asymmetry. The ETL pipelines ran through BFCM 2024, so the team had one full season of production data showing how those pipelines perform under holiday load. But their API layer launched after peak season ended. They were preparing for BFCM on APIs that had never seen holiday traffic.

This matters a lot because during BFCM, merchants obsessively check their analytics. They want real-time sales numbers, conversion rates, traffic patterns, and data about popular products. Every single one of these queries hits the API layer. If those APIs canâ€™t handle the load, merchants lose visibility during their most critical sales period.

Shopify ran Game Days specifically for the analytics infrastructure. These were controlled experiments designed to reveal failure modes and bottlenecks. The team simulated increased traffic loads, introduced database latency, and tested cache failures to systematically map how the system behaves under stress.

The results showed four critical issues that needed fixes:

First, the ETL pipelines needed Kafka partition increases to maintain data freshness during traffic spikes. Apache Kafka is a distributed streaming platform that handles real-time data flows. More partitions mean more parallel processing, which keeps data fresh for the APIs to serve.

Second, the API layer memory usage required optimization. The team found this through profiling, which means measuring exactly how the code uses memory. Each API request was using too much memory. Under high load, this would cause out-of-memory errors, slower response times, or complete crashes.

Third, connection timeouts needed tuning to prevent pool exhaustion. A connection pool is a set of reusable database connections. Creating new connections is expensive, so applications reuse them. The problem was that timeouts were too long, meaning connections would get stuck waiting. Under high load, you run out of available connections, and new requests start failing. Shopify tuned the timeouts to release connections faster.

Fourth, the team split API requests through a different load balancer approach. Originally, API requests would all enqueue to one region, which added latency and load. By scaling up the secondary regionâ€™s cluster and updating the load balancing policy,  they better distributed the work and prevented API servers from being overwhelmed.

Beyond the performance fixes, the team validated alerting and documented response procedures. Their teams were trained and prepared to handle failures during the actual event.

The Scale Tests

Game Days and load testing prepare individual components, but scale testing is different. It validates the entire platform working together at BFCM volumes, revealing issues that only surface when everything runs at capacity simultaneously.

From April through October, Shopify ran five major scale tests at their forecasted traffic levels, specifically their peak p90 traffic assumptions. In statistics, p90 means the 90th percentile, or the traffic level that 90% of requests will be below.

Here are the details of those scale tests:

The first two tests validated baseline performance against 2024â€™s actual numbers.

Tests three through five ramped up to 2025 projections, targeting 150% of last yearâ€™s load.

By the fourth test, Shopify hit 146 million requests per minute and over 80,000 checkouts per minute. On the final test of the year, they tested their p99 scenario, which reached 200 million requests per minute.

These tests are extraordinarily large, and therefore, Shopify runs them at night and coordinates with YouTube because the tests impact shared cloud infrastructure. The team tested resilience, not just raw load capacity. They executed regional failovers, evacuating traffic from core US and EU regions to validate their disaster recovery procedures actually work.

Shopify ran four types of tests:

Architecture scale-up tests validated that their infrastructure handles planned capacity.

Load tests during normal operations established baseline performance at peak load.

Load tests with failover validated disaster recovery and cross-region failover capabilities.

Game Day simulations tested cross-system resilience through chaos engineering.

The team simulated real user behavior, such as storefront browsing and checkout, admin API traffic from apps and integrations, analytics and reporting loads, and backend webhook processing. They also tested critical scenarios like sustained peak load, regional failover, and cascading failures where multiple systems fail simultaneously.

Each test cycle identified issues that would never appear under steady-state load, and the team fixed each issue as it emerged. Some of the key issues were as follows:

Scale Tests 1 and 2 revealed that under heavy load, core operations threw errors, and checkout queues backed up.

Scale Test 3 validated key migrations and confirmed that regional routing behaved as expected after infrastructure changes.

Scale Test 4 hit limits that triggered an unplanned failover, identifying priority issues in test traffic routing and discovering delays when bringing regions back online during rebalancing.

Scale Test 5 performed a full dress rehearsal and was the only test run during North American business hours to simulate real BFCM conditions. All the other tests ran at night.

Mid-program, Shopify made an important shift. They added authenticated checkout flows to their test scenarios. Modeling real logged-in buyers exposed rate-limiting code paths that anonymous browsing never touches. Even though authenticated flows were a small percentage of traffic, they revealed bottlenecks that would have caused problems during the real event.

BFCM Weekend Operations

BFCM preparation gets Shopify ready, but operational excellence keeps them steady when traffic actually spikes.

The operational plan coordinates engineering teams, incident response, and live system tuning. Here are the key components of this plan:

The plan for BFCM weekend includes real-time monitoring with dashboard visibility across all regions and automated alerts.

For incident response, Incident Manager OnCall teams provide 24/7 coverage with clear escalation paths.

Merchant communications ensure stores get status updates and notifications about any issues.

Live optimization allows system tuning based on real-time traffic patterns as they develop.

After BFCM ends, the post-mortem process correlates monitoring data with actual merchant outcomes to understand what worked and what needs improvement.

The philosophy is simple: preparation gets you ready, but operational excellence keeps you steady.

Conclusion

Shopifyâ€™s 2025 BFCM readiness program shows what systematic preparation looks like at scale. Thousands of engineers worked for nine months, running five major scale tests that pushed their infrastructure to 150% of expected load. They executed regional failovers, ran chaos engineering exercises, documented system vulnerabilities, and hardened systems with updated runbooks before merchants needed them.

What makes this different from typical pre-launch preparation is the systematic approach. Most companies load test once, maybe twice, fix critical bugs, and hope for the best. Shopify spent nine months continuously testing, finding breaking points, fixing issues, and validating that the fixes actually work.

Also, the tools Shopify built arenâ€™t temporary BFCM scaffolding. The Resiliency Matrix, Critical Journey Game Days, and real-time adaptive forecasting became permanent infrastructure improvements. They make Shopify more resilient every day, not just during peak season.

To provide a visualization of BFCM, Shopify also launched an interesting pinball game to showcase the Shopify Live Globe. The game itself runs at 120fps in a browser with a full 3d environment, physics engine, and VR Support. Behind the scenes, the game is a three[dot]js app built with â€œreact-three-fiberâ€. Every merchant sale shows up a few seconds later on this globe. Everyone can check out the game and the visualization on the homepage for

Shopify Live Globe

References:

How we prepare Shopify for BFCM

Extract, Transform, Load

Toxiproxy

Shopify Live Globe

Details about the Shopify Live Globe Pinball Game

SPONSOR US

Get your product in front of more than 1,000,000 tech professionals.

Our newsletter puts your products and services directly in front of an audience that matters - hundreds of thousands of engineering leaders and senior engineers - who have influence over significant tech decisions and big purchases.

Space Fills Up Fast - Reserve Today

Ad spots typically sell out about 4 weeks in advance. To ensure your ad reaches this influential audience, reserve your space now by emailing

sponsorship@bytebytego.com.",https://blog.bytebytego.com
https://blog.bytebytego.com/p/multimodal-llms-basics-how-llms-process,"Multimodal LLMs Basics: How LLMs Process Text, Images, Audio & Videos",,2026-01-02T16:54:45.149071,ByteByteGo,"Yesterdayâ€™s data canâ€™t answer todayâ€™s questions. (Sponsored)

Static training data canâ€™t keep up with fast-changing information, leaving your models to guess. We recommend

this technical guide

from

You.com

, which gives developers the code and framework to connect GenAI apps to the live web for accurate, real-time insights.

What youâ€™ll get:

A step-by-step Python tutorial to integrate real-time search with a single GET request

The exact code logic to build a â€œReal-Time Market Intelligence Agentâ€ that automates daily briefings

Best practices for optimizing latency, ensuring zero data retention, and establishing traceability

Turn â€œoutdatedâ€ into â€œreal-time.â€

Download the API Integration Guide

For a long time, AI systems were specialists confined to a single sense. For example:

Computer vision models could identify objects in photographs, but couldnâ€™t describe what they saw.

Natural language processing systems could write eloquent prose but remained blind to images.

Audio processing models could transcribe speech, but had no visual context.

This fragmentation represented a fundamental departure from how humans experience the world. Human cognition is inherently multimodal. We donâ€™t just read text or just see images. We simultaneously observe facial expressions while listening to the tone of voice. We connect the visual shape of a dog with the sound of a bark and the written word â€œdog.â€

To create AI that truly operates in the real world, these separated sensory channels needed to converge.

Multimodal Large Language Models represent this convergence. For example, GPT-4o can respond to voice input in just 232 milliseconds, matching human conversation speed. Googleâ€™s Gemini can process an entire hour of video in a single prompt.

These capabilities emerge from a single unified neural network that can see, hear, and read simultaneously.

But how does a single AI system understand such fundamentally different types of data? In this article, we try to answer this question.

Cut Complexity and Drive Growth with Automation (Sponsored)

What if you could spend most of your IT resources on innovation, not maintenance?

The latest report from the IBM Institute for Business Value explores how businesses are using intelligent automation to get more out of their technology, drive growth & cost the cost of complexity.

Get the insights

A Shared Mathematical Language

The core breakthrough behind multimodal LLMs is quite simple. Every type of input, whether text, images, or audio, gets converted into the same type of mathematical representation called embedding vectors. Just as human brains convert light photons, sound waves, and written symbols into uniform neural signals, multimodal LLMs convert diverse data types into vectors that occupy the same mathematical space.

Let us consider a concrete example. A photograph of a dog, the spoken word â€œdog,â€ and the written text â€œdogâ€ all get transformed into points in a high-dimensional mathematical space. These points cluster together, close to each other, because they represent the same concept.

This unified representation enables what researchers call cross-modal reasoning. The model can understand that a barking sound, a photo of a golden retriever, and the sentence â€œthe dog is happyâ€ all relate to the same underlying concept. The model doesnâ€™t need separate systems for each modality. Instead, it processes everything through a single architecture that treats visual patches and audio segments just like text tokens.

The Three-Part Architecture: Building Blocks of Multimodal LLM

The diagram below shows the high-level view of a multimodal LLM works:

Modern multimodal LLMs consist of three essential components working together to process diverse inputs.

Modality-Specific Encoders

The first component handles the translation of raw sensory data into initial mathematical representations.

Vision Transformers process images by treating them like sentences, dividing photographs into small patches and processing each patch as if it were a word.

Audio encoders convert sound waves into spectrograms, which are visual-like representations showing how frequencies change over time.

These encoders are typically pre-trained on massive datasets to become highly skilled at their specific tasks.

Projection Layers: The Translator

The second component acts as a bridge. Even though both encoders produce vectors, these vectors exist in different mathematical spaces. In other words, the vision encoderâ€™s representation of â€œcatâ€ lives in a different geometric region than the language modelâ€™s representation of the word â€œcat.â€

Projection layers align these different representations into the shared space where the language model operates. Often, these projectors are surprisingly simple, sometimes just a linear transformation or a small two-layer neural network. Despite their simplicity, theyâ€™re crucial for enabling the model to understand visual and auditory concepts.

Language Model Backbone

The third component is the core LLM, such as GPT or LLaMA.

This is the â€œbrainâ€ that does the actual reasoning and generates responses. It receives all inputs as sequences of tokens, whether those tokens originated from text, image patches, or audio segments.

The language model treats them identically, processing everything through the same transformer architecture that powers text-only models. This unified processing is what allows the model to reason across modalities as naturally as it handles pure text.

See the diagram below that shows the transformers architecture:

How Images Become Something an LLM Can Understand

The breakthrough that enabled modern multimodal vision came from a 2020 paper with a memorable title: â€œAn Image is Worth 16x16 Words.â€ This paper introduced the idea of processing images exactly like sentences by treating small patches as tokens.

The process works through several steps:

First, the image gets divided into a grid of fixed-size patches, typically 16x16 pixels each.

A standard 224x224 pixel image becomes approximately 196 distinct patches, each representing a small square region.

Each patch is flattened from a 2D grid into a 1D vector of numbers representing pixel intensities.

Positional embeddings are added so the model knows where each patch came from in the original image.

These patch embeddings flow through transformer layers, where attention mechanisms allow patches to learn from each other.

The attention mechanism is where understanding emerges. A patch showing a dogâ€™s ear learns it connects to nearby patches showing the dogâ€™s face and body. Patches depicting a beach scene learn to associate with each other to represent the broader context of sand and water. By the final layer, these visual tokens carry rich contextual information. The model doesnâ€™t just see â€œbrown pixelsâ€ but understands â€œgolden retriever sitting on beach.â€

OpenAI CLIP

The second critical innovation was CLIP, developed by OpenAI. CLIP revolutionized how vision encoders are trained by changing the fundamental objective. Instead of training on labeled image categories, CLIP was trained on 400 million pairs of images and their text captions from the internet.

CLIP uses a contrastive learning approach. Given a batch of image-text pairs, it computes embeddings for all images and all text descriptions. The goal is to maximize the similarity between embeddings of correct image-text pairs while minimizing similarity between incorrect pairings. An image of a dog should produce a vector close to the caption â€œa dog in the parkâ€ but far from â€œa plate of pasta.â€

How Audio Becomes Understandable

Audio presents unique challenges for language models.

Unlike text, which naturally divides into discrete words, or images, which can be divided into spatial patches, sound is continuous and temporal. For example, a 30-second audio clip sampled at 16,000 Hz contains 480,000 individual data points. Feeding this massive stream of numbers directly into a transformer is computationally impossible and inefficient. The solution requires converting audio into a more tractable representation.

The key innovation is transforming audio into spectrograms, which are essentially images of sound. The process involves several mathematical transformations:

The long audio signal gets sliced into tiny overlapping windows, typically 25 milliseconds each.

A Fast Fourier Transform extracts which frequencies are present in each window

These frequencies are mapped onto the mel scale, which matches human hearing sensitivity by giving more resolution to lower frequencies

The result is a 2D heat map where time runs along one axis, frequency along the other, and color intensity represents volume

This mel-spectrogram looks like an image to the AI model. For a 30-second clip, this might produce an 80x3,000 grid, which is essentially a visual representation of acoustic patterns that can be processed similarly to photographs.

Once audio is converted to a spectrogram, models can apply the same techniques used for vision. The Audio Spectrogram Transformer divides the spectrogram into patches, just as an image is divided. For example, models like Whisper, trained on 680,000 hours of multilingual audio, excel at this transformation.

The Training Process

The training process goes through different stages:

Stage 1: Feature Alignment

Training a multimodal LLM typically happens in two distinct stages.

The first stage focuses purely on alignment, teaching the model that visual and textual representations of the same concept should be similar. During this stage, both the pre-trained vision encoder and the pre-trained language model remain frozen. Only the projection layerâ€™s weights get updated through training.

Stage 2: Visual Instruction Tuning

Alignment alone isnâ€™t sufficient for practical use. A model might describe whatâ€™s in an image but fail at complex tasks like â€œWhy does the person look sad?â€ or â€œCompare the two chartsâ€.

Visual instruction tuning addresses this by training the model to follow sophisticated multimodal instructions.

During this stage, the projection layer continues training and the language model is also updated, often using parameter-efficient methods. The training data shifts to instruction-response datasets formatted as conversations.

An important innovation here was using GPT-4 to generate synthetic training data. Researchers fed GPT-4 textual descriptions of images and prompted it to create realistic conversations about those images. Training on this synthetic but high-quality data effectively distills GPT-4â€™s reasoning capabilities into the multimodal model, teaching it to engage in nuanced visual dialogue rather than just describing what it sees.

Conclusion

Multimodal LLMs achieve their remarkable capabilities through a unifying principle. By converting all inputs into sequences of embedding vectors that occupy a shared mathematical space, a single transformer architecture can reason across modalities as fluidly as it processes language alone.

The architectural innovations powering this capability represent genuine advances: Vision Transformers treating images as visual sentences, contrastive learning aligning modalities without explicit labels, and cross-attention enabling selective information retrieval across different data types.

The future points toward any-to-any models that can both understand and generate all modalities. In other words, a model that outputs text, generates images, and synthesizes speech in a single response.

SPONSOR US

Get your product in front of more than 1,000,000 tech professionals.

Our newsletter puts your products and services directly in front of an audience that matters - hundreds of thousands of engineering leaders and senior engineers - who have influence over significant tech decisions and big purchases.

Space Fills Up Fast - Reserve Today

Ad spots typically sell out about 4 weeks in advance. To ensure your ad reaches this influential audience, reserve your space now by emailing

sponsorship@bytebytego.com.",https://blog.bytebytego.com
https://blog.bytebytego.com/p/ep194-evolution-of-http,EP194: Evolution of HTTP,,2026-01-02T16:54:49.522720,ByteByteGo,"âœ‚ï¸ Cut your QA cycles down to minutes with QA Wolf (Sponsored)

If slow QA processes bottleneck you or your software engineering team and youâ€™re releasing slower because of it â€” you need to check out QA Wolf.

QA Wolfâ€™s AI-native service

supports web and mobile apps

, delivering

80% automated test coverage in weeks

and helping teams

ship 5x faster

by reducing QA cycles to minutes.

QA Wolf

takes testing off your plate. They can get you:

Unlimited parallel test runs for mobile and web apps

24-hour maintenance and on-demand test creation

Human-verified bug reports sent directly to your team

Zero flakes guaranteed

The benefit? No more manual E2E testing. No more slow QA cycles. No more bugs reaching production.

With QA Wolf,

Drataâ€™s team of 80+ engineers

achieved 4x more test cases and

86% faster QA cycles

.

Schedule a demo to learn more

This weekâ€™s system design refresher:

Evolution of HTTP

System Performance Metrics Every Engineer Should Know

Why Is Nginx So Popular?

Network Debugging Commands Every Engineer Should Know

Hub, Switch, & Router Explained

SPONSOR US

Evolution of HTTP

The Hypertext Transfer Protocol (HTTP) has evolved over the years to meet the needs of modern applications, from simple text delivery to high-performance, real-time experiences.

Here is how HTTP has progressed:

HTTP/0.9: Built to fetch simple HTML documents with a single GET request.

HTTP/1.0: Added headers and status codes to support richer interactions, but every request still required a new connection.

HTTP/1.1: Introduced persistent connections and more methods, making the web faster and more efficient for everyday browsing.

HTTP/2: Solved performance bottlenecks with multiplexing, enabling multiple requests to share one connection.

HTTP/3 (QUIC): Shifted to UDP with QUIC to reduce latency and improve reliability, especially for mobile and real-time apps.

Over to you: Are you already taking advantage of HTTP/3 in your projects?

2026 OKR: Use AI to make MTTR 70% faster & optimize costs (Sponsored)

Code from Claude is about to hit prod, but it doesnâ€™t have to be painful.

Engineering teams at Coinbase, Toast, Gametime, MSCI, and Zscaler use Resolve AI to resolve incidents, optimize costs, and build with production context using AI that works across code, infra, and telemetry

The results mean 70% faster MTTR, 30% fewer engineers pulled in per incident, and thousands of saved engineering hours. Imagine what you could ship with that time in 2026

Learn more about AI for prod, workflow-autonomous multi-agent systems, and how you can cut orchestration tax, improve investigations, and shift engineering time from grunt work to great work.

Download free AI for Prod eBook

System Performance Metrics Every Engineer Should Know

Your API is slow. But how slow, exactly? You need numbers. Real metrics that tell you what's actually broken and where to fix it.

Here are the four core metrics every engineer should know when analyzing system performance:

Queries Per Second (QPS): How many incoming requests your system handles per second. Your server gets 1,000 requests in one second? That's 1,000 QPS. Sounds straightforward until you realize most systems can't sustain their peak QPS for long without things starting to break.

Transactions Per Second (TPS): How many completed transactions your system processes per second. A transaction includes the full round trip, i.e., the request goes out, hits the database, and comes back with a response.

TPS tells you about actual work completed, not just requests received. This is what your business cares about.

Concurrency: How many simultaneous active requests your system is handling at any given moment. You could have 100 requests per second, but if each takes 5 seconds to complete, you're actually handling 500 concurrent requests at once.

High concurrency means you need more resources, better connection pooling, and smarter thread management.

Response Time (RT): The elapsed time from when a request starts until the response is received. Measured at both the client level and server level.

A simple relationship ties them all together: QPS = Concurrency Ã· Average Response Time

More concurrency or lower response time = higher throughput.

Over to you: When you analyze performance, which metric do you look at first, QPS, TPS, or Response Time?

Why Is Nginx So Popular?

Apache dominated web servers for 20 years, then Nginx showed up and changed everything. Now Nginx powers some of the largest sites on the internet, including Netflix, Airbnb, Dropbox, and WordPress. com. Not because it's newer or trendier, but because it solves problems that Apache couldn't handle efficiently.

Hereâ€™s what makes Nginx so popular:

High-Performance Web Server

Reverse Proxy & Load Balancer

Caching Layer

SSL Termination (Offloading)

Over to you: Whatâ€™s your primary use for Nginx today, web server, reverse proxy, or load balancer?

Network Debugging Commands Every Engineer Should Know

When someone says â€œItâ€™s a network issue,â€ these commands help you find whatâ€™s wrong fast.

ping: Checks if the destination responds and reports the round-trip time for basic reachability.

traceroute / tracert: Shows each hop on the path so you can see where packets slow down or stop.

mtr / pathping: Continuously measures latency and loss per hop to catch intermittent issues.

ip addr, ip link / ipconfig /all: Prints local IPs, MACs, and interface status so you can verify the machineâ€™s network identity.

ip route: Reveals the routing table to confirm which gateway and next hop the system will use.

ip neigh: Displays IP-to-MAC entries to detect duplicates or stale ARP records on the LAN.

ss -tulpn: Lists listening sockets and PIDs so you can confirm a service is actually bound to the expected port.

dig: Resolves DNS records to verify the exact IPs clients will connect to.

curl -I: Fetches only HTTP(S) headers to check status codes, redirects, and cache settings.

tcpdump / tshark: Captures packets so you can inspect real traffic and validate whatâ€™s sent and received.

iperf3: Measures end-to-end throughput between two hosts to separate bandwidth limits from app issues.

ssh: Opens a secure shell on the remote machine to run checks and apply fixes directly.

sftp: Transfers files securely so you can pull logs or push artifacts during an incident.

nmap: Scans open ports and probes versions to confirm which services are exposed and responding.

Over to you: What's your go-to command when debugging network issues?

Hub, Switch, & Router Explained

Every home and office network relies on these three devices, hub, switch, and router, yet their roles are often mixed up.

A hub operates at Layer 1 (Physical Layer). Itâ€™s the simplest of the three, it doesnâ€™t understand addresses or data types. When a packet arrives, it simply broadcasts it to every connected device, creating one big collision domain. That means all devices compete for the same bandwidth, making hubs inefficient in modern networks.

A switch works at Layer 2 (Data Link Layer). It learns MAC addresses and forwards frames only to the correct destination device. Each port on a switch acts as its own collision domain, improving efficiency and speeding up communication within a LAN.

A router operates at Layer 3 (Network Layer). It routes packets based on IP addresses and connects different networks together, for example, your home network to the Internet. Each router interface forms a separate broadcast domain, keeping local and external traffic isolated.

Understanding how these three layers work together is the foundation of every modern network, from your home Wi-Fi to the global Internet backbone.

Over to you: How do you usually figure out whether a network issue is caused by the router or the switch?

SPONSOR US

Get your product in front of more than 1,000,000 tech professionals.

Our newsletter puts your products and services directly in front of an audience that matters - hundreds of thousands of engineering leaders and senior engineers - who have influence over significant tech decisions and big purchases.

Space Fills Up Fast - Reserve Today

Ad spots typically sell out about 4 weeks in advance. To ensure your ad reaches this influential audience, reserve your space now by emailing

sponsorship@bytebytego.com.",https://blog.bytebytego.com
https://blog.bytebytego.com/p/a-guide-to-retry-pattern-in-distributed,A Guide to Retry Pattern in Distributed Systems,,2026-01-02T16:54:50.848005,ByteByteGo,"In a monolithic application, a function call is a local, in-memory process. Aside from a catastrophic hardware failure or a process crash, the execution of a function is essentially guaranteed. If the process is alive, the call succeeds.

However, in distributed systems, this guarantee does not hold. Components communicate over physical networks that are inherently unreliable. This reality is captured in the â€œFallacies of Distributed Computing,â€ specifically the first fallacy: â€œThe network is reliableâ€. In truth, it is not. A request sent from Service A to Service B may fail not because Service B is broken, but simply because the communication medium momentarily faltered.

This creates a need for defensive programming patterns, and one of the primary mechanisms we use is the Retry pattern. By automatically retrying a failed operation, a system can trade latency for availability, turning what would have been a failed user request into a successful one.

However, retries are both essential and dangerous in distributed systems. On the one hand, they transform unreliable networks into reliable ones. But on the other hand, indiscriminate retries can lead to latency amplification, resource exhaustion, and cascading failures that can take down entire platforms.

In this article, we will explore the retry pattern in depth, understand when and how to use it safely and effectively.

What is a Retry?",https://blog.bytebytego.com
https://blog.bytebytego.com/p/how-meta-built-a-new-ai-powered-ads,How Meta Built a New AI-Powered Ads Model for 5% Better Conversions,,2026-01-02T16:54:53.029051,ByteByteGo,"Cut Code Review Time & Bugs in Half (Sponsored)

Code reviews are critical but time-consuming. CodeRabbit acts as your AI co-pilot, providing instant Code review comments and potential impacts of every pull request.

Beyond just flagging issues, CodeRabbit provides one-click fix suggestions and lets you define custom code quality rules using AST Grep patterns, catching subtle issues that traditional static analysis tools might miss.

CodeRabbit has so far reviewed more than 10 million PRs, installed on 2 million repositories, and used by 100 thousand Open-source projects. CodeRabbit is free for all open-source repoâ€™s.

Get Started Today

Disclaimer: The details in this post have been derived from the details shared online by the Meta Engineering Team. All credit for the technical details goes to the Meta Engineering Team.  The links to the original articles and sources are present in the references section at the end of the post. Weâ€™ve attempted to analyze the details and provide our input about them. If you find any inaccuracies or omissions, please leave a comment, and we will do our best to fix them.

When Meta announced in Q2 2025 that its new Generative Ads Model (GEM) had driven a 5% increase in ad conversions on Instagram and a 3% increase on Facebook Feed, the numbers might have seemed modest.

However, at Metaâ€™s scale, these percentages translate to billions of dollars in additional revenue and represent a fundamental shift in how AI-powered advertising works.

GEM is the largest foundation model ever built for recommendation systems. It has been trained at the scale typically reserved for large language models like GPT-4 or Claude. Yet hereâ€™s the paradox: GEM is so powerful and computationally intensive that Meta canâ€™t actually use it directly to serve ads to users.

Instead, the company developed a teacher-student architecture that lets smaller, faster models benefit from GEMâ€™s intelligence without inheriting its computational cost.

In this article, we look at how the Meta engineering team built GEM and the challenges they overcame.

ğŸ‘‹ Goodbye low test coverage and slow QA cycles (Sponsored)

Bugs sneak out when less than 80% of user flows are tested before shipping. However, getting that kind of coverage (and staying there) is hard and pricey for any team.

QA Wolfâ€™s

AI-native solution provides high-volume, high-speed test coverage for web and mobile apps, reducing your organizationâ€™s QA cycle to minutes.

They can get you:

80% automated E2E test coverage in weeksâ€”not years

Unlimited parallel test runs

24-hour maintenance and on-demand test creation

Zero flakes, guaranteed

The benefit? No more manual E2E testing. No more slow QA cycles. No more bugs reaching production.

With QA Wolf,

Drataâ€™s team of engineers

achieved 4x more test cases and

86% faster QA cycles.

â­ Rated 4.8/5 on G2

Schedule a demo to learn more

The Core Problem GEM Solves

Every day, billions of users scroll through Facebook, Instagram, and other Meta platforms, generating trillions of potential ad impression opportunities. Each impression represents a decision point: which ad, from millions of possibilities, should be shown to this specific user at this particular moment? Getting this wrong means wasting advertiser budgets on irrelevant ads and annoying users with content they donâ€™t care about. Getting it right creates value for everyone involved.

Traditional ad recommendation systems struggled with this in several ways. Some systems treated each platform separately, which meant that insights about user behavior on Instagram couldnâ€™t inform predictions on Facebook. This siloed approach missed valuable cross-platform patterns. Other systems tried to treat all platforms identically, ignoring the fact that people interact with Instagram Stories very differently from how they browse Facebook Feed. Neither approach was optimal.

The data complexity also compounds these challenges in the following ways:

Meaningful signals like clicks and conversions are extremely sparse compared to total impression volume.

User features are dynamic and constantly changing.

The system must process multimodal inputs, including text, images, video, and complex behavioral sequences.

Traditional models had severe memory limitations, typically only considering a userâ€™s last 10 to 20 actions.

GEMâ€™s goal was to create a unified intelligence that understands users holistically across Metaâ€™s entire ecosystem, learning from long behavioral histories and complex cross-platform patterns while maintaining the nuance needed to optimize for each specific surface and objective.

How GEM Understands Users?

GEMâ€™s architecture processes user and ad information through three complementary systems, each handling a different aspect of the prediction problem.

The first system handles what Meta calls non-sequence features, which are essentially static attributes and their combinations. These include user demographics like age and location, user interests, ad characteristics like format and creative content, and advertiser objectives.

The challenge here isnâ€™t just knowing these individual features but understanding how they interact. For example, a 25-year-old tech worker has very different purchasing patterns than a 25-year-old teacher, even if they share some interests. The system needs to learn which combinations of features actually matter.

GEM uses an enhanced version of the Wukong architecture with stackable factorization machines that can scale both vertically for deeper interactions and horizontally for broader feature coverage. This architecture works through multiple stacked layers, where each successive layer learns increasingly complex patterns from the simpler patterns discovered by previous layers. For instance, an early layer might discover the basic pattern that young professionals respond well to tech product ads. A layer deeper in the stack builds on this by learning that young professionals in urban areas who show interest in fitness respond especially well to smart wearable ads. An even deeper layer might refine this further, discovering that this combination works best specifically when those ads emphasize data tracking features rather than fashion elements.

The second system handles sequence features, which capture the timeline of user behavior. A userâ€™s actions donâ€™t exist in isolation. They tell a story with order and meaning. Someone who clicked on home workout content, then searched for gyms nearby, then viewed several gym websites, then researched membership costs is clearly on a specific journey. Traditional architectures struggled to process long sequences efficiently because the computational cost grows rapidly with sequence length.

GEM overcomes this with a pyramid-parallel structure. Think of it as processing your behavior history in chunks at the bottom level, then combining those chunks into broader patterns at middle levels, and finally synthesizing everything into a complete journey understanding at the top level. Multiple chunks can be processed simultaneously rather than sequentially, which dramatically improves efficiency.

The breakthrough here is scale. GEM can now analyze thousands of your past actions rather than just the most recent handful. This extended view reveals patterns that shorter windows simply cannot capture, like the progression from casual interest to serious purchase intent that might develop over months.

See the diagram below:

The third system, called InterFormer, handles cross-feature learning by connecting your static profile with your behavioral timeline. This is where GEMâ€™s intelligence really becomes evident. Previous approaches would compress your entire behavior history into a compact summary vector (like reducing an entire novel to a single rating). This compression inevitably loses critical details about your journey.

InterFormer takes a different approach using an interleaving structure. It alternates between layers that focus purely on understanding your behavior sequence and layers that connect those behaviors to your profile attributes.

The first sequence layer might identify that youâ€™ve shown increasing interest in fitness over time.

The first cross-feature layer then considers how your age, income, and location context shape what that fitness interest means.

The second sequence layer re-examines your behavior with these new insights and might notice that your fitness research intensified after a gym opened near your workplace.

The second cross-feature layer then makes even deeper connections about purchase intent and timing.

This alternating process continues through multiple layers, with each cycle refining understanding without losing access to the complete behavioral record.

The Practical Problem with Using GEM

Despite GEMâ€™s obvious strengths, Meta faced a fundamental engineering challenge in using GEM.

GEM is enormous and trained using thousands of GPUs over extended periods. Running GEM directly for every ad prediction would be impossibly slow and expensive. When a user scrolls through Instagram, the system needs to make ad decisions in tens of milliseconds. GEM simply cannot operate at that speed while serving billions of users simultaneously.

Metaâ€™s solution was a teacher-student architecture where GEM acts as the master teacher that trains hundreds of smaller, faster Vertical Models (VMs) that actually serve ads in production. These VMs are specialized for specific contexts like Instagram Stories click prediction or Facebook Feed conversion prediction. Each VM is lightweight enough to make predictions in milliseconds, but theyâ€™re much smarter than they would be if trained independently because they learn from GEM.

The knowledge transfer happens through two strategies. Direct transfer works when a VM operates in the same domain where GEM was trained, with similar data and objectives. GEM can teach these models directly. Hierarchical transfer applies when VMs work in specialized areas quite different from GEMâ€™s training domain. In these cases, GEM first teaches medium-sized domain-specific foundation models for areas like Instagram or Facebook Marketplace. These domain models then teach the even smaller VMs. The knowledge flows down through levels, getting adapted and specialized at each stage.

Meta employs three sophisticated techniques to maximize transfer efficiency:

Knowledge distillation with Student Adapter:

Student models learn to replicate GEMâ€™s reasoning process, not just final predictions. The Student Adapter refines GEMâ€™s predictions using recent ground-truth data, adjusting for timing delays and domain-specific differences.

Representation learning:

Creates a shared conceptual framework between teacher and students. GEM learns to encode information in ways that transfer well across different model sizes, adding no computational overhead during ad serving.

Parameter sharing:

This lets VMs selectively incorporate specific components directly from GEM. Small VMs stay fast while borrowing GEMâ€™s sophisticated components for complex user understanding tasks.

Together, these three techniques achieve twice the effectiveness of standard knowledge distillation alone. The continuous improvement cycle works like this:

Users interact with fast VMs in real time

Their engagement data flows back into Metaâ€™s data pipelines

GEM periodically re-trains on this fresh data, updated knowledge transfers to VMs through the post-training techniques, and

Improved VMs get deployed to production.

This cycle repeats continuously, with GEM getting smarter and VMs getting regular intelligence updates.

Training at Unprecedented Scale

Building GEM required Meta to rebuild its training infrastructure from the ground up.

The challenge was training a model at LLM scale, but for the fundamentally different task of recommendation rather than language generation. The company achieved a 23x increase in effective training throughput while using 16x more GPUs and simultaneously improving hardware efficiency by 1.43x.

This required innovations across multiple areas. Multi-dimensional parallelism orchestrates how thousands of GPUs work together, splitting the modelâ€™s dense components using techniques like Hybrid Sharded Distributed Parallel while handling sparse components like embedding tables through a combination of data and model parallelism. The goal was to ensure every GPU stayed busy with minimal idle time waiting for communication from other GPUs.

System-level optimizations pushed GPU utilization even higher:

Custom GPU kernels designed for variable-length user sequences, fusing operations to reduce memory bandwidth bottlenecks.

PyTorch 2.0 graph-level compilation automates optimizations like activation checkpointing and operator fusion.

Memory compression, including FP8 quantization to reduce the footprint without impacting accuracy.

NCCLX communication collectives that handle inter-GPU communication without consuming the main compute resources.

The efficiency gains extended beyond raw training speed.

Meta reduced job startup time by 5x through optimizations in trainer initialization, data reader setup, and checkpointing. They cut PyTorch 2.0 compilation time by 7x using intelligent caching strategies. These might seem like minor details, but when youâ€™re training models that cost millions of dollars in compute resources, every percentage point of efficiency improvement matters enormously.

The result is a training system that can iterate rapidly on GEM, incorporating new data and architectural improvements at a pace that would have been impossible with previous infrastructure. This enables Meta to keep GEM at the frontier of recommendation AI while controlling costs enough to make the massive investment worthwhile.

Conclusion

Metaâ€™s roadmap for GEM extends well beyond its current capabilities.

The next major evolution involves true multimodal learning, where GEM processes text, images, audio, and video together rather than treating them as separate input streams. This will enable an even richer understanding of both user preferences and ad creative effectiveness across all content types. The company is also exploring inference-time scaling, which would allow the system to dynamically allocate more computational resources to difficult predictions while handling straightforward cases more efficiently.

Perhaps most ambitiously, Meta envisions a unified engagement model that ranks both organic content and ads using the same underlying intelligence. This would fundamentally change how advertising integrates into social feeds, potentially creating more seamless experiences where ads feel like natural content recommendations rather than interruptions. On the advertiser side, GEMâ€™s intelligence will enable more sophisticated agentic automation, where AI systems can manage and optimize campaigns with minimal human intervention while achieving better results.

References:

Metaâ€™s Generative Ads Model (GEM): The Central Brain Accelerating Ads Recommendation AI Innovation

InterFormer Research Paper

Wukong: Towards a Scaling Law for Large-Scale Recommendation

SPONSOR US

Get your product in front of more than 1,000,000 tech professionals.

Our newsletter puts your products and services directly in front of an audience that matters - hundreds of thousands of engineering leaders and senior engineers - who have influence over significant tech decisions and big purchases.

Space Fills Up Fast - Reserve Today

Ad spots typically sell out about 4 weeks in advance. To ensure your ad reaches this influential audience, reserve your space now by emailing

sponsorship@bytebytego.com.",https://blog.bytebytego.com
https://blog.bytebytego.com/p/how-linkedin-built-an-ai-powered,How LinkedIn Built an AI-Powered Hiring Assistant,,2026-01-02T16:54:54.829802,ByteByteGo,"Solve Enterprise Auth, Identity, and Security for Your App (Sponsored)

Enterprise customers expect SSO, Directory Sync, RBAC, and Audit Logs, but building and maintaining that infrastructure slows teams down and pulls focus from core product work.

WorkOS

provides these features through simple APIs and a hosted Admin Portal that integrates with every identity provider. You get production-ready enterprise capabilities without owning the complexity yourself.

Trusted by OpenAI, Cursor, Vercel, 1000+ more. Your first million MAUs are free.

Add enterprise features today â†’

Disclaimer: The details in this post have been derived from the details shared online by the LinkedIn Engineering Team. All credit for the technical details goes to the LinkedIn Engineering Team.  The links to the original articles and sources are present in the references section at the end of the post. Weâ€™ve attempted to analyze the details and provide our input about them. If you find any inaccuracies or omissions, please leave a comment, and we will do our best to fix them.

Recruiting is a profession that demands both strategic thinking and meticulous attention to detail. Recruiters must make high-value decisions about which candidates are the best fit for a role, but they also spend countless hours on repetitive pattern recognition tasks. Sorting through hundreds of resumes, evaluating qualifications against job requirements, and drafting personalized outreach messages are all essential activities. However, they also consume enormous amounts of time that could otherwise be spent on relationship-building and strategic hiring decisions.

LinkedInâ€™s Hiring Assistant represents a new approach to solving this challenge.

Rather than replacing recruiters, this AI agent is designed to handle the repetitive, time-consuming aspects of the recruiting workflow, freeing professionals to focus on what they do best: connecting with people and making critical hiring choices.

The most labor-intensive parts of recruiting fall into three main categories.

First, sourcing candidates requires searching through LinkedInâ€™s network of over 1.2 billion profiles to identify qualified individuals.

Second, evaluating candidates involves carefully reading resumes and profiles to assess whether each person meets the specific requirements of a role.

Third, engaging candidates means drafting and sending personalized communications to potential hires, answering their questions, and maintaining ongoing dialogue throughout the hiring process.

To address these challenges, LinkedIn built the Hiring Assistant with three core capabilities.

The system delivers value at scale by efficiently searching across billions of profiles and handling enterprise-level workloads reliably.

It enables interactive communication by understanding recruiter intent through natural conversation, asking clarifying questions when needed, and adapting its behavior based on real-time feedback.

Lastly, it also features continuous learning by improving over time based on observing what recruiters do, learning individual preferences, and remembering past interactions and decisions.

In this article, we will look at the architecture and technical building blocks of LinkedInâ€™s Hiring Assistant.

Better Deals, By Design with Verizon This Holiday (Sponsored)

This holiday season, the equation is simple: everyone gets a better deal with Verizon. Best devices. Best plans. Add that to an award-winning network, and you have the best deals. Period.

Unbeatable Deal

: Switch to Verizon and get four lines on Unlimited Welcome for $25 per line/month (on Auto Pay plus taxes & fees) and get four of the newest, premium devices like the iPhone 17 Pro, Samsung Galaxy S25+, or Google Pixel 10 Pro XL all on Verizon.

Enjoy flexibility and save money this holiday season because every dollar you spend matters.

Explore Holiday Deals

. See

here

for full terms.

The High-Level Architecture

At its core, the Hiring Assistant is built on what LinkedIn calls a â€œplan-and-executeâ€ architecture as shown in the diagram below:

To understand why this matters, it helps to know what they avoided. A simpler approach, known as ReAct, would have the AI try to handle everything at once in a single continuous loop. While straightforward, this method runs into problems when tasks get complex. Large language models, the AI systems that power tools like this, can become unreliable when asked to juggle too many things simultaneously.

See the diagram below for the ReAct pattern

.Instead, LinkedIn split the work into two distinct phases:

The Planner acts as the strategic thinker. When a recruiter makes a request, the Planner examines it from a high level, breaks the work into smaller, manageable steps, and creates a structured plan for what needs to happen. Think of it as a project manager outlining the approach before any actual work begins.

The Executor then takes over. It works through the plan step by step, using available tools to complete each task. For each step, the Executor runs its own loop of reasoning and action, figuring out what needs to happen and then making it happen.

This divide-and-conquer strategy brings several advantages:

First, it makes the system more reliable. Breaking complex recruiting workflows into discrete steps means the AI is less likely to get confused or make mistakes.

Second, it allows for better cost management. LinkedIn can use more powerful AI models for complex reasoning tasks while deploying simpler, cheaper models for straightforward steps.

Third, tasks are far more likely to be completed successfully when they are well-defined and manageable in scope.

Beyond the plan-and-execute design, the Hiring Assistant uses a message-driven architecture.

Each recruiter gets their own individual instance of the assistant, complete with its own identity and mailbox. Everything works through asynchronous messages, much like email. When a recruiter asks the assistant to find candidates, they do not have to sit and wait for results. The assistant receives the message, processes it in the background, and sends updates when ready.

This asynchronous approach is what enables the assistant to work at scale. While a recruiter focuses on other tasks, their assistant can be searching through millions of profiles, evaluating candidates, and preparing recommendations, all without requiring constant attention or supervision.

The Agentic User Experience

The Hiring Assistant operates in two complementary modes, each designed for different stages of the recruiting process:

Interactive Mode:

When recruiters first start a new project, they work with the assistant in interactive mode. This feels like having a conversation with a colleague. Recruiters can clarify what kind of person they are looking for, refine job requirements, and get immediate feedback on their requests. The assistant shows its reasoning as it works, making the process transparent. This builds trust because recruiters can see exactly what the system is doing and correct course quickly if something seems off.

Asynchronous Mode:

Once the recruiter and assistant are aligned on what success looks like, the system shifts into asynchronous mode. This is where the real power of automation comes into play. The assistant works autonomously in the background, running large-scale searches across millions of profiles, continuously updating candidate pipelines, and evaluating new applicants as they appear.

LinkedIn describes this as a â€œsource while you sleepâ€ capability.

The assistant can review thousands of candidates overnight, a task that would take a human recruiter weeks to complete manually.

Yet even in this autonomous mode, humans remain in control of important decisions. The assistant surfaces candidates and provides recommendations, but recruiters make the final calls about who to contact and ultimately hire. This balance between automation and human judgment is central to how the system is designed.

Technical Building Blocks

The Hiring Assistant is built on top of LinkedInâ€™s broader agent platform, a foundation of reusable components that can power any AI agent product across the company. This approach means the LinkedIn engineering team does not have to reinvent the wheel each time it builds a new intelligent system.

At the user-facing level, a client-side SDK embeds the assistant directly into recruiter workflows. This SDK creates dynamic interfaces that adapt based on what the AI needs at any given moment. It supports multiple input methods, including chat, voice, and typing assistance, while logging all interactions for future analysis and improvement.

Connecting this interface to backend services is a GraphQL API, which delivers data in structured packages called view models. These contain everything needed to display information on screen. LinkedIn calls it the agent-driven UI, where the AI itself can determine what recruiters see, dynamically adjusting the interface as tasks progress.

Rather than the traditional request-response pattern where you ask a question and wait for an answer, the system uses a push-based, event-driven architecture. It works as follows:

The user interface subscribes to updates from the agent, and when something changes, the agent publishes that update. This means the interface refreshes automatically without users needing to manually reload anything.

Long-running AI tasks are delivered through streaming responses. Instead of waiting for a complete answer, recruiters see the AIâ€™s reasoning unfold in real time, with results appearing as soon as they become available.

If a recruiter is logged in on multiple devices, cross-session synchronization keeps everything in sync. An action taken on a phone immediately reflects on a desktop browser.

The Supervisor Agent

At the center of the Hiring Assistant sits what LinkedIn calls the supervisor agent. If the overall system is a team, the supervisor is the team leader who makes sure everyone works together effectively.

See the diagram below:

The supervisor handles several critical responsibilities:

It oversees workflow management for the entire hiring process, ensuring tasks move forward in the right sequence.

When a recruiter sends a message or request, the supervisor receives it and routes it to the appropriate sub-agent for handling.

It also makes judgment calls about task prioritization, deciding what requires human input versus what can be safely automated.

Beyond just delegating work, the supervisor coordinates between different sub-agents to ensure they work together smoothly. It actively observes the environment, watching for changes like new candidate activity or application submissions, and triggers appropriate actions in response.

The supervisor also manages the human-in-the-loop aspect of the system. It knows which decisions are significant enough to require human approval and surfaces those moments to recruiters.

All communication, whether from users or between sub-agents, flows through the supervisor. It serves as the central hub that keeps the entire operation organized and aligned with recruiter goals.

The Specialized Sub-Agents

The Hiring Assistant divides recruiting work among several specialized sub-agents, each focused on a specific part of the workflow. This modular design allows each component to excel at its particular task while working together as a cohesive system. Letâ€™s look at the various sub-agents in detail:

Intake Agent

The intake agent serves as the starting point for every hiring project.

It gathers job requirements from recruiters, confirming essential details like job title, location, and seniority level. When information is missing, the agent leverages LinkedInâ€™s Economic Graph (a digital map of the global economy) to intelligently fill in gaps. The agent then generates specific qualifications based on successful past hires and industry knowledge, creating a clear framework for evaluating candidates.

Sourcing Agent

Finding the right candidates is perhaps the most knowledge-intensive part of recruiting, and the sourcing agent approaches this challenge with multiple strategies.

It creates search queries using traditional Boolean logic (AND, OR, NOT operators), generates AI-powered queries based on hiring requirements, and draws on historical recruiter search patterns as starting points. Importantly, customer data never crosses company boundaries, maintaining strict data isolation.

What sets this agent apart is its integration with LinkedInâ€™s Economic Graph.

This gives it access to insights about top locations, job titles, and skills for specific talent pools. It can identify which candidates are actively looking or were recently hired, understand talent flow patterns between companies and industries, spot fast-growing companies and skill sets, flag companies experiencing layoffs, and highlight opportunities at top schools or companies with open positions. These insights help the agent find hidden gems that might otherwise be overlooked, going well beyond simple keyword matching.

The sourcing agent also implements a closed feedback loop. It combines sourcing with evaluation results, using AI reasoning to refine queries based on which candidates prove to be good matches. This allows the system to balance precision (finding exactly the right candidates) with liquidity (finding enough candidates), continuously improving the quality and volume of results over time.

Evaluation Agent

Reading resumes and assessing qualifications is one of the most time-consuming tasks for recruiters.

The evaluation agent tackles this by reading candidate profiles and resumes, comparing them against job qualifications, and providing structured recommendations backed by evidence. It shows why a candidate may or may not match requirements, rather than simply offering a yes or no answer.

LinkedIn engineered this agent to address several complex challenges.

Before any evaluation begins, recruiters must review and approve the qualifications being used.

Safety checks ensure these qualifications follow responsible AI policies. The agent searches through profiles and resumes for specific evidence demonstrating how candidates meet each qualification, surfacing this evidence to recruiters for review.

To ensure accuracy, LinkedIn built quality benchmarks for testing the evaluation agent across different scenarios.

They developed custom AI models specifically optimized for qualification evaluation, as general-purpose models could not achieve the necessary combination of accuracy and speed. Using techniques like speculative decoding and custom serving infrastructure, these fine-tuned models can evaluate candidates in seconds rather than minutes, fast enough to support real-time, conversational refinement of requirements.

Candidate Outreach Agent

Once promising candidates are identified, the outreach agent handles communication.

It writes personalized messages, sends initial outreach and follow-ups, and replies to candidate questions using job-specific FAQs defined during intake. The agent can even schedule phone screenings directly through messaging, streamlining coordination.

Candidate Screening Agent

Supporting the interview process, the screening agent prepares tailored interview questions based on hiring requirements and candidate profiles.

It can transcribe and summarize screening conversations while capturing notes and insights. Importantly, recruiters maintain full control, able to take over conversations at any time or guide the process as needed.

Learning Agent

The learning agent enables the system to improve over time.

It analyzes recruiter actions such as which candidates they message or add to pipelines, learning from both explicit feedback and implicit behavioral signals. The agent updates job qualifications based on these patterns, but any suggested changes must be reviewed and approved by recruiters before being applied. This ensures the assistant adapts while keeping humans in control.

Cognitive Memory Agent

Finally, the cognitive memory agent gives the assistant persistent memory across interactions.

It remembers past conversations, preferences, and decisions, helping personalize recommendations over time. All memory data remains scoped to the individual recruiterâ€™s environment with strong privacy protections.

This data is never used to train AI models, ensuring customer information stays secure and confidential.

The Quality Pillars

Building an AI agent that operates at scale requires a comprehensive approach to quality that ensures the system behaves safely, responsibly, and effectively.

The LinkedIn engineering team built its quality framework on two complementary pillars:

1 - The Rails

Product policy serves as the rails that keep the system on track. These policies set clear boundaries for safety, compliance, and legal standards while defining expected agent behavior. They establish minimum quality thresholds that must be met.

To enforce these standards, LinkedIn employs AI-powered judges that evaluate different aspects of quality. Some judges check for coherence, asking whether outputs make logical sense. Others verify factual accuracy, ensuring the system does not generate false or misleading information.

2 - The Compass

Human alignment acts as the compass, ensuring the assistant moves toward genuinely valuable outcomes.

This pillar is grounded in human-validated data, including annotated datasets where people label examples, and real recruiter activity. When a recruiter messages a candidate or adds them to a pipeline, the system treats this as a strong positive signal.

Over time, the assistant learns to recommend candidates matching these recruiter-validated patterns. Human alignment also serves to validate whether product policies are actually working in practice.

Conclusion

LinkedInâ€™s Hiring Assistant demonstrates a big approach to building enterprise-grade AI agents.

By adopting a plan-and-execute architecture, the system breaks complex recruiting workflows into manageable steps, improving reliability and reducing errors. The message-driven design allows each recruiter to have their own assistant instance that works asynchronously in the background, enabling true scale.

The division of labor among specialized sub-agents ensures that each component can focus on what it does best, from sourcing and evaluation to outreach and screening. Integration with LinkedInâ€™s Economic Graph provides market intelligence that goes beyond simple keyword matching, helping uncover candidates who might otherwise be overlooked.

Perhaps most importantly, the system balances automation with human judgment. The quality framework keeps the assistant safe and aligned with real hiring outcomes, while the learning agent ensures continuous improvement based on individual recruiter preferences.

References:

Building the agentic future of recruiting: how we engineered LinkedInâ€™s Hiring Assistant

Under the hood: The tech behind the first agent from LinkedIn

The LinkedIn Generative AI Application Tech Stack

SPONSOR US

Get your product in front of more than 1,000,000 tech professionals.

Our newsletter puts your products and services directly in front of an audience that matters - hundreds of thousands of engineering leaders and senior engineers - who have influence over significant tech decisions and big purchases.

Space Fills Up Fast - Reserve Today

Ad spots typically sell out about 4 weeks in advance. To ensure your ad reaches this influential audience, reserve your space now by emailing

sponsorship@bytebytego.com.",https://blog.bytebytego.com
https://blog.bytebytego.com/p/top-ai-agentic-workflow-patterns,Top AI Agentic Workflow Patterns,,2026-01-02T16:54:58.921737,ByteByteGo,"Tinkering with prompts can only get you so far. (Sponsored)

Most companies get stuck tinkering with prompts and wonder why their agents fail to deliver dependable results.

This guide from You.com

breaks down the evolution of agent management, revealing the five stages for building a successful AI agent and why most organizations havenâ€™t gotten there yet.

In this guide, youâ€™ll learn:

Why prompts alone arenâ€™t enough and how context and metadata unlock reliable agent automation

Four essential ways to calculate ROI, plus when and how to use each metric

Real-world challenges at each stage of agent management and how to avoid them

Go beyond the prompt: get the guide.

When we first interact with large language models, the experience is straightforward. We type a prompt, the model generates a response, and the interaction ends.

This single-turn approach works well for simple questions or basic content generation, but it quickly reveals its limitations when we tackle more complex tasks. Imagine asking an AI to analyze market trends, create a comprehensive report, and provide actionable recommendations. A single response, no matter how well-crafted, often falls short because it lacks the opportunity to gather additional information, reflect on its reasoning, or refine its output based on feedback.

This is where agentic workflows come into play.

Rather than treating AI interactions as one-and-done transactions, agentic workflows introduce iterative processes, tool integration, and structured problem-solving approaches. These workflows transform language models from sophisticated text generators into capable agents that can break down complex problems, adapt their strategies, and produce higher-quality results. The difference is similar to comparing a quick sketch to a carefully refined painting. Both have their place, but when quality and reliability matter, the iterative approach wins.

In this article, we will look at the most popular agentic workflow patterns and how they work.

Understanding Agentic Workflows

An agentic workflow doesnâ€™t just respond to a single instruction. Instead, it operates with a degree of autonomy, making decisions about how to approach a task, what steps to take, and how to adapt based on what it discovers along the way. This represents a fundamental shift in how we think about using AI systems.

Consider the difference between asking a basic chatbot and an agentic system to help write a research report. The basic chatbot receives our request and generates a report based on its training data, delivering whatever it produces in one response. An agentic system, however, might first search the web for current information on the topic, then organize the findings into themes, draft sections of the report, review each section for accuracy and coherence, revise weak areas, and finally compile everything into a polished document. Each of these steps might involve multiple sub-steps, decisions about which tools to use, and adaptations based on what the agent discovers.

What makes workflows truly agentic are the iteration and feedback loops built into the process. Instead of generating output in a single pass, agentic workflows involve cycles where the agent takes an action, observes the result, and uses that observation to inform the next action. This mirrors how humans actually solve complex problems. We rarely figure everything out up front and execute a perfect plan. Instead, we try something, see what happens, learn from the result, and adjust our approach. Agentic workflows bring this same adaptive, iterative quality to AI systems.

The Five Essential Agentic Workflow Patterns

Let us now look at five essential agentic workflow patterns:

Reflection Pattern: The Self-Improving Agent

At its core, reflection is about having an agent review and critique its own work, then revise based on that critique. This simple idea improves output quality because it introduces an iterative refinement process that catches errors, identifies weaknesses, and enhances strengths.

Hereâ€™s how the reflection cycle works in practice.

The agent first generates an initial output based on the task or prompt it receives.

Then, instead of immediately presenting this output as final, the agent switches into critique mode. It examines what it just produced, looking for problems, inconsistencies, areas that lack clarity, or opportunities for improvement. This critique becomes the basis for revision.

The agent generates an improved version that addresses the issues it identified. Depending on the implementation, this cycle might repeat multiple times, with each iteration refining the output further.

See the diagram below:

The power of reflection becomes even more apparent when we specialize in the type of critique being performed. Some examples are as follows:

An agent might reflect specifically on accuracy, checking whether the facts and claims it made are correct and well-supported.

Alternatively, reflection might focus on clarity, asking whether someone unfamiliar with the topic would understand the explanation.

For creative writing, reflection might evaluate tone, ensuring the voice matches the intended style and audience.

For code generation, reflection could focus on identifying bugs, security vulnerabilities, or opportunities to optimize performance.

The reflection pattern works best for tasks where quality matters more than speed and where there are subjective aspects that benefit from review. The pattern, however, is less necessary for simple, factual queries where the answer is straightforward or for tasks where speed is paramount and good enough is truly sufficient.

Tool Use Pattern

The tool use pattern represents a fundamental expansion of what AI agents can accomplish.

A language model by itself, no matter how sophisticated, is limited to reasoning about information it learned during training and generating text based on that knowledge. It cannot access current information, perform precise calculations with large numbers, retrieve data from specific databases, or interact with external systems. Tools change everything.

In the tool use pattern, agents are equipped with a set of capabilities they can invoke when needed. These might include web search engines for finding current information, APIs for accessing services like weather data or stock prices, code interpreters for running programs and performing calculations, database query tools for retrieving specific records, file system access for reading and writing documents, and countless other specialized functions. The critical distinction from traditional software is that the agent itself decides when and how to use these tools based on the task at hand.

See the diagram below:

When an agent receives a task, it analyzes what capabilities are needed to accomplish that task. For example:

If the task requires information the agent doesnâ€™t have, it recognizes the need for a search or data retrieval tool.

If the task involves mathematical operations, it accesses a calculator or code interpreter.

If the task requires interacting with a specific service, it uses the appropriate API tool.

What makes tool use powerful is the dynamic nature of tool selection and the ability to chain multiple tool calls together.

The agent doesnâ€™t follow a predetermined script. If the first search doesnâ€™t return adequate information, the agent might reformulate its query and search again. If an API call fails or returns an error, the agent might try an alternative approach or a different tool entirely. This adaptability makes tool-enabled agents far more capable than rigid automated workflows.

Reason and Act Pattern (ReAct)

The Reason and Act pattern, commonly known as ReAct, represents a sophisticated approach to problem-solving that combines explicit reasoning with iterative action. Rather than thinking through an entire plan before acting, or blindly taking actions without reflection, ReAct agents alternate between reasoning about what to do next and actually doing it. This interleaving of thought and action creates a natural, adaptive problem-solving process.

The ReAct cycle follows a clear pattern.

First, the agent reasons about the current situation and what it needs to accomplish. This reasoning step is made explicit, often literally written out as the agentâ€™s internal thought process. The agent might think about what information it has, what it still needs, what approaches might work, and what the best next step is.

Then, based on this reasoning, the agent takes an action. This might be using a tool to gather information, performing a calculation, or making a decision.

After the action, the agent observes the results and enters a new reasoning phase, thinking about what it learned and what to do next. This cycle continues until the agent determines it has accomplished the goal or reached a point where it cannot proceed further.

See the diagram below:

The explicit reasoning steps serve multiple important purposes.

First, they help the agent stay on track and maintain focus on the goal. By articulating what itâ€™s trying to accomplish and why each action makes sense, the agent is less likely to go down irrelevant paths or get stuck in unproductive loops.

Second, reasoning steps enable adaptation. When an action doesnâ€™t yield expected results, the reasoning phase allows the agent to diagnose why and adjust its approach rather than blindly continuing.

Third, the reasoning trail provides transparency. Users and developers can see not just what the agent did, but why it made those choices, which is valuable for trust, debugging, and understanding the agentâ€™s decision-making process.

Comparing ReAct to pure planning or pure execution highlights its strengths.

Pure planning means figuring out all the steps before taking any action. This works well when we have complete information, and the environment is predictable, but it struggles when we need to discover information along the way or when circumstances change.

Pure execution means taking actions without much forethought, which is fast but often inefficient and prone to mistakes.

ReAct finds a middle ground, providing enough structure through reasoning while maintaining flexibility through iterative action.

Planning Pattern

The planning pattern takes a different approach from ReAct by emphasizing upfront strategic thinking before execution begins.

When using the planning pattern, the agent starts by analyzing the overall goal and understanding what success looks like. It then breaks down this goal into smaller, more manageable subtasks. This decomposition continues until the agent has identified concrete, actionable steps.

Crucially, the agent identifies dependencies between tasks, determining which steps must be completed before others can begin and which steps can potentially happen in parallel. The agent also considers what resources, tools, or information each step will require. Only after creating this structured plan does the agent begin execution.

See the diagram below:

One of the planning patternâ€™s key strengths is adaptive planning.

The planning pattern works best for tasks with natural phases or stages where some activities logically precede others. Itâ€™s valuable for tasks with constraints like deadlines, budgets, or resource limitations where coordination matters. It shines in situations where mistakes or backtracking would be costly, making it worth investing time in thoughtful planning. Complex projects involving multiple work streams benefit greatly from planning.

However, the planning pattern has limitations.

For simple, linear tasks where each step naturally suggests the next one, the overhead of creating a formal plan provides little benefit.

For highly uncertain tasks where weâ€™re likely to discover critical information during execution that fundamentally changes the approach, extensive upfront planning might be wasted effort.

Multi-Agent Pattern

The multi-agent pattern represents perhaps the most sophisticated approach to building AI systems.

Instead of relying on a single agent to handle everything, this pattern uses multiple specialized agents that collaborate to accomplish tasks. Each agent has specific expertise, capabilities, or perspectives, and they work together much like human teams do.

The core insight behind multi-agent systems is that specialization often leads to better performance than generalization.

A single agent trying to be excellent at everything faces challenges. It must balance competing requirements in its design and training. It needs broad knowledge but also deep expertise. It must be creative but also critical. By dividing responsibilities among multiple agents, each can be optimized for its specific role.

In a multi-agent system, we typically see several types of roles.

There are specialist agents focused on particular domains or tasks, such as a research agent that excels at finding and synthesizing information, a coding agent optimized for writing and debugging code, or a data analysis agent skilled at statistical analysis and visualization.

There are often critics or review agents whose job is to evaluate outputs from other agents, identifying flaws, suggesting improvements, or verifying quality.

Thereâ€™s usually a coordinator or orchestrator agent that manages the overall workflow, deciding which specialist should handle each subtask and ensuring all the pieces come together coherently.

The multi-agent pattern introduces complexity trade-offs as follows:

Coordination overhead increases with more agents.

Communication between agents requires clear protocols.

Debugging becomes more challenging because problems might arise from interactions between agents rather than individual agent errors.

The benefits must justify these costs. For simple tasks, a single capable agent is almost always better. For complex tasks requiring diverse expertise, careful coordination, or multiple perspectives, the multi-agent approach often produces superior results despite its added complexity.

Conclusion

The various agentic workflow patterns represent a fundamental evolution in how we build and deploy AI systems.

Moving beyond simple prompting to sophisticated, iterative processes has transformed what AI agents can reliably accomplish. Hereâ€™s a quick summary of the patterns we have covered:

The reflection pattern ensures quality through self-improvement.

Tool use extends capabilities far beyond pure language generation.

ReAct combines thoughtful reasoning with adaptive action.

Planning brings strategic thinking to complex tasks.

Multi-agent collaboration leverages specialization and diverse perspectives.

Together, these patterns provide a robust toolkit for building AI systems capable of handling real-world complexity.

What makes these patterns particularly powerful is that theyâ€™re not mutually exclusive. The most sophisticated agent systems often combine multiple patterns to achieve their goals.

SPONSOR US

Get your product in front of more than 1,000,000 tech professionals.

Our newsletter puts your products and services directly in front of an audience that matters - hundreds of thousands of engineering leaders and senior engineers - who have influence over significant tech decisions and big purchases.

Space Fills Up Fast - Reserve Today

Ad spots typically sell out about 4 weeks in advance. To ensure your ad reaches this influential audience, reserve your space now by emailing

sponsorship@bytebytego.com.",https://blog.bytebytego.com
https://blog.bytebytego.com/p/ep193-database-types-you-should-know,EP193: Database Types You Should Know in 2025,,2026-01-02T16:55:03.663196,ByteByteGo,"8 Insights into Real-World Cloud Security Postures  (Sponsored)

To better understand the vulnerabilities and threats facing modern DevOps organizations, Datadog analyzed security posture data from a sample of thousands of organizations that use AWS, Azure, or Google Cloud.

In this report, youâ€™ll gain valuable cloud security insights based on this research including:

How long-lived credentials create opportunities for attackers to breach cloud environments

Adoption of proactive cloud security mechanisms such as S3 Public Access Block or IMDSv2 in AWS

Most common risks when using managed Kubernetes distributions

Read the report

This weekâ€™s system design refresher:

Transformers Step-by-Step Explained (Youtube video)

Database Types You Should Know in 2025

Apache Kafka vs. RabbitMQ

The HTTP Mindmap

How DNS Works

SPONSOR US

Transformers Step-by-Step Explained (Attention Is All You Need)

Database Types You Should Know in 2025

Thereâ€™s no such thing as a one-size-fits-all database anymore. Modern applications rely on multiple database types, from real-time analytics to vector search for AI. Knowing which type to use can make or break your systemâ€™s performance.

Relational: Traditional row-and-column databases, great for structured data and transactions.

Columnar: Optimized for analytics, storing data by columns for fast aggregations.

Key-Value: Stores data as simple keyâ€“value pairs, enabling fast lookups.

In-memory: Stores data in RAM for ultra-low latency lookups, ideal for caching or session management.

Wide-Column: Handles massive amounts of semi-structured data across distributed nodes.

Time-series: Specialized for metrics, logs, and sensor data with time as a primary dimension.

Immutable Ledger: Ensures tamper-proof, cryptographically verifiable transaction logs.

Graph: Models complex relationships, perfect for social networks and fraud detection

Document: Flexible JSON-like storage, great for modern apps with evolving schemas.

Geospatial: Manages location-aware data such as maps, routes, and spatial queries.

Text-search: Full-text indexing and search with ranking, filters, and analytics.

Blob: Stores unstructured objects like images, videos, and files.

Vector: Powers AI/ML apps by enabling similarity search across embeddings.

Over to you: Which database type do you think will grow fastest in the next 5 years?

Apache Kafka vs. RabbitMQ

Kafka and RabbitMQ both handle messages, but they solve fundamentally different problems. Understanding the difference matters when designing distributed systems.

Kafka is a distributed log. Producers append messages to partitions. Those messages stick around based on retention policy, not because someone consumed them. Consumers pull messages at their own pace using offsets. You can rewind, replay, reprocess everything. It is designed for high throughput event streaming where multiple consumers need the same data independently.

RabbitMQ is a message broker. Producers publish messages to exchanges. Those exchanges route to queues based on binding keys and patterns (direct, topic, fanout). Messages get pushed to consumers and then deleted once acknowledged. It is built for task distribution and traditional messaging workflows.

The common mistake is using Kafka like a queue or RabbitMQ like an event log. Theyâ€™re different tools built for different use cases.

Over to you: If you had to explain when NOT to use Kafka, what would you say?

The HTTP Mindmap

HTTP has evolved from HTTP/1.1 to HTTP/2, and now HTTP/3, which uses the QUIC protocol over UDP for improved performance. Today, itâ€™s the backbone of almost everything on the internet, from browsers and APIs to streaming, cloud, and AI systems.

At the foundation, we have underlying protocols. TCP/IP for IPv4 and IPv6 traffic. Unix domain sockets for local communication. HTTP/3 running over UDP instead of TCP. These handle the actual data transport before HTTP even comes into play.

Security wraps around everything. HTTPS isnâ€™t optional anymore. WebSockets power real-time connections. Web servers manage workloads. CDNs distribute content globally. DNS resolves everything to IPs. Proxies (forward, reverse, and API gateways) route, filter, and secure traffic in between.

Web services exchange data in different formats, REST with JSON, SOAP for enterprise systems, RPC for direct calls, and GraphQL for flexible queries. Crawlers and bots index the web, guided by robots.txt files that set the boundaries.

The network world connects everything. LANs, WANs, and protocols like FTP for file transfers, IMAP/POP3 for email, and BitTorrent for peer-to-peer communication. For observability, packet capture tools like Wireshark, tcpdump, and OpenTelemetry let developers peek under the hood to understand performance, latency, and behavior across the stack.

Over to you: HTTP has been evolving for 30+ years, what do you think the next big shift will be?

How DNS Works

You type a domain name and hit enter, but what actually happens before that webpage loads is more complex than most people realize. DNS is the phonebook of the internet, and every request you make triggers a chain of lookups across multiple servers.

Step 1: Someone types bytebytego. com into their browser and presses enter.

Step 2: Before doing anything, the browser looks for a cached IP address. Operating system cache gets checked too.

Step 3: Cache miss triggers a DNS query. The browser sends a query to the configured DNS resolver, usually provided by your ISP or a service like Google DNS or Cloudflare.

Step 4: Resolver checks its own cache.

Step 5-6: If the resolver doesnâ€™t have the answer cached, it asks the root servers, â€œWhere can I find .com?â€ For bytebytego. com, the root server responds with the address of the .com TLD name server.

Step 7-8: Resolver queries the .com TLD server. TLD server returns the authoritative server address.

Step 9-10: This server has the actual A/AAAA record mapping the domain to an IP address. The resolver finally gets the answer: 172. 67. 21. 11 for bytebytego. com.

Step 11-12: The IP gets cached at the resolver level for future lookups, and returned to the browser.

Step 13-14: The browser stores this for its own future use, and uses the IP to make the actual HTTP request.

Step 15: The web server returns the requested content.

All this happens in milliseconds, before your first page even starts loading.

Over to you: Which DNS tools or commands do you rely on most, dig, nslookup, or something else?

Can a web server provide real-time updates?

An HTTP server cannot automatically initiate a connection to a browser. As a result, the web browser is the initiator. What should we do next to get real-time updates from the HTTP server?

Both the web browser and the HTTP server could be responsible for this task.

Web browsers do the heavy lifting: short polling or long polling. With short polling, the browser will retry until it gets the latest data. With long polling, the HTTP server doesnâ€™t return results until new data has arrived.

HTTP server and web browser cooperate: WebSocket or SSE (server-sent event). In both cases, the HTTP server could directly send the latest data to the browser after the connection is established. The difference is that SSE is uni-directional, so the browser cannot send a new request to the server, while WebSocket is fully-duplex, so the browser can keep sending new requests.

Over to you: of the four solutions (long polling, short polling, SSE, WebSocket), which ones are commonly used, and for what use cases?

SPONSOR US

Get your product in front of more than 1,000,000 tech professionals.

Our newsletter puts your products and services directly in front of an audience that matters - hundreds of thousands of engineering leaders and senior engineers - who have influence over significant tech decisions and big purchases.

Space Fills Up Fast - Reserve Today

Ad spots typically sell out about 4 weeks in advance. To ensure your ad reaches this influential audience, reserve your space now by emailing

sponsorship@bytebytego.com.",https://blog.bytebytego.com
https://newsletter.systemdesign.one/p/how-mcp-works,MCP - A Deep Dive,#110: Understanding Model Context Protocol,2026-01-02T16:55:08.704634,Eric Roby,"Get my system design playbook for FREE on newsletter signup:

Subscribe

Share this post

& I'll send you some rewards for the referrals.

Every AI tool you use is starving for â€œcontext

1

â€.

Your AI needs your codebase. Your custom agents

2

need database access. But getting that context to them? That is where things get messy.

When large language models

3

(LLMs) emerged, each link between AI and data sources required a custom integration built from scratch.

Onward.

Cut Code Review Time & Bugs in Half (Sponsor)

Code reviews are critical but time-consuming.

CodeRabbit

acts as your AI co-pilot, providing instant code review comments and potential impacts of every pull request.

Beyond just flagging issues, CodeRabbit provides one-click fix suggestions and lets you define custom code quality rules using AST Grep patterns, catching subtle issues that traditional static analysis tools might miss.

CodeRabbit has so far reviewed more than 10 million PRs, installed on 2 million repositories, and used by 100 thousand open-source projects.

CodeRabbit is free for all open-source repos.

Get Started Today

I want to introduce

Eric Roby

as a guest author.

Heâ€™s a senior backend and AI engineer focused on building real-world systems and teaching developers how to do the same. He runs the YouTube channel

codingwithroby

, where he focuses on scalable backend architecture and integrating AI into production applications.

Through his content and courses, he helps engineers go beyond tutorials, think in systems, and develop the skills that actually matter for senior-level roles.

Find him on:

LinkedIn

Substack

In November 2024, Anthropic launched the Model Context Protocol (MCP) to fix this problem. This wasnâ€™t just another developer tool. It aimed to tackle a key infrastructure issue in AI engineering.

The challenge?

Every AI application needs to connect to data sources. Until now, they had to build custom integrations from scratch. This approach creates a fragmented ecosystem that does not scale.

For example, the world isnâ€™t frozen in time, but LLMs areâ€¦

If you want an AI to â€œmonitor server logs,â€ you canâ€™t feed it a file. You would have to build a data pipeline that polls an API every few seconds. Filter out noise, then push only the relevant anomalies into the AIâ€™s context window

4

. If the API changes its rate limits or response format, your entire agent breaks.

A better way to think of MCP is like a USB-C port.

The Old Way (Before USB):

If you bought a mouse, it had a PS/2 plug. A printer had a massive parallel port. A camera had a proprietary cable. If you wanted to connect these to a computer, the computer needed a specific physical port for each one.

This is the â€œNÃ—Mâ€ problem:

Each device maker had to figure out how to connect to each computer. So, computer makers had to create ports for every device.

The MCP Way (With USB-C):

Now, everything uses a standard port.

Computer (AI Model

5

) needs one USB-C port. It doesnâ€™t need to know if you are plugging in a hard drive or a microphone; it just speaks â€œUSB.â€

Device (Data Source) requires a USB-C port. It doesnâ€™t need to know if itâ€™s plugged into a Mac, a PC, or an iPad.

The result: you can plug anything into anything instantly.

The analogy makes sense in theory. But to understand why MCP matters, you need to see how painful the current reality actually isâ€¦

The Integration Complexity Problem

Every AI assistant needs context to be useful:

Claude

6

needs access to your codebase. ChatGPT may require your Google Drive files. Custom agents need database connections. But how do these AI systems actually get that context?

Traditionally, each connection requires a custom integration.

If youâ€™re building an AI coding assistant, you need to:

Write code to connect to the GitHub API.

Add authentication and security.

Create a way to change GitHubâ€™s data format into a version that your AI can understand.

Handle rate limiting, errors, and edge cases.

Repeat this entire process for GitLab, Bitbucket, and every other source control system.

This creates the NÃ—M problem:

â€˜Nâ€™ AI assistants multiplied by â€˜Mâ€™ data sources equals NÃ—M unique integrations that need to be built and maintained.

When Cursor wants to add Notion support, they build it from scratch. When GitHub Copilot wants the same thing, they build it again. The engineering effort gets duplicated across every AI platform.

The traditional approaches have fundamental limitations:

Static, build-time integration

:

Integrations are hard-coded into applications. You canâ€™t add a new data source without updating the application itself. This makes rapid experimentation impossible and forces users to wait for official support.

Application-specific security implementations

:

Every integration reinvents authentication, authorization, and data protection. This leads to inconsistent security models and increases the attack surface.

No standard way to discover

:

AI systems canâ€™t find out what capabilities a data source has. Everything needs clear programming. This makes it hard to create agents that adapt. They canâ€™t use the new tools on their own.

The result is a â€˜brokenâ€™ ecosystem!

Innovation is stuck because of integration work. Users can access only the connections that application developers create. So thatâ€™s a mess.

Now letâ€™s look at how MCP cleans it upâ€¦

How MCP Solves It

MCPâ€™s solution is straightforward:

define a single protocol that functions across all systems.

Instead of building NÃ—M integrations, you build N clients (one per AI application) and M servers (one per data source). The total integration work drops from NÃ—M to N+M.

When a new AI assistant

7

wants to support all existing data sources, it just needs to implement the MCP client protocol once.

When a new data source wants to be available to all AI assistants, it just needs to implement the MCP server protocol once.

Two critical architectural features power this efficiency:

1. Dynamic Capability Discovery

In traditional integrations, the AI application

8

must know the data source details in advance.

If the API changes, the application breaksâ€¦

MCP flips this. It uses a â€œhandshakeâ€ model. When an AI connects to an MCP server, it asks,

â€œWhat can you do?â€

.

The server returns a list of available resources and tools in real time.

RESULT:

You can add a new tool to your database server, like a â€œRefund Userâ€ function. The AI agent finds it right away the next time it connects. You wonâ€™t need to change any code in the AI application.

2. Decoupling Intelligence from Data

MCP separates the thinking system (AI model) from the knowing system (the data source).

For Data Teams:

They can build robust, secure MCP servers for their internal APIs without worrying about which AI model will use them.

For AI Teams:

They can swap models without having to rebuild their data integrations.

This decoupling means your infrastructure doesnâ€™t become obsolete whenever a new AI model gets released.

You build your data layer once, and it works with whatever intelligence layer you choose to plug into it.

An AI agent using MCP doesnâ€™t need to know in advance what tools are available. It simply connects, negotiates capabilities, and uses them as needed. This enables a level of flexibility and scale that is impossible with traditional static integrations.

The high-level concept is straightforward.

But the real elegance is in how the architecture actually works under the hoodâ€¦

MCP Architecture Deep Dive

MCPâ€™s architecture has three main layers.

These layers separate concerns and allow for easy scalability:

The Three-Layer Model

Hosts

are user-facing apps.

This includes the Claude Desktop app, IDEs like VSCode, or custom AI agents you create. Hosts are where users connect with the AI. They are also where requests begin. They do more than display the UI; they orchestrate the entire user experience.

The host application interprets the userâ€™s prompt.

It decides whether external data or tools are needed to fulfill the request. If access is necessary, the host creates and manages several internal clients. It keeps one client for each MCP server it connects to.

The protocol layer handles the mechanics of data access.

Clients

are protocol-speaking connection managers that run on hosts.

Each client maintains a dedicated 1:1 connection with a single MCP server. They serve as the translation layer.

They convert abstract AI requests from the host into clear MCP messages. These messages, like tools/call or resources/read, can be understood by the server. Clients do more than send messages. They manage the entire session lifecycle. This includes handling connection drops, reconnections, and state.

When a connection starts, the client takes charge of capability negotiation. It asks the server which tools, resources, and prompts it supports.

Servers

provide context.

They act as the layer that connects real-world systems, such as PostgreSQL databases, GitHub repositories, or Slack workspaces.

A server connects the MCP protocol to the data source. It translates MCP requests into the systemâ€™s native operations. For example, it turns an MCP read request into a SQL

SELECT

query. They are very flexible. They can run on a userâ€™s machine for private access or remotely as a cloud service.

Servers share their available capabilities when connected. They inform the client about the Resources, Prompts, and Tools they offer.

Core Primitives

MCP defines three fundamental primitives that servers can expose:

1 Resources

Theyâ€™re context-controlled by applications. They provide data that the AI can read but not change.

Resources might be:

Database query results.

Files from a file system.

API responses from a web service.

Documentation or knowledge base articles.

2 Prompts

Theyâ€™re reusable instruction templates with variables. Think of them as parametrised prompts that can be shared across applications.

Users and applications can invoke prompts with different values for code and focus areas. Prompts help standardize common AI workflows.

Plus, they make it easy to share effective prompting techniques.

3 Tools",https://newsletter.systemdesign.one
https://newsletter.systemdesign.one/p/what-is-context-engineering,Context Engineering 101: How ChatGPT Stays on Track,#109: What is Context Engineering,2026-01-02T16:55:11.108138,Neo Kim,"Get my system design playbook for FREE on newsletter signup:

Subscribe

Share this post

& I'll send you some rewards for the referrals.

Youâ€™ve probably used an AI assistant like ChatGPT and gotten an answer that felt off.

You rewrote your question, added â€œ

think step by step,

â€ and maybe gave a bit more detail. Sometimes it helped. Sometimes it didnâ€™tâ€¦

That kind of trial and error is a form of prompt engineering: trying different ways of asking to get a better response. For simple tasks, itâ€™s often enough. But once you ask the assistant to do something more complex, wording usually isnâ€™t the main problem anymore.

More often, the issue is that the model is working with the wrong information.

Something important is missing, buried in the conversation, or mixed in with a lot of irrelevant text. The result can look like confusion: it loses the thread, makes shaky assumptions, or answers confidently without solid grounding.

This is where

context engineering

comes in.

Instead of asking, â€œ

How do I phrase this better?

â€, you ask, â€œ

What information should the model see right now?

â€

Andrej Karpathy, one of the founding members of OpenAI, describes it as:

â€œThe delicate art and science of filling the context window with just the right information for the next step.â€

This newsletter looks at what that means in practice and how you can start doing it yourself (and how your favourite products like ChatGPT do it for the best user experience).

Onward.

Context is a system. Ours is the best. (Newsletter partner).

Most AI tools treat context like â€œa few files + a prompt.â€

Augment Codeâ€™s Context Engine

treats it like an engineering problem: retrieve the right dependencies, contracts, call paths, and historyâ€”then keep it fresh as your code changes.

Less drift. Less guessing. More ship.

See Augmentâ€™s Context Engine

I want to introduce

Louis-FranÃ§ois Bouchard

as a guest author.

He focuses on making AI more accessible by helping people learn practical AI skills for the industry alongside 500k+ fellow learners.

What is Context?

Before getting into techniques or frameworks, it helps to be clear about what â€œcontextâ€ actually means.

When you send a message to an AI assistant, it doesnâ€™t just see your latest question. It sees the information included with your message, such as system instructions that guide its behavior, relevant parts of the conversation so far, any examples you provide, and sometimes documents or tool outputs.

All of that together is the context.

This matters because the model lacks long-term memory as humans do. It cannot recall past conversations unless that information is included again. Each response is generated solely from the current context.

The model can pay attention to only a limited amount of text at once. This limit is often called the

context window

. Dumping more into that space often makes answers worse, not better.

Everything the model sees when generating a response: system instructions, examples, tools, conversation history, and the current messageâ€”all compete for limited space.

Context engineering is about managing that working space.

The goal is not to give the model as much information as possible, but to give it the right information at the moment it needs to respond.

Why does this matter for agents?

This becomes more important when the AI is doing more than answering a single question.

A simple chatbot takes your question, replies, and stops. But more advanced AI systems, often called

agents

, work on tasks that unfold over many steps. They might search for information, read results, summarize what matters, and then decide what to do next.

Each step generates new information that is added to what the model sees next, such as search results, summaries, and intermediate notes. Over time, the context grows, and much of it becomes no longer relevant to the current step. This is called

context rot

, where useful information gets buried under outdated details.

Model performance degrades as the context grows. The sweet spot is finding the minimum high-value information neededâ€”more context isnâ€™t always better.

Agents often work well on focused tasks.

But when a task is broad and requires many steps, the quality can vary. As the context gets heavier, important details from earlier can get lost.

The Anatomy of Context

Understanding what causes context rot is the first step.

The next is knowing exactly what goes into the context window so you can control it.

When an AI generates a response, it is not just reacting to your last message. It is responding to a structured bundle of inputs. Each part plays a different role, but they all compete for the same limited space.

System Prompt and User Prompt

The

system prompt

determines the model's overall behavior.

It describes how the assistant should act, the rules it should follow, and the kinds of responses expected.

Most of the time, you do not see the system prompt directly. Itâ€™s defined by the product or application you are using. This is why two assistants built on the same underlying model can behave very differently.

For example, ChatGPT tends to answer politely, refuse certain requests, and format responses in predictable ways, even if you never explicitly asked it to do so.

The

user prompt

is your message.

This includes your current question and, in a chat setting, earlier messages that are still included.

Both are sent to the model together. The system prompt guides behavior, and the user prompt describes what to do right now.

If you are building an AI feature and you control the system prompt, the hard part is balance. If the instructions are too strict, the assistant can become brittle when something unexpected occurs. If they are too vague, responses become inconsistent.

A practical approach is to start minimal, test with real use cases, and add rules only when you see specific failures.

Examples

Sometimes the clearest way to guide an AI is to show it what you want.

Instead of writing a long list of rules, you can include one or two example inputs and the exact outputs you expect. This is often called

few-shot prompting

.

You have probably done this in ChatGPT without realizing it. If you say, â€œ

Format it like this

,â€ and paste a sample answer, the model will usually follow the pattern.

Examples work because they remove ambiguity. They show tone, structure, and level of detail in a way that instructions often cannot.

The tradeoff is space. Examples take up room in the context window, so they need to earn their spot. A few well-chosen examples are usually better than a long list.

Message History

In a chat, the model can respond to follow-up questions because earlier messages remain in context.

For example, if you ask ChatGPT, â€œ

What is the capital of France?

â€ and then ask, â€œ

What is the population?

â€, it can usually infer you still mean the capital you just discussed.

This works because the conversation so far acts like shared scratch paper. The model does not truly remember the earlier exchange. It is simply reading it again as part of the input.

The problem is that the message history grows over time. As more turns accumulate, older messages take up space even when they are no longer relevant. That can make the model less focused. It may repeat itself, follow outdated assumptions, or miss a detail that matters now.

Managing message history usually means keeping what is still relevant, summarizing what is settled, and letting the rest drop out of the active context.

Tools

On its own, an LLM can only generate text. Tools let it do more than that.

Tools allow an agent to search the web, read documents, run code, query databases, or interact with external systems. When a tool is used, the result is usually fed back into the context so the model can use it in the next step.

You have seen this in ChatGPT when it searches the web or analyzes a file you uploaded. The output becomes part of what the model sees before it responds.

Tools are powerful, but every tool call adds more text that competes for attention. If a tool returns too much information or in an unclear format, it can overwhelm the model rather than help it.

Good tool design keeps results focused and predictable. Clear names, narrow responsibilities, and concise outputs make it easier for the model to use tools effectively.

Data

Beyond messages and tools, agents often work with external data.

This can be a document you upload, an article you paste into the chat, or files the system can access. When that information is included, it becomes part of the context.

Large documents do not always behave the way you expect. The model may focus on the wrong section or miss details. This is often a context management problem, not carelessness.

Managing documents usually means breaking them into smaller pieces, pulling in what is relevant to the current step, and leaving the rest out of the active context until needed.

Stop stuffing the context window. (Newsletter partner).

The trick isnâ€™t more contextâ€”itâ€™s the right context at the right step.

Augmentâ€™s Context Engine

curates and composes context for professional engineers working in real, messy systemsâ€”so your agent can reason about architecture, not just autocomplete a diff.

Explore how it works

Context Retrieval Strategies

System instructions, examples, tools, and message history are the context in which you can write directly.

But often the most important information is not known in advance. It has to be retrieved during the task.

For example, if you ask ChatGPT a question about a PDF you uploaded, it needs to find the relevant section. If you ask it to search the web, it has to decide what to search for and which results matter.

Loading upfront (left) retrieves chunks upfront based on the query. Just-in-time (right) retrieves as the model reasonsâ€”more precise, but more round-trips.

How an agent retrieves and injects information is a major part of context engineering. There are two main approaches:

loading everything upfront

, or

retrieving as you go

.

Loading Upfront

The simplest approach is to retrieve relevant information before the model starts responding, then include it in the context all at once.

This is what happens when ChatGPT searches the web and then writes an answer using the results it just found. The model is not answering from memory. It is answering based on the information that was retrieved and added to its context.

This pattern is commonly called retrieval augmented generation (

RAG

).

Loading upfront works well when the question is clear, and the agent can predict what information will be useful. The downside is that the agent makes an early retrieval decision and may stick with it.

If something important is missing or the task changes direction, it can be harder to correct course.

Just-in-Time Retrieval

Another approach is to retrieve information as the task unfolds.

Instead of loading everything at the start, the agent takes a step, looks at what it has learned so far, and retrieves more information only when needed. You can sometimes see this in ChatGPT when it searches, reads, refines the query, and searches again during longer tasks.

This keeps the context cleaner because only the information actually needed gets pulled in. The tradeoff is that it takes more steps and requires the agent to decide when to retrieve and when to stop.

A useful pattern within just-in-time retrieval is to start broad and then drill down. This specification is called

Progressive Disclosure.

Rather than loading full documents immediately, the agent may start with short snippets or summaries, identify what looks relevant, and pull in more detail only then.

This is how humans tackle research, too.

You do not read every article in a database. You scan titles, read abstracts of promising ones, and dive deep only into the sources that matter.

Hybrid Strategy

Fortunately, you donâ€™t have to pick one or the other.

Many agents combine both approaches. They load a small amount of baseline information upfront, then retrieve more as needed.

You can see this in tools like ChatGPT. Some instructions and conversation history are already present, and additional information, such as search results or document excerpts, is pulled in based on what you ask.

For simpler use cases, loading upfront is often enough. As tasks get more complex and span multiple steps, retrieving as you go becomes more important.

The right choice depends on how predictable your agentâ€™s information needs are.

Your codebase is bigger than your IDE. (Newsletter partner).

Modern systems sprawl across repos, services, configs, and runbooks.

Augmentâ€™s Context Engine

is built to pull relevant context across that realityâ€”so AI answers donâ€™t collapse the moment you leave the file you have open.

Get real context

Techniques for Long-Horizon Tasks

Retrieval helps an agent pull in the right information.

But some tasks create a different problem. They run long enough that the agent produces more text than can fit in the context window.

You may have seen this in ChatGPT during long conversations or research tasks. Early responses are clear, but after many steps, the answers can drift or repeat themselves, especially when you send very long instructions, like asking for help with entire code bases. Over a long task, the agent can encounter far more information than it can keep in working memory at once.

Larger context windows are not a complete solution. They can be slower and more expensive, and they still accumulate irrelevant information over time. A better approach is to actively manage what stays in context and preserve the important parts as the task grows.

Three techniques help with this:

compressing the context

when it gets full,

keeping external notes

,

splitting work across multiple agents

.

1. Compaction

When the context approaches its limit, one option is to compress whatâ€™s there.

The agent summarizes the conversation so far, keeping the essential information and discarding the rest. This compressed summary becomes the new starting point, and the conversation continues from there.

You may have noticed something like this in long ChatGPT conversations. After many messages, earlier details can fade. This often happens because older parts of the conversation are shortened or dropped to make room for new input.

When context fills up, compress what mattersâ€”key decisions, goals, and factsâ€”while keeping recent messages verbatim. The result: a fresh context that preserves the essential state.

The hard part is deciding what to keep.

The goal, key constraints, open questions, and decisions that affect future steps should stay. Raw tool outputs that have already been used can usually go. Repeated back and forth that does not change the plan can go too.

There is always a risk of losing something that matters later. A common safeguard is to store important details outside the context before discarding them, so the agent can retrieve them if needed.

2. Structured Note Taking

Compaction occurs when youâ€™re running out of space. Structured note-taking happens continuously.

As the agent works, it keeps a small set of notes outside the context window. These notes capture stable information, such as the goal, constraints, decisions made so far, and a short list of what remains.

You can see a user-level version of this idea in features like ChatGPTâ€™s memory. If you tell it to remember something, that information can persist beyond a single conversation and be brought back when relevant.

This works well for tasks with checkpoints or tasks that span multiple sessions.

A coding agent might keep a checklist of completed steps. A support assistant might store user preferences so that it does not have to ask the same questions again.

3. Sub-Agent Architectures

Sometimes the best approach is to break a large task into pieces and assign each piece to a separate agent with its own context window.

In many research-style agent designs, a main agent coordinates the overall task, while sub-agents handle focused subtasks. A sub-agent explores one area in depth, then returns a short summary. The main agent keeps the summary and moves on without carrying all the raw details forward.

You can think of research features in tools like ChatGPT as an example of the kind of workflow where this pattern is useful.

Sub-agents work in their own context windows, using tens of thousands of tokens, but only pass condensed summaries to the lead agentâ€”keeping its context clean and focused.

This works well when subtasks can run independently or require deep exploration.

The tradeoff is complexity. Coordinating multiple agents is harder than managing a single one, so it is usually best to start with simpler techniques and add sub-agents when a single agent becomes overwhelmed.

Choosing the Right Technique

Thereâ€™s no one-size-fits-all solution. The right approach depends on your â€œagent and your use caseâ€. These rules of thumb can help:

Compaction

works best for long, continuous conversations where context gradually accumulates.

Structured notes

work best for tasks with natural checkpoints or when information needs to persist across sessions.

Sub-agents

work best when subtasks can run in parallel or require deep, independent exploration.

These techniques can be combined. Start with the simplest approach and add complexity as needed.

Putting It All Together

Context engineering is not a single technique.

Itâ€™s an approach to designing AI systems. At each step, you decide what goes into the modelâ€™s context, what stays out, and what gets compressed.

Start simple and iterate. Test on small tasks firstâ€”if it works, scale up and test again. If it fails, diagnose the problem type and apply the targeted fix. Either way, loop back and keep testing until it works reliably.

The components we covered work together.

System prompts and examples shape behavior. Message history maintains continuity. Tools let the agent take actions. Data gives it information to work with. Retrieval strategies determine how and when that information gets loaded. For long-running tasks, compaction, external notes, and sub-agents help manage context that would otherwise overflow.

When something goes wrong, context is often the place to look. If the agent hallucinates, it might need better retrieval to ground its answers. If it picks the wrong tool, the tool descriptions might be unclear. If it loses track after many turns, the message history might need summarization.

A practical approach is to start simple.

Test with small tasks first. If it works, scale up. If it fails, identify what went wrong and address the specific issue.

Context engineering, operationalized. (Newsletter partner).

If you like the idea of context engineering, youâ€™ll like what Augment does in practice:

A dedicated

Context Engine

that continuously grounds your AI in the code, relationships, and constraints that actually matterâ€”without you playing prompt Tetris.

Learn more

I launched

Design, Build, Scale

(newsletter series exclusive to PAID subscribers).

When you upgrade, youâ€™ll get:

High-level architecture of real-world systems.

Deep dive into how popular real-world systems actually work.

How real-world systems handle scale, reliability, and performance.

10x the results you currently get with 1/10th of your time, energy, and effort.

ğŸ‘‰

CLICK HERE TO ACCESS DESIGN, BUILD, SCALE!

ğŸ‘‹ Find me on

LinkedIn

|

Twitter

|

Threads

|

Instagram

Want to reach 200K+ tech professionals at scale?

ğŸ“°

If your company wants to reach 200K+ tech professionals,

advertise with me

.

Thank you for supporting this newsletter.

You are now 200,001+ readers strong, very close to 201k. Letâ€™s try to get 201k readers by 21 December. Consider sharing this post with your friends and get rewards.

Yâ€™all are the best.

Share",https://newsletter.systemdesign.one
https://newsletter.systemdesign.one/p/design-a-chat-system,System Design Question: Design WhatsApp,#108: System Design Interview - Part 2,2026-01-02T16:55:15.680827,Neo Kim,"Get my system design playbook for FREE on newsletter signup:

Subscribe

Share this post

& Iâ€™ll send you some rewards for the referrals.

Block diagrams created using

Eraser

.

Designing a real-time chat app like WhatsApp is a popular system design interview question.

In a short period, you need to show you understand not just the happy path, but also all the things that can go wrong when messages fly between disconnected users on mobile networks.

In Part 1, we covered:

Core architecture and data models

Service discovery and presence management

Handling online and offline message delivery

And many more!

Today weâ€™ll cover:

Scaling from one server to thousands with Redis Pub/Sub

Database sharding and replication strategies

Message storage service internals

Scheduled jobs and cleanup

Message deduplication, failure recovery, and edge cases

End-to-end encryption

1

and key management

Rate limiting and abuse prevention

Analytics and monitoring infrastructure

Multi-region deployment and global distribution

Schema versioning and backward compatibility

By the end, youâ€™ll understand not just what to build, but why each decision is made and what breaks when you get it wrong.

Onward.

Software engineers, are you prepared if layoffs happen again? (Sponsor)

Build a GitHub portfolio with 4 production-grade projects.

Build real AI systems:

autonomous agents, recommendation engines, and multimodal pipelines.

Master the modern stack:

LangChain, TensorFlow, PyTorch, multi-agent architectures, and RAG systems.

Flexible & Practical:

5-month, fully online, project-based program for working engineers

Live hands-on workshops + 24/7 support

Earn up to 56% more by mastering production-grade AI.

Next cohort starts on 19 January 2026.

Imagine leading AI initiatives with confidence:

CLICK HERE TO KNOW MORE

I want to reintroduce

Hayk Simonyan

as a guest author.

Heâ€™s a senior software engineer specializing in helping developers break through their career plateaus and secure senior roles.

If you want to master the essential system design skills and land senior developer roles, I highly recommend checking out Haykâ€™s

YouTube channel

.

His approach focuses on what top employers actually care about: system design expertise, advanced project experience, and elite-level interview performance.

Scaling

Our single chat server design works great for 1000 users. But itâ€™s terrible for a billion.

So letâ€™s scale horizontally...

Problem: Users on Different Servers

User A connects to Server 1. User B connects to Server 2.

So how does a message from User A reach B?

Option 1: Load balancer does nothing

This approach doesnâ€™t work. Server 1 receives the message but has no connection to User B.

Option 2: Kafka topics per user

Create a Kafka topic (a category or feed name to which messages are published) for each user. Servers subscribe to topics for their connected users.

Why this approach wonâ€™t work?

Kafka needs about 50KB of overhead per topic.

1 billion users = 50 TB just for topic metadata.

Kafka wasnâ€™t built for billions of topics. Itâ€™s designed for thousands of high-throughput topics, not billions of low-throughput ones.

Option 3: Consistent hashing

Hash user IDs to specific servers

2

.

Always route the same user to the same server. When a message arrives for User B, forward it directly to Server 2.

Pros:

Direct server-to-server communication,

No extra infrastructure.

Cons:

All-to-all connections between servers (if you have 1000 servers, each needs connections to 999 others).

Adding new servers requires careful coordination to avoid dropping messages during the transition.

Servers need to be big to keep the fleet small.

Option 4: Redis Pub/Sub (what we actually use)

This is the cleanest solution at our scale.

How it works:

Redis Pub/Sub

3

is a lightweight messaging system built into Redis. It lets servers subscribe to channels and publish messages to those channels. Think of it like a radio station (channel) where servers can tune in to listen and broadcast messages.

When User B connects to Chat Server 2, Server 2 subscribes to Redis channel of user:B.

When Server 1 needs to deliver a message to User B, Server 1 publishes the message to Redis channel user:B.

Redis routes it to Server 2. Then Server 2 delivers to User B via WebSocket

4

.

Why we chose this:

Redis Pub/Sub uses only a few bytes per channel (versus Kafkaâ€™s 50KB per topic).

Itâ€™s â€œat most onceâ€ delivery, meaning if no oneâ€™s listening when you publish, the message is lost. But thatâ€™s fine for our use case. We already have durability through the Message Queue

5

and database. Pub/Sub is just for real-time delivery to online users. If theyâ€™re offline, they still get messages from their inbox.

The beauty is that chat servers donâ€™t need to know about each other. They talk only to Redis. Add or remove chat servers at will. No coordination needed.

Tradeoffs:

Redis Pub/Sub adds a few milliseconds of latency (single-digit, usually under 5ms) because messages bounce through Redis.

But this is negligible compared to network latency.

We also need to run a Redis cluster for high availability, which adds operational complexity. Yet the simplicity of not managing server-to-server connections makes it worth it.

Message Storage Service: The Detailed Flow

The Message Storage Service sits between Message Queue and Database.

Letâ€™s break down exactly what it doesâ€¦

Architecture:

â†’ Message Queue (Kafka)

â†’ Message Storage Service instances

â†’ Database Cluster

Each instance:

Consumes from assigned Kafka partitions

Batches writes for efficiency

Handles retries and failures

Updates indexes

Detailed Processing Flow:

Step 1: Consuming from Queue

Storage Service Instance:

Subscribes to Kafka partitions 0-9 (out of 100 total)

Polls for new messages every 100ms

Receives a batch of 500 messages

Processes batch in parallel (thread pool of 20 workers)

Step 2: Message Validation and Enrichment

For each message:

Validate schema (required fields present)

Check message size limits (text < 4KB, no huge strings)

Enrich with metadata:

Calculate message hash (for deduplication)

Extract mentions (

@username

patterns)

Identify links for preview generation

Classify content type

Generate indexes needed for queries

Step 3: Batch Writing

Instead of writing messages one-by-one:

Accumulate messages in the memory buffer

When buffer hits 1000 messages OR 5 seconds elapsed:

Sort messages by target shard

Group messages by conversation

Prepare batch

INSERT

statements

3. Write the entire batch in one operation per shard

Why batching matters:

A single database INSERT takes ~5ms.

1000 individual INSERTs = 5 seconds.

One batched INSERT of 1000 messages = 50ms.

Thatâ€™s a 100x throughput improvement.

Step 4: Writing to Database (Cassandra)

For each shard:

Cassandra processes this batch atomically. All succeed, or all fail.

Step 5: Updating Indexes

Simultaneously with message write:

Update conversation index:

Latest message timestamp

Unread count

Last message preview

Update search index (if full-text search enabled):

Extract keywords

Send to Elasticsearch cluster

Update analytics:

Increment counters (messages/day, active users)

Send to analytics pipeline (separate Kafka topic)

Step 6: Handling Failures

If write fails:

Log error with message details

Check error type:

Temporary (connection timeout): Retry with exponential backoff

6

Permanent (validation error): Move to dead-letter

7

queue for investigation

Duplicate key: Skip (already written, this is retry)

Donâ€™t block other messages

Update monitoring metrics

Step 7: Committing Offset

After a successful write:

Commit Kafka offset (mark messages as processed)

This ensures we donâ€™t reprocess on restart

If service crashes before commit:

On restart, reprocess from last committed offset

Duplicates handled by checking

message_id

Scaling the Storage Service:

Each instance is stateless. To handle more throughput:

Add more instances (they auto-distribute Kafka partitions)

Each instance processes independently

No coordination needed between instances

Kafka handles partition assignment

If you have 10 instances and add an 11th, Kafka automatically rebalances. Some instances give up partitions to the new instance. Within seconds, the load is redistributed.

Monitoring Critical Metrics:

Consumer lag: How many messages behind are we?

(Alert if lag > 10,000 messages)

Write latency: How long does each batch take?

(Alert if p99 > 500ms)

Error rate: How many failures?

(Alert if > 0.1%)

Throughput: Messages per second processed

(Scale up if maxed out)

Tradeoffs:

Message Storage Service adds latency (messages sit in the queue before being written).

But it decouples chat servers from database performance. Chat servers can acknowledge messages immediately without waiting for database writes.

This keeps user-facing latency low even if the database is temporarily slow.

End-to-End Encryption and Key Management

WhatsAppâ€™s defining feature is end-to-end encryption.

Messages are encrypted on the senderâ€™s device and only decrypted on the recipientâ€™s device. Even WhatsAppâ€™s servers canâ€™t read the content.

The challenge is how to encrypt messages when users might have multiple devices, be offline, or need to verify identities?

Signal Protocol (What WhatsApp Uses):

WhatsApp implements the Signal Protocol, which provides:

End-to-end encryption

Forward secrecy

8

(compromising one message doesnâ€™t compromise others)

Future secrecy (compromising a key doesnâ€™t compromise future messages)

Key Exchange Flow:

When User A first messages User B:

Identity Keys (long-term):

Each user generates an identity key pair on registration

Public key gets uploaded to server

Private key never leaves device

Prekeys (medium-term):

User B generates 100 signed prekeys

Uploads to server

Server stores them for future initiators

One-time Prekeys:

User B generates additional one-time prekeys

Used once and discarded

Provides forward secrecy

Session Establishment:

User A fetches User Bâ€™s public identity key and a prekey from server

User A generates ephemeral key pair

Performs Diffie-Hellman key exchange

Derives shared secret (session key)

Encrypts message with session key

Sends encrypted message + Aâ€™s ephemeral public key

User B receives:

Uses their private keys + Aâ€™s ephemeral public key

Derives the same session key

Decrypts message

Establishes bidirectional encrypted session

Message Encryption:

Each message is encrypted with:

Encrypted Message = AES-256(message, session_key + message_counter)

MAC = HMAC-SHA256(encrypted_message, mac_key)

The

message_counter

prevents replay attacks. Each message increments the counter.

Ratcheting (Forward Secrecy):

After each message, keys are â€œratchetedâ€ forward:

New ephemeral key pair generated

New session key derived

Previous keys discarded

This means compromising todayâ€™s key doesnâ€™t reveal:

Yesterdayâ€™s messages,

Or tomorrowâ€™s messages.

Multi-Device Support:

User A has a phone and a laptop.

How does encryption work?

Approach 1: Sender Keys (Groups):

For groups, sending individual encrypted copies to each device is expensive.

Instead:

User A generates a sender key for the group

Encrypts sender key for each member using their session key

Distributes sender key

All future messages get encrypted with sender key

Much more efficient for large groups

Approach 2: Session Per Device (1:1):

For 1:1 chats.

Each device has its own identity

User Aâ€™s phone and laptop are separate entities

When messaging User B, send two encrypted copies (one for each of Bâ€™s devices)

Tradeoffs:

End-to-end encryption adds complexity.

Canâ€™t search messages server-side

Canâ€™t provide message previews in push notifications

9

(would require server decryption)

Server canâ€™t filter spam/abuse (content is opaque)

Multi-device sync is harder

But the privacy benefits are worth it. Users trust WhatsApp because messages are truly private.

Ready for the best part?

Database Scaling: Sharding Strategy",https://newsletter.systemdesign.one
https://newsletter.systemdesign.one/p/system-design-stock-exchange,Design Stock Exchange - A Deep Dive,#107: System Design Stock Exchange - Part 3,2026-01-02T16:55:17.129895,Neo Kim,"Share this post

& I'll send you some rewards for the referrals.

Block diagrams created with

Eraser

.

Last month, I launched

Design, Build, Scale

â€¦ the newsletter series that will elevate your software engineering career.

This email (Part 3) is a little longer than the previous two, so Iâ€™ll get straight into itâ€¦

TL;DR

of

Part 1

&

Part 2

!

Key components of an exchange

are

:

Broker - allows people to buy and sell stocks on the exchange.

Gateway - entry point for brokers to send BUY or SELL orders to the exchange.

Matching Engine - matches BUY and SELL orders to create trades.

Order Processing Workflow:

User creates a BUY or SELL order through the broker.

Gateway validates the order: authentication, check request format & rate limiting.

Then it forwards the request to the order manager. It performs risk checks using rules defined in the Risk Manager.

Risk manager verifies whether the userâ€™s wallet has sufficient funds. Plus, it freezes the required amount in the wallet to prevent double-spending.

Order Manager sends the request to the Matching Engine.

Matching Engine keeps an in-memory â€œorder bookâ€ for each stock. It matches BUY and SELL orders based on price and time priority.

After a match occurs, the Matching Engine creates two executions (fills). One for the buyer. One for the seller.

Matching Engine distributes trade results as market data to clients through the Gateway. It uses UDP multicast

1

for efficiency.

Most steps â€œbefore and afterâ€ matching run in parallel to BOOST throughput.

Onward.

Start building AI backendsâ€“Agents as microservices (Sponsor)

Chatbots and copilots are just the surface.

The real shift is happening underneath, where AI agents work behind the scenes, making smart decisions that developers used to hardcode.

AgentField

is the open-source control plane for that:

Run agents as services

Discover services/agents

Coordinate tasks asynchronously

Use cryptographic IAM and audit trails

Use it to build AI-native backends that can reason about orders, customers, payments, and incidents (while following the rules YOU set).

You focus on the logic. AgentField handles running it in production.

Explore the code on GitHub

:

https://github.com/Agent-Field/agentfield

Try examples, test features, and â­ the repo if it helps!

New to AI backends?

Start here:

https://www.agentfield.ai/blog/posts/ai-backend

See why autonomous backends are crucial for system design.

Want to run 1000s of agents?

Read this:

https://agentfield.ai/docs

Learn how to run agents safely at scale.

â­ STAR THE REPO

FIX Protocol

Broker send orders to the gateway using the Financial Information eXchange (

FIX

) protocol (for institutional clients).

FIX is a text-based protocol. Although reliable, itâ€™s slow because text parsing consumes CPU. Gateway uses a FIX engine to receive and send FIX messages.

You can build your own engine for ultra-low latency, or buy a pre-optimized commercial one.

Exchange streams market data using FIX Adapted for STreaming

2

(

FAST

).

Itâ€™s much faster because itâ€™s a binary, compressed version of FIX... binary encoding is compact and easy for CPUs to parse.

Many exchanges provide even faster binary protocols for high-frequency trading:

Integrated Trade Capture Handler (

ITCH

) - a fast market data feed

Order Unified Communication Handler (

OUCH

) - a fast order entry protocol

These avoid text parsing and are specifically designed for low-latency trading.

Letâ€™s keep going!

Gateway

It accepts connections from brokers and supports REST + WebSocket APIs

3

(for retail traders).

Gateway could use Nginx server for HTTPS termination and WebSocket routing. Plus, exchange could run many gateway servers in parallel behind a load balancer for high availability.

Gateway:

Tracks sequence numbers of each client,

Normalize and validate incoming messages,

Protects matching engine from malformed or abusive client traffic,

Uses TCP backpressure

4

to slow down misbehaving or overloaded clients automatically,

Allows only ONE outstanding order per client to maintain strict, deterministic ordering.

Exchanges used to have MANY gateways (some faster, some slower).

But traders exploited this by:

Flooding slower gateways with junk orders to delay competitors,

Routing orders only through the fastest gateway (unfair advantage).

So modern exchanges use a SINGLE gateway for

all clients of the â€œsame classâ€

.

This means all clients in the same class (institutional clients, retail traders) have identical latency

5

. Thus ensuring fairness and reducing system load.

Hereâ€™s how Gateway handles concurrency:

Inbound (broker â†’ gateway):

Assigns one thread per TCP connection

Parses messages and publishes them to a Disruptor ring buffer

Disruptor efficiently distributes events to internal services (order manager, risk checks,...)

Outbound (matching engine â†’ broker):

Outbound events come through the ring buffer

Ring buffer dispatches events to subscribed services

Each session has its own outbound handler thread

If a client is slow, backpressure (ring buffer fills up) prevents system slowdowns

NOTE: Inbound and outbound paths share no mutable state.

Wallet

A wallet stores a userâ€™s funds & assets for trading.

But thereâ€™s a risk of DOUBLE SPENDING if two orders from the same user arrive at onceâ€¦ and both threads try to read the â€œsameâ€ balance.

Example:

Thread A reads balance X and subtracts Y

Thread B reads balance X and subtracts Z

Here are three ways to prevent this problem:

1 Mutex Locks

Only one thread updates the userâ€™s balance at a time. This is safe, but it significantly slows down the system.

2 ConcurrentHashMaps

They allow fast (parallel) reads/writes if keys fall into different segments. Yet it doesnâ€™t protect the correctness of multi-step operations (

read â†’ modify â†’ write

).

3 Database Row-Level Locking

Exchanges rely on SQL databases (like MySQL InnoDB) because they:

Support ACID (safe transactions)

Provide row-level locking, so two threads cannot update the same userâ€™s balance at once

Plus, this approach makes the

read â†’ modify â†’ write

operations safer.

Sample Database Schema

user_account (user_id, balance)

wallet (user_id, symbol, quantity)

stock (symbol, price)

Tables can be sharded by

â€œuser_idâ€

for performance.

Data Structures

account balance -

HashMap {user_id: balance}

stock price -

HashMap {symbol: price}

Ready for the next technique?

Disruptor Pattern

Disruptor is a concurrency model that doesnâ€™t use locks.

Itâ€™s built as a ring buffer (fixed-size circular array with up to 20 million slots).

Imagine the ring buffer as:

â€œsuper-fast, in-memory queue for passing events between threads.â€

Producers write events; consumers read them in order.

Each thread maintains its own sequence number, so there is no locking, blocking, or contention.

A disruptor offers:

high throughput,

microsecond-level latency.

Itâ€™s excellent for workloads where many components must process the same event.

Hereâ€™s WHERE the stock exchange uses the disruptor:

1 Gateway

Gateway publishes incoming orders to an input disruptor.

It allows many tasks to run in parallel (without blocking each other), such as:

journaling to disk

unmarshalling (parsing messages)

assigning global sequence numbers

replicating events to standby engines

Each step is independent; disruptor provides high throughput without locks.

2 Order Manager

Order manager uses disruptors to broadcast events to various internal consumers, such as:

risk checks

routing logic

audit/journal writers

user balance service

sequence enforcement

Each consumer reads using its own sequence counter,,, so they never block each other.

3 Market Data

Output disruptors take results and prepare them for network output:

execution reports

market data updates

client notifications via gateway

Each topic (e.g., trades, market data) gets its own disruptor to avoid interference.

Ready for the best part?

Reminder: this is a teaser of the subscriber-only newsletter series, exclusive to my golden members.

When you upgrade, youâ€™ll get:

High-level architecture of real-world systems.

Deep dive into how popular real-world systems actually work.

How real-world systems handle scale, reliability, and performance.

Subscribe",https://newsletter.systemdesign.one
https://newsletter.systemdesign.one/p/whatsapp-system-design,System Design Interview: Design WhatsApp,#106: System Design Interview - Part 1,2026-01-02T16:55:18.467793,Neo Kim,"Get my system design playbook for FREE on newsletter signup:

Subscribe

Share this post

& Iâ€™ll send you some rewards for the referrals.

Block diagrams created using

Eraser

.

Designing a real-time chat application like WhatsApp is one of the most common system design interview questions.

In a short period, you need to show that you understand not just the happy path, but also all the things that can go wrong when messages fly between disconnected users on mobile networks.

WhatsApp handles over 100 billion messages daily for 2+ billion users.

Weâ€™ll build this from scratch, starting with a simple version that works for a few thousand users, then scale it up to handle billions of users sending messages across unreliable mobile networks.

This is a practical guide:

We wonâ€™t hand-wave away the hard parts or pretend everything works. Instead, weâ€™ll build something functional, identify where it breaks, and fix it properly.

What weâ€™ll cover in Part 1:

Requirements and capacity estimation

Core architecture and data models

Why WebSockets beat other protocols for real-time messaging

Service discovery and presence management

Handling online and offline message delivery

Multi-device synchronization

Media file uploads without killing your servers

By the end, youâ€™ll understand not just what to build, but why each decision is made and what breaks when you get it wrong.

Onward.

ğŸ”¥

Ditch the vibes, get the context (sponsor)

Augment Code

â€™s powerful AI coding agent meets professional software developers exactly where they are, delivering production-grade features and deep context into even the gnarliest codebases.

With Augment Code, you can:

Keep using VS Code, JetBrains, Android Studio, or even Vim

Index and navigate millions of lines of code

Get instant answers about any part of your codebase

Build with the AI agent that gets you, your team, and your code

ğŸ‘‰

Ditch the Vibes and Get the Context You Need to Engineer Whatâ€™s Next

I want to introduce

Hayk Simonyan

as a guest author.

Heâ€™s a senior software engineer specializing in helping developers break through their career plateaus and secure senior roles.

If you want to master the essential system design skills and land senior developer roles, I highly recommend checking out Haykâ€™s

YouTube channel

.

His approach focuses on what top employers actually care about: system design expertise, advanced project experience, and elite-level interview performance.

Requirements: What Are We Actually Building?

Starting simpleâ€¦ weâ€™re designing a real-time messaging app that lets people send text messages and media to each other. But letâ€™s get specific about what â€œreal-timeâ€ actually means here.

Core requirements:

Users can send and receive text messages in 1:1 chats

Group chats with up to 100 participants (weâ€™ll see why this number matters later)

Messages delivered while users are offline get queued up for 30 days

Media attachments (images, videos, audio clips)

Message status tracking (sent, delivered, read)

Online/offline status with

â€œlast seenâ€

Non-functional requirements:

Low latency delivery (under 500ms when users are online)

Guaranteed message delivery (messages canâ€™t just disappear)

Handle billions of users with high throughput

Donâ€™t store messages on servers longer than necessary

Stay resilient when individual components fail

The tricky bit most people miss is the offline requirement.

Itâ€™s easy to design for everyone to be online. But real life doesnâ€™t work that wayâ€¦

Back-of-the-Envelope: Understanding the Scale

Letâ€™s run the numbers so we don't build something that collapses under real load.

User metrics:

1 billion registered users

500 million daily active users

50 million concurrent connections during peak hours

Average 10-20 messages per user daily

Message estimations:

Daily messages: 500M users Ã— 20 messages = 10 billion messages/day

Thatâ€™s roughly 115,000 messages per second on average.

But during peak hours, we need to multiply that by 3-5x. So weâ€™re looking at 350,000-500,000 messages per second.

Storage calculation:

Each message with metadata averages about 1KB (text content, sender/receiver IDs, timestamp, status flags, etc.)

Daily storage: 1KB Ã— 10B messages = 10 TB/day

Annual storage: 3.6 PB/year

But hereâ€™s the thing:

Most messages get delivered and cleared from the server within seconds. Weâ€™re not storing everything forever. Users download messages to their devices, and we clean them up on the server.

Factor in the 30-day retention for undelivered messages and 10% of users keeping history on servers. And suddenly weâ€™re looking at 400-500 TB of active storage instead of petabytes.

Bandwidth:

50M concurrent connections at peak.

If each connection averages 10KB/s for active messaging, thatâ€™s 500 GB/s of bandwidth. This is why WhatsApp was famous for running on minimal infrastructure.

They are very efficient in not sending unnecessary data.

High-Level System Design

Letâ€™s build this in stages, starting simple and adding complexity only when we need it:

The Basic Components

Mobile App (Client)

The userâ€™s phone. It maintains a persistent connection to our backend, handles the UI, manages local message storage, and retries failed operations.

Load Balancer

This sits in front of our chat servers and distributes incoming connections across multiple servers.

Think of it as a traffic cop routing cars to different lanes. It routes incoming WebSocket connections to chat servers and uses sticky sessions (meaning once you connect to Server A, you stay on Server A). Simply put, users donâ€™t bounce between servers mid-conversation.

It also monitors server health and stops sending traffic to dead servers.

Chat Servers

Chat servers maintain WebSocket connections to clients (a persistent two-way communication channel that stays open), route messages between users, track whoâ€™s online, and handle message persistence.

Each server can handle hundreds of thousands of simultaneous connections.

Message Queue

This decouples message writing from delivery.

When a chat server receives a message, it immediately acknowledges receipt to the sender, then asynchronously pushes it to the queue for storage and delivery.

We can use tools like

Kafka

or

RabbitMQ

that work well here.

Message Storage Service

Consumes from the queue and writes messages to the database.

It also handles querying message history and managing retention policies (such as deleting old messages after 30 days).

Message Database

Persistent storage for messages.

NoSQL works well here (

Cassandra

,

DynamoDB

) because we need high write throughput and our query patterns are straightforward (fetch messages by user/conversation/timestamp).

Weâ€™re writing billions of messages per day, so we need something that can handle massive write volume without slowing down.

User Connection Cache

In-memory store (

Redis

) tracks which users are online, which chat server theyâ€™re connected to, and their last activity timestamp.

This makes routing decisions fast. Checking Redis takes microseconds, whereas querying a database takes milliseconds. At scale, that difference matters.

Blob Storage + CDN

Blob storage (e.g.,

AWS S3

or

Google Cloud Storage

) stores media files; it holds the actual files.

CDN (Content Delivery Network) caches popular files at edge locations worldwide so downloads are fast regardless of where users are. It offers direct upload/download paths so chat servers donâ€™t become bottlenecks for large file transfers.

Notification Service

Handles push notifications for offline users through APNs (Apple Push Notification Service for iOS) and FCM (Firebase Cloud Messaging for Android).

When someone messages you while youâ€™re offline, this makes your phone buzz.

Presence Service

Dedicated service for managing online/offline status. Receives heartbeats from users, updates Redis, and publishes presence changes to interested subscribers.

The WebSocket Decision

Most messaging systems use WebSockets.

Letâ€™s understand why by looking at what doesnâ€™t work:

Polling

The client asks,

â€œAny new messages?â€

every few seconds.

This wastes bandwidth and adds latency.

If youâ€™re checking every 2 seconds and a message arrives right after you check, you wait 2 seconds to see it. Multiply that by millions of users constantly asking for updates even when thereâ€™s nothing new, and youâ€™re burning resources for no reason.

Long polling

Client opens a request, server holds it open until thereâ€™s a message or a timeout.

Long polling is better than regular polling.

But itâ€™s still problematic:

Youâ€™re constantly opening and closing connections. Each message requires a full HTTP handshake (the back-and-forth to establish a connection).

At scale, this overhead kills you. Your servers spend more time managing connection lifecycles than actually delivering messages. Plus, idle HTTP connections still consume server resources (memory for connection state, thread pool slots) without the efficiency benefits of WebSockets.

WebSockets (WHAT WE USE)

One persistent bidirectional connection that stays open. Both the client and the server can push data at any time. Minimal overhead once established.

Why we chose WebSocket:

The connection remains open throughout the session. When User A sends a message to User B, it flows through Aâ€™s WebSocket connection to the server, which pushes it through Bâ€™s WebSocket connection instantly.

No polling and no repeated handshakes. Just data flowing both ways. The connection overhead happens only once when you open the app; everything after that is just message content.

WebSocket Tradeoffs:

It requires special load balancers that support them (Layer 4 load balancing instead of Layer 7).

They also use more memory per connection on the server since youâ€™re keeping connections open.

But for real-time messaging, the latency and bandwidth benefits make it worthwhile.

Handling Idle WebSocket Connections:

What happens when a user opens WhatsApp but doesnâ€™t send any messages for hours?

The connection stays open but idle!

We handle this with heartbeats: Client sends a lightweight ping every 30 seconds. Server responds with Pong.

If the server doesnâ€™t receive a heartbeat for 60 seconds, it assumes the connection died (network issue, app backgrounded) and closes it. This prevents the accumulation of zombie connections that waste server resources.

Idle connections still consume memory (connection state, socket buffers), but the cost is minimal compared to the benefit of instant message delivery when the user sends something.

Identifying Servers That Donâ€™t Stand Still

In production, chat servers come and goâ€¦ they crash, get deployed, scale up and down. So how does the system keep track of whatâ€™s available?

The Problem:

Your load balancer needs to know which chat servers are healthy.

Chat servers need to know which other services are available (Message Storage Service instances, Redis cluster nodes). Hardcoding IPs doesnâ€™t work when instances are ephemeral.

Solution: Service Registry Pattern

We use a service registry such as

Consul

or

AWS Cloud Map

.

Hereâ€™s how it works:

Registration:

When the Chat Server starts:

Server boots up on IP

10.0.1.45:8080

Registers itself with Consul:

Service name:

â€œchat-serverâ€

Instance ID:

â€œchat-server-abc123â€

Health check endpoint:

â€œ/healthâ€

Metadata:

{region: â€œus-eastâ€, capacity: 100000}

3. Sends a heartbeat every 10 seconds

4. If the heartbeat stops, Consul marks it unhealthy after 30 seconds

Discovery:

When the Load Balancer needs available servers:

Query Consul:

â€œGive me all healthy chat-server instancesâ€

Consul returns list:

[10.0.1.45:8080, 10.0.1.67:8080, ...]

Load balancer updates the routing table

Watches for changes (Consul notifies on updates)

Why we chose this:

Services discover each other dynamically.

Deploy new servers; they auto-register.

Kill a server, and itâ€™s automatically removed.

No manual configuration updates.

Health Checks:

Consul hits this endpoint every 10 seconds.

If the instance fails three times in a row, it is marked unhealthy.

Tradeoffs:

Service discovery adds another system to maintain (Consul cluster needs to be highly available).

But the alternative is manual instance management, which doesnâ€™t scale and causes outages when you forget to update configurations.

Data Models

Letâ€™s design tables that actually support our use cases:

Users Table (SQL -

PostgreSQL

works)

Messages Table (NoSQL - Cassandra)

Groups Table (SQL)

User Connection Registry (Redis)

Message Inbox (Redis)

WebSocket and REST API Schemas

Now letâ€™s get concrete with actual message formats:

WebSocket Connection Establishment:

Client -> Server (Initial handshake over WSS):
{
  â€œtypeâ€: â€œauthâ€,
  â€œpayloadâ€: {
    â€œuser_idâ€: â€œ123â€,
    â€œauth_tokenâ€: â€œjwt_token_hereâ€,
    â€œdevice_idâ€: â€œphone-abcâ€,
    â€œplatformâ€: â€œiosâ€,
    â€œapp_versionâ€: â€œ2.24.5â€
  }
}

Server -> Client (Auth success):
{
  â€œtypeâ€: â€œauth_successâ€,
  â€œpayloadâ€: {
    â€œsession_idâ€: â€œsess_xyz789â€,
    â€œserver_timeâ€: 1699189200000,
    â€œunread_countâ€: 47
  }
}

Sending a Message:

Client -> Server:
{
  â€œtypeâ€: â€œmessageâ€,
  â€œclient_message_idâ€: â€œclient_abc123â€,
  â€œpayloadâ€: {
    â€œreceiver_idâ€: â€œ456â€,
    â€œcontentâ€: â€œHey, are you free tomorrow?â€,
    â€œmedia_urlâ€: null,
    â€œreply_toâ€: null,
    â€œtimestampâ€: 1699189200000
  }
}

Server -> Client (ACK):
{
  â€œtypeâ€: â€œmessage_ackâ€,
  â€œclient_message_idâ€: â€œclient_abc123â€,
  â€œpayloadâ€: {
    â€œmessage_idâ€: â€œ20241105120000000001â€,
    â€œstatusâ€: â€œsentâ€,
    â€œtimestampâ€: 1699189200123
  }
}

Receiving a Message:

Server -> Client:
{
  â€œtypeâ€: â€œmessageâ€,
  â€œpayloadâ€: {
    â€œmessage_idâ€: â€œ20241105120000000001â€,
    â€œsender_idâ€: â€œ456â€,
    â€œreceiver_idâ€: â€œ123â€,
    â€œcontentâ€: â€œYes, what time works for you?â€,
    â€œmedia_urlâ€: null,
    â€œtimestampâ€: 1699189200123,
    â€œstatusâ€: â€œdeliveredâ€
  }
}

Client -> Server (Delivery ACK):
{
  â€œtypeâ€: â€œdelivery_ackâ€,
  â€œmessage_idâ€: â€œ20241105120000000001â€,
  â€œtimestampâ€: 1699189200456
}

Read Receipt:

Client -> Server (User opened chat):
{
  â€œtypeâ€: â€œread_receiptâ€,
  â€œmessage_idsâ€: [
    â€œ20241105120000000001â€,
    â€œ20241105120000000002â€,
    â€œ20241105120000000003â€
  ],
  â€œtimestampâ€: 1699189260000
}

Server -> Original Sender:
{
  â€œtypeâ€: â€œread_receiptâ€,
  â€œpayloadâ€: {
    â€œmessage_idsâ€: [â€20241105120000000001â€, ...],
    â€œread_byâ€: â€œ123â€,
    â€œtimestampâ€: 1699189260000
  }
}

Presence Heartbeat:

Client -> Server (every 5 seconds):
{
  â€œtypeâ€: â€œheartbeatâ€,
  â€œtimestampâ€: 1699189200000
}

Server -> Client:
{
  â€œtypeâ€: â€œheartbeat_ackâ€,
  â€œserver_timeâ€: 1699189200123
}

Media Upload Request:

Client -> Server (REST API):
POST /api/media/upload
{
  â€œfile_typeâ€: â€œimage/jpegâ€,
  â€œfile_sizeâ€: 2457600,
  â€œconversation_idâ€: â€œconv_123_456â€
}

Server -> Client:
{
  â€œupload_urlâ€: â€œhttps://s3.amazonaws.com/bucket/signed_url...â€,
  â€œmedia_idâ€: â€œmedia_xyz789â€,
  â€œexpires_inâ€: 3600
}

Group Message:

Client -> Server:
{
  â€œtypeâ€: â€œgroup_messageâ€,
  â€œclient_message_idâ€: â€œclient_def456â€,
  â€œpayloadâ€: {
    â€œgroup_idâ€: â€œgroup_789â€,
    â€œcontentâ€: â€œMeeting at 3 PMâ€,
    â€œmedia_urlâ€: null,
    â€œtimestampâ€: 1699189200000
  }
}

Server fans out to all members, each receives:
{
  â€œtypeâ€: â€œgroup_messageâ€,
  â€œpayloadâ€: {
    â€œmessage_idâ€: â€œ20241105120000000005â€,
    â€œgroup_idâ€: â€œgroup_789â€,
    â€œsender_idâ€: â€œ123â€,
    â€œcontentâ€: â€œMeeting at 3 PMâ€,
    â€œtimestampâ€: 1699189200123
  }
}

Ready for the best part?

The Online/Offline Control with Presence Server",https://newsletter.systemdesign.one
https://newsletter.systemdesign.one/p/distributed-systems,Distributed Systems â€“ A Deep Dive,#105: Understanding Distributed Systems,2026-01-02T16:55:20.054060,Neo Kim,"Get my system design playbook for FREE on newsletter signup:

Subscribe

Share this post

& I'll send you some rewards for the referrals.

Distributed systems arenâ€™t just a concept; theyâ€™re the invisible machinery holding our digital world together.

Every time you send a message, book a taxi, or stream a song, dozens of nodes across the world work together to make it look simple. But simplicity on the surface hides chaos underneath.

Weâ€™ll see how these systems serve millions globally while maintaining control over the chaos.

But first, letâ€™s understand why we need distributed systemsâ€¦

The AI Agent for production-grade codebases (sponsor)

Augment Codeâ€™s powerful AI coding agent and industry-leading context engine meet professional software developers exactly where they are, delivering production-grade features and deep context into even the largest and gnarliest codebases.

With

Augment Code

you can:

Index and navigate millions of lines of code

Get instant answers about any part of your codebase

Automate processes across your entire development stack

ğŸ‘‰

BUILD WITH AI AGENT THAT GETS YOU, YOUR TEAM, AND YOUR CODEBASE

I want to introduce

Sahil Sarwar

as a guest author.

Heâ€™s a software engineer at

Confluent

, passionate about distributed systems, system internals, and the philosophy behind designing large-scale systems.

Substack

LinkedIn

Connect with him if youâ€™re interested in deep dives into distributed systems, infrastructure, and the mindset behind technical design.

Why Distributed Systems?

Traditionally, software systems ran on a single machine.

But as the internet became popular, we realized a single machine might be insufficient at scale. Thatâ€™s how the idea of vertical and horizontal scaling came inâ€¦

1 Vertical Scaling

A simple way to scale is by adding:

more CPU,

more memory,

faster processors.

However, itâ€™s impossible to scale vertically forever because of hardware limitations & cost effectiveness.

2 Horizontal Scaling

Instead of relying on one powerful server,

â€œWhat if many small servers shared the work?â€

And those servers coordinate to act as one unit. Thatâ€™s the key idea behind a DISTRIBUTED SYSTEM.

What are Distributed Systems?

When you hear â€œdistributed systemsâ€, you might think of buzzwords like clusters, microservices, or Kubernetesâ€¦

But the core idea is actually simple:

A distributed system is a group of independent servers that work together and appear to the user as one system.

Each server has its own CPU, memory, and processes.

They communicate over a network to achieve one shared goal. Thereâ€™s no shared RAM, no shared global clock, and no single machine that â€œknows everything.â€

These features define distributed systems:

No Shared Memory

Each node works independently and canâ€™t directly read another machineâ€™s variables.

All communication happens through messages sent over the network.

We need idempotency

1

, retry logic, and consensus algorithms

2

to maintain system correctness even when messages are lost or fail.

No Global Clock

There is no universal time across machines.

Clocks drift, and network delays make timing unpredictable.

This makes it difficult to determine the exact order in which events occurred.

To solve this, we use

logical time

:

Lamport Clocks

If timestamp

a

is less than

b

, then event

a

happened before event

b

in logical order.

Vector Clocks

Extend Lamport clocks by tracking event counts from every node, allowing systems to detect concurrency and understand ordering across many processes.

Message Communication

Nodes communicate using protocols such as

gRPC

3

,

HTTP

4

,

Kafka events

5

, and so on.

Messages may not arrive, may arrive late, may be duplicated, or may even arrive out of order.

A distributed system stays correct not because communication is perfect, but because it handles failures gracefully.

Letâ€™s keep goingâ€¦

Distributed Systems in Real Life

Distributed systems arenâ€™t just theory - theyâ€™re everywhere, and in everything that we use:

Google Search

: a massive network of crawlers, indexers, and ranking nodes running across many data centers.

Netflix

: region-wide services for streaming, recommendations, authentication, video transcoding, and content delivery.

DynamoDB & Cassandra

: distributed keyâ€“value stores that replicate data across nodes for availability and scale.

Kafka

: distributed, fault-tolerant event log for asynchronous communication.

Stripe

: event-driven architectures built on reliable messaging and idempotent operations.

Distributed vs Decentralized vs Parallel Systems

These three terms sound similar, but they describe different designsâ€¦

1 Distributed Systems

A distributed system is a group of independent nodes that work together to present themselves as a single, logical system.

Examples:

Google Spanner

(globally distributed database)

Apache Kafka

(distributed log system)

There is often a â€œleader or a control planeâ€ responsible for managing consistency, replication, and communication between nodes.

2 Decentralized Systems

A decentralized system removes or minimizes the need for a single point of control.

Examples:

Bitcoin and Ethereum (blockchain networks)

BitTorrent (peer-to-peer file sharing)

Each node operates more autonomously, and coordination occurs through peer-to-peer consensus, not central orchestration.

3 Parallel Systems

They run multiple computations simultaneously, but on a single machine or a tightly coupled cluster with shared memory.

They typically have:

One global clock,

A shared memory space.

Examples:

GPU workloads

Multithreaded programs

High-performance computing clusters

TL;DR

Distributed systems: loosely coupled, message-passing, reliability-oriented

Decentralized systems: trustless, peer-based, no central authority

Parallel systems: tightly coupled, shared memory, performance-oriented

If many machines need to work together, they must communicate and share state efficiently.

The next question then is:

â€œHow do machines communicate effectively at scale?â€

Communication in Distributed Systems

When machines communicate in a distributed system, all interactions occur over a network. And networks come with their own set of messy realities.

Onward.

1 Network Realities

By default, the network is unreliableâ€¦ Messages can:

Be lost: never reach the destination

Be duplicated: delivered more than once

Be corrupted: bits flipped during transit

Arrive out of order: later messages come before earlier ones

Experience latency: unpredictable delays

Designing a distributed system is really about expecting the WORST and still making the system work.

As Murphyâ€™s law states:

â€œIf something can go wrong, it will go wrong.â€

2 TCP: Reliability Layer

Distributed systems rely heavily on TCP

6

because it provides:

Reliable delivery: data gets delivered reliably, or the sender is notified of failure

Ordering: packets arrive in the order they were sent

Error checking: corrupted packets are detected and retransmitted

TCP establishes this connection via a 3-way handshake:

SYN: Client says,

â€œI want to connect.â€

SYN-ACK: Server replies,

â€œGot it, ready to proceed.â€

ACK: Client confirms,

â€œGreat, letâ€™s begin.â€

3 TLS: Security Layer

You can consider TCP as a â€œmailmanâ€ and TLS

7

as the envelope with a lock and signature. TLS ensures:

Encryption

8

: nobody can read the data

Authentication: you know who youâ€™re talking to

Integrity: messages canâ€™t be altered without detection

During the TLS handshake, both sides:

Choose cipher suites

Agree on the TLS version

Verify the serverâ€™s TLS certificate

Generate session keys for encrypted communication

Reliability loses its meaning if someone can intercept or tamper with the data. In modern systems, every RPC and network call should use TLS.

If servers spread worldwide, how do they find each other?

4 DNS: Finding the Right Node

Thatâ€™s where DNS (Domain Name System) comes in:

It resolves human-readable names to IP addresses.

And acts as a basic service discovery

9

mechanism.

In large distributed systems, DNS is usually layered with service registries or load balancers. But at the core, you always need a way to translate a name into a network endpoint.

Communication alone is not enoughâ€¦ For a distributed system to function asÂ a single, coherent system, the nodes must also coordinate their actions.

And that brings us to the next major challenge: coordination.

Share this post & earn rewards for referrals.

Share

Coordination Challenges in Distributed Systems

Once machines start working together, they need to stay in sync. Yet coordinating many nodes creates new problems.

Letâ€™s look at the key challenges:

1 Failure Detection

Determining whether a machine has failed may seem simpleâ€¦ but itâ€™s not.

â€œIf a node doesnâ€™t respond, is it dead, or just slow?â€

Distributed systems usually detect failures using these two ways:

Heartbeat mechanism

Heartbeat messages are small signals that nodes send to each other to show they are still alive.

These messages serve as health checks, enabling each node to determine whether its peers are functioning properly or have failed.

Gossip protocol

Each node regularly sends a small â€œI am aliveâ€ message to others.

System considers the node dead if these messages stop for too long.

Like real gossip, nodes share what they know with others.

These techniques help to detect failures in a noisy, unreliable network.

But youâ€™ve got to find a balance between speed and accuracy:

Check often â†’ false positives (you think a node failed when it hasnâ€™t)

Check slowly â†’ system reacts late to real failures

If machines are spread around the world, it becomes hard to know which event happened first between two different servers.

So letâ€™s read the solution to this problem:

2 Event Ordering and Timing

If we use

time.Now()

to decide â€œwhich event happened when,â€ we run into issues:

Each machine has its own local clockâ€¦ and these clocks can drift

10

.

Because of this, it is almost impossible to keep all clocks perfectly in sync across many machines.

In distributed systems, we rarely care about the exact timestamp. What we really need to know is which event happened first.

To solve this, we use algorithms called â€œlogical clocksâ€.

Lamport Clocks

Lamport clocks use a simple counter on each machine:

Each process starts with a counter at 0

For every local event, increase the counter:

LC = LC + 1

When sending a message, attach the current counter value

When receiving a message, update the counter to:

LC = max(local LC, received LC) + 1

Example:

Consider two machines - M1 and M2.

M1 sends a message with

LC = 5

M2 receives it when its own clock is

LC = 3

So M2 updates its clock to

max(3, 5) + 1 = 6

From the above interactions:

Send event has LC = 5

Receive event has LC = 6

This keeps the correct order

11

: send < receive.

It tells you what happened before whatâ€¦ but not if two events were independentâ€¦ so it cannot detect concurrency.

Letâ€™s understand why:

M1 has

LC = 5

and does event A â†’

LC(A) = 6

M2 has

LC = 2

and does event B â†’

LC(B) = 3

These two events are independent; there is no communication between the machines.

But Lamport clocks say

3 < 6

, so it looks like B happened before A, even though they were concurrent. So Lamport clocks only show ordering, not concurrency.

Vector clocks fix this problem!

Vector Clocks

They extend the idea on which Lamportâ€™s clocks are based:

â€œWhat if every machine knows every other machineâ€™s order?â€

If there are N machines, the vector keeps N entries, and each entry tracks that machineâ€™s event count, M[i].

This means:

Every event carries a snapshot of what that machine knows about othersâ€™ state.

When a machine receives a message, it merges the knowledge using element-wise maximum.

This gives each machine a partial history of the entire system.

With vector clocks, a system can tell:

Which event happened first,

Whether two events happened at the same time (concurrently).

In a distributed system, we need a single source of truth for all operationsâ€¦ so it helps to have one machine act as the leader.

Letâ€™s dive in!

3 Leader Election

The leader becomes the source of truth, and the other machines (followers) replicate its state.

But if all servers share the same design, how can we determine which server qualifies to become the â€œleaderâ€?

Algorithms like Raft

12

solve this in a clean and predictable way:

All nodes start as followers.

If no leader is found, the followers enter a candidate state to become the leader.

Voting occurs, and the node that gets the most votes becomes the leader.

Once elected, the leader coordinates updates, and followers replicate its log.

Raft is popular because itâ€™s simple, has well-defined states (follower, candidate, leader), and provides strong safety guarantees. Itâ€™s used in many distributed databases and consensus systems.

4 Data Replication and Consistency

Maintaining data consistency across many machines is one of the most challenging problems in distributed systems.

Because data is stored on different nodes, every write must be replicated to other nodes to ensure consistency.

But not all systems need the same level of strictnessâ€¦ different consistency models exist depending on what the application needs:

Linearizability

Every operation looks instant and globally ordered.

Readers always see the latest write.

This is the strictest model and is expensive at scale because every operation requires coordination across machines.

Plus, it often adds latency.

Sequential Consistency

Each nodeâ€™s operations appear in order, but different nodes may see different global orders.

It's easier to achieve than linearizability.

Writes can be done locally first and then replicated asynchronously to others.

Eventual Consistency

If no new writes occur, all replicas will eventually converge on the same value.

Used in high-availability systems like DynamoDB and Cassandra.

You trade immediate correctness for higher availability and better partition tolerance.

CAP Theorem

These consistency choices connect directly to the CAP theorem, which says a distributed system can only provide two out of three at the same time:

Consistency (C): Every read returns the latest write (or an error).

Availability (A): Every request gets a response (even if itâ€™s outdated).

Partition Tolerance (P): The system keeps working even if the network breaks into parts.

So why not all three?

Network partitions (P) are common in distributed systems. When they occur, the system must choose between:

Consistency (C): Stop serving outdated data until the partition heals.

Availability (A): Serve whatever data is available, even if outdated.

Thus, a system can be CP, AP, or CA, but never all three at the same time. (There is always a tradeoff.)

Next, we look at scalability techniquesâ€¦ the reason distributed systems exist in the first place: to handle more load than a single machine ever could.

Get my system design playbook for FREE on newsletter signup:

Subscribe

Scalability Techniques in Distributed Systems",https://newsletter.systemdesign.one
https://newsletter.systemdesign.one/p/disruptor-pattern,How Stock Exchange Processes 6 Million Events per Second with Microsecond Latency,#104: Stock Exchange System Design - Part 2,2026-01-02T16:55:23.739804,Neo Kim,"Share this post

& Iâ€™ll send you some rewards for the referrals.

I created block diagrams for this newsletter using

Eraser

.

This month, I launched

Design, Build, Scale

â€¦ the newsletter series that will elevate your software engineering career.

This is Part 2 of how a stock exchange works.

Letâ€™s start with a

TL;DR of

Part 1

!

Three key components of an exchange are:

Broker - allows people to buy and sell stocks on the exchange.

Gateway - entry point for brokers to send BUY or SELL orders to the exchange.

Matching Engine - a component that matches BUY and SELL orders to create trades.

A stock market reduces latency by avoiding unnecessary services in the critical path.

BUTâ€¦ threads have to pass events efficiently to achieve â€œultra-lowâ€ latency at scale.

Examples:

New order event,

Risk check event,

Cancel order event,

Trade execution event,

Market data update event,

Replication or Journal event.

So the

concurrency model

matters!

Onward.

EverMemOSâ€”The Next-Generation AI Memory System (Sponsor)

Most AI agents forget everything after a sessionâ€”making them inconsistent, hard to debug, and impossible to scale.

Inspired by human brain memory mechanisms,

EverMemOS

provides an open-source memory OS that supports 1-on-1 conversation scenarios and complex multi-agent workflows. As reported, EverMemOS achieved 92.4% on LoCoMo and 82% on LongMemEval-S, both SOTA results of the two benchmarks.

Later this year, EverMind will launch the cloud service version, offering enterprise users advanced technical support, persistent storage, and scalable infrastructure.

If youâ€™re building agentic apps, EverMemOS gives you the memory layer youâ€™ve been missing.

ğŸ‘‰ Explore EverMemOS

Most system design goo-roos think using locks, or queues, or message brokers is enough.

Theyâ€™re WRONGâ€¦

Hereâ€™s why:

1 Locking

There are two key locking models:

Pessimistic,

Optimistic.

Pessimistic locking

A thread acquires a lock before touching shared data. So other threads must wait until the lock gets released. This means only one thread can update the shared data at a time.

But on a stock exchange:

Many threads try to read or update the same data at scale.

And locks force them to wait in line, which causes latency spikes.

More threads â†’ more blocking â†’ much SLOWER.

Even a single lock can slow down a thread by 10x, and with many threads, performance could drop 100x.

Besides, thereâ€™s also a risk of deadlock

1

if two threads wait on each other.

So this approach wouldnâ€™t work for a stock exchange.

Optimistic locking

It doesnâ€™t use traditional locks; instead, it uses compare-and-swap (

CAS

) to detect conflicts.

Hereâ€™s how:

A thread reads shared data + its version.

Then it writes new data only if the original version hasnâ€™t changed in the meantime.

If another thread updates it first, the write fails, and the thread has to RETRY.

This technique works well when conflicts are rare,,, but on a stock exchange:

Many threads try to change hot data (sequence counters, and so on).

Thus CAS failures become more frequent, and spin + retry loops waste CPU.

So performance weakens at scale.

2 Traditional Queues (ArrayBlockingQueue)

A Queue has:

Head (where consumers read from)

Tail (where producers write to)

Yet when there are many threads at once:

Consumers compete to update the head,

Producers compete to update the tail.

This creates contention and forces locking or other synchronization techniquesâ€¦ which slows everything down.

Plus, head, tail, and size variables are often on the same CPU cache line

2

.

NOTE: A CPU reads and writes memory in chunks (usually 64 bytes) called

cache lines

, not single variables.

So thereâ€™s a risk of

false sharing

3

when:

Two threads update different variables

And those variables live on the same cache line

Even though the threads arenâ€™t sharing data (i.e., no locks), the hardware thinks theyâ€™re, causing:

massive slowdowns

constant cache invalidations

This is a performance problem for traditional queues under high concurrency.

Also, a queue is often:

Full (because producers are fast)

Or empty (because consumers are fast)

So producers/consumers constantly fight over the same memory region. And this causes contention.

Besides, in some implementations, reads may require writes when consumed. This also causes contention.

3 Message Brokers

Message brokers like Kafka and RabbitMQ are built for general messaging... NOT for microsecond latency.

They introduce extra overhead because of:

serialization,

buffering,

network hops.

Plus, their queues use locks and background threads, which add jitter.

So theyâ€™re too slow for a stock exchange!

Then how do you achieve â€œultra-lowâ€ latency at SCALE?

Reminder: this is a teaser of the subscriber-only newsletter series, exclusive to my golden members.

When you upgrade, youâ€™ll get:

High-level architecture of real-world systems.

Deep dive into how popular real-world systems actually work.

How real-world systems handle scale, reliability, and performance.

Subscribe",https://newsletter.systemdesign.one
https://newsletter.systemdesign.one/p/system-design-interview-twitter,System Design Interview: Design Twitter/X Timeline - A Frontend Deep Dive,#103: System Design behind Twitter/X Timeline Frontend,2026-01-02T16:55:25.376157,Neo Kim,"Get my system design playbook for FREE on newsletter signup:

Subscribe

Share this post

& Iâ€™ll send you some rewards for the referrals.

Traditional system design interviews tend to focus on backend architecture:

API servers, databases, caching layers, load balancers, and service-oriented designs.

â€œFront-endâ€ system design, by contrast, deals with the problems of scale and complexity that manifest in the browser or client environment.

Instead of optimizing for query latency or data replication, the focus shifts to rendering efficiency, client structure, network utilization, and managing data consistency between the client and server.

System design interviews matter because they reveal how well an engineer can think beyond individual components and consider the entire user experience as a system. Front-end system design interviews are a relatively new but increasingly important part of the hiring process for front-end engineers. They test not only oneâ€™s technical ability to build user interfaces but also the architectural thinking required to scale those interfaces to millions of users.

For companies with complex, dynamic interfaces such as Twitter/X, Netflix, Airbnb, Google Docs, and Figma, the frontend client is no longer a thin layer that merely renders data. It is a distributed system in its own right, managing state, orchestrating data fetching, caching results, handling concurrency, and maintaining real-time synchronization with the backend.

Designing this layer effectively can have a dramatic impact on user engagement and perceived performance, ultimately driving business goals.

Front-end system design is therefore not merely about choosing frameworks or UI libraries. It is about building systems that can evolve, scale, and deliver consistent performance as features grow and teams expand. Understanding this distinction is essential before diving into the architecture of complex interfaces like the Twitter/X timeline, where milliseconds of latency and subtle interaction patterns can define the entire product experience.

Onward.

Give Your AI Tools the Context They Need So They Stop Guessing (Sponsor)

Your AI tools are only as good as the context they have.

Unblocked

pieces together knowledge from your teamâ€™s GitHub, Slack, Confluence, and Jira, so your AI tools generate production-ready code.

See How

I want to introduce

Yangshun Tay

as a guest author.

Yangshun is the creator of

GreatFrontEnd

, a platform for frontend engineers to prepare for front-end interviews.

Check out his website and social media:

GreatFrontEnd

LinkedIn

Twitter

GitHub

He was previously a Staff Engineer at Meta. Also, he created Docusaurus 2, Blind 75, and the

Tech Interview Handbook

.

P.S.

GreatFrontEnd

is currently running a Black Friday sale

, their biggest sale of the year. Donâ€™t miss out on this opportunity!

Types of questions in front-end system design interviews

Front-end system design interviews fall into two categories:

â€œapplication design and component design.â€

While both draw on the same foundation of architectural thinking, they operate at different levels of abstraction and test different problem-solving instincts.

The

application-level design

focuses on how to structure and architect an entire product or large feature. Candidates might be asked to design something like the Twitter timeline, a media streaming website, or a collaborative document editor.

These questions assess a candidateâ€™s ability to design systems that can handle complex data flows, synchronize with back-end services, manage client-side state, and maintain consistent performance as the application scales.

Strong answers demonstrate an understanding of how to divide an application into layers, manage global versus local state, handle rendering efficiently, and make trade-offs between client-side and server-side responsibilities.

Each kind of app brings unique challenges that candidates should spend most of their time discussing:

Social media (e.g., Facebook, Twitter)

: Timeline API, pagination, rendering posts in various formats, performance

E-commerce & travel booking (e.g., Amazon, Airbnb)

: SEO, performance, forms, localization

Media streaming (e.g., Netflix, YouTube)

: Media streaming protocols, media player implementations

Collaborative apps (e.g., Google Docs, Google Sheets)

: Real-time collaboration models, conflict resolution approaches

The

component-level design

narrows the focus to building a single, self-contained piece of UI that appears simple on the surface but hides significant complexity beneath. Common examples include designing an autocomplete input, image carousels, or a modal dialog.

These components must handle accessibility, keyboard navigation, focus management, animations, and edge cases such as async data fetching and nested interactions.

Interviewers use these problems to evaluate a candidateâ€™s depth of understanding in UI behavior, browser APIs, rendering models, and user experience details.

Although these two categories differ in scope, they complement each otherâ€¦

Application-level design tests a candidateâ€™s ability to think in systems and to understand how data and state flow through a complex interface.

Component-level design tests a candidateâ€™s ability to execute on the smallest building blocks of those systems with precision and craft.

Together, they form a complete picture of what it means to design and engineer modern front-end applications at scale.

Framework for approaching front-end system design interviews

Many engineers struggle with system design interviews because they approach problems reactively, jumping straight into technical details before establishing a clear structure, or simply just hopping around topics with no clear flow, and risk ending up not sufficiently covering the important areas of the question.

I know it all too well because that was how I failed my first few system design interviews.

Therefore, I came up with an easy-to-remember mnemonic,

RADIO,

to help me remember how to approach system design questions.

The

RADIO framework

offers a systematic way to think through design questions methodically. It ensures you start with clarity, communicate your reasoning, and cover both breadth and depth.

RADIO

stands for:

Requirements

,

Architecture

,

Data model

,

Interface

,

Optimizations

.

Letâ€™s dive in!

Requirements

Begin by clarifying what problem you are solving and what success looks like.

Identify both functional and non-functional requirements, what the system must do, and how well it must perform. This is also where you scope the problem to fit the interview.

For example, when asked to design the Twitter timeline, you might clarify whether the focus is on rendering the feed, handling infinite scroll, or supporting real-time updates.

Architecture

Once the goals are clear, outline the high-level structure of the system and describe its responsibilities.

In front-end design interviews, itâ€™s common to have the layers for data fetching, state management, and the view (what users see and interact with). You should explain how data flows through these layers, where caching happens, and how user actions propagate changes.

The goal is to show that you can decompose a complex UI into well-defined subsystems that interact cohesively.

Data model

Describe the main data entities and state in the system.

With Twitter, that would be the feed, tweets, composer message, etc., and how they relate to one another. A solid understanding of data modeling shows you can design efficient structures for rendering what the user needs to see and interact with.

Interface (API)

Define how different parts of the system communicate.

This includes both internal interfaces, such as component boundaries and state management APIs, and external interfaces, such as REST or GraphQL endpoints. Each API should consist of the intention, parameters, and response shape.

Optimizations

After establishing a functional design, discuss the aspects of the system that are unique or complex and how you would improve it for scale and performance.

This can include performance techniques like DOM virtualization

1

, lazy loading

2

, media optimizations, and more.

RADIO Framework

Use the

RADIO framework

as a guide to ensure a comprehensive answerâ€¦ not strict linear steps.

Lead the discussion with requirements, then be flexible to dive wherever the conversation or technical complexity requires.

Architecture, Data model, and the Interface (API) are usually discussed as a whole and not strictly in order.

Some candidates prefer to design the data model first before the Architecture and API, which might make more sense depending on the question.

Lastly, end with optimizations and deep dives, but revisit any of the earlier areas if youâ€™re discussing new requirements or extending to new features.

You can read more about the

RADIO framework on GreatFrontEnd

.

Design Twitter/X Timelineâ€™s frontend

Letâ€™s apply the RADIO framework to a common front-end system design question:

â€œDesign Twitter/X Timeline.â€

By the end of this newsletter, we will have a solid front-end design for implementing a Twitter timeline, including specific optimizations for each part.

From here on, weâ€™ll refer to the product as â€œTwitterâ€ rather than â€œXâ€, since â€œTwitterâ€ is more readable and recognizable.

Requirements

Letâ€™s define some core functional requirements of the Twitter timeline:

Users can browse a timeline UI containing a list of tweets

Timeline contains text and image-based tweets

Users can perform actions on tweets, such as liking and retweeting

Users can click on a tweet to view the replies

When users scroll to the bottom of the timeline, more tweets load

Users can create new tweets

In reality, Twitterâ€™s timeline supports more formats, such as videos, polls, and other actions, like replying to tweets.

But letâ€™s first focus on the core functionality: consuming timeline tweets and creating new tweets.

Besides functional requirements, we can also discuss how to improve non-functional aspects such as performance, user experience, and accessibility through optimizations and deep dives.

Architecture

A well-structured front-end architecture separates concerns across distinct layers, each responsible for a specific set of tasks.

Before we discuss the layers involved, we have two decisions to make:

(1) where to render the webpage (client vs. server vs. hybrid),

(2) how navigations occur (the traditional way of full-page reloads vs. client-side).

Rendering approach

A large-scale application like Twitter can be built using several rendering approaches, each offering different trade-offs among performance, complexity, and infrastructure costs.

The main models are:

server-side rendering (SSR),

client-side rendering (CSR),

hybrid model.

Server-side rendering (SSR)

SSR generates the full HTML for a page on the server before sending it to the client.

This leads to faster first-paint times and better SEO because users and crawlers receive meaningful content immediately. Once the HTML is loaded, the client â€œhydratesâ€ the page to attach event listeners and to enable interactivity.

Modern websites rely heavily on SSR to ensure quick perceived performance.

Client-side rendering (CSR)

In this model, the server first delivers a minimal HTML shell and a JavaScript bundle.

The browser then executes the JavaScript to fetch data and render the UI. CSR offers great interactivity once loaded, since subsequent updates can happen entirely on the client without page reloads.

However, it has slower first load performance and weaker SEO, since users initially see a blank page/spinner until the data is fetched.

Hybrid rendering

Hybrid architectures combine SSR and CSR to get the best of both worlds.

The server renders the initial page load for speed and SEO, and the client handles dynamic updates. This is often implemented using frameworks like Next.js and Remix. Twitter could render the first batch of tweets server-side and then fetch and render new tweets client-side as the user scrolls.

Since timelines are highly personalized, rendering timelines on the server primarily benefits performance instead of SEO. In reality, Twitter likely uses CSR because SSR requires more infrastructure resources, and fetching a timeline on Twitter is already pretty quick (takes less than 1s); the benefits SSR brings donâ€™t outweigh the complexity required.

In practice, frameworks like

Next.js

,

React Router

, and

Tanstack Start

help you implement these various rendering patterns in your app, even allowing for custom rendering per page.

Navigation approach

Website navigation follows either a single-page application or a multi-page application approachâ€¦ and the choice has direct implications for user experience and performance.

A

single-page application (SPA)

is a web app that loads a single HTML page and dynamically updates its content as the user interacts with the app, without requiring a full page reload. This works by using JavaScript to modify the page URL, fetch data from the server, and update the DOM. This results in fast, seamless navigation and an app-like experience.

In

multi-page applications (MPA)

, each route corresponds to a separate HTML page. Navigating between pages triggers a full-page reload, which can simplify server-side rendering and improve SEO. Content-heavy sites typically use MPAs, but they can feel slower and less fluid for highly interactive features.

For a social media app like Twitter, SPAs are preferred, and SPAs rely on CSR.

More importantly, pages in SPAs can benefit from data in a shared client store. Most users access a tweet from the timeline. In an SPA, the key details of the tweet (text, media) are already loaded on the page and stored in the store; navigation to the tweet details page is instant and requires no server-side interaction. Additional data, such as replies, is fetched after navigation occurs.

On the other hand, in an MPA, navigation blows away the current page state. Hence, users will experience longer delays when navigating between pages, as they have to wait for the server to finish generating the HTML.

Architecture layers

With rendering and navigation decided, we can define the layers of the Twitter front end, which can be broken down into four main layers:

View

,

Store

,

Data access

, and the

Server

.

View

The view layer is what users interact with directly.

It includes the timeline page, tweet detail page, and a tweet composer component. Itâ€™s responsible for rendering data from the store and triggering user actions, such as liking a tweet or composing a new one. Its main goal is to provide a fast, interactive, and accessible user experience while remaining declarative and predictable in how it reacts to data changes.

In practice, these are your JavaScript frameworks/libraries, such as

React

,

Vue

,

Svelte

, etc.

Store layer

The store acts as the source of truth for client-side data and state.

It holds all application state in memory: the timeline, tweets, users, composer message, etc. And ensures consistency across different parts of the interface.

When a user performs an action, such as liking a tweet, the store immediately updates the local state (often optimistically) before synchronizing with the server. It also caches data for quick retrieval, normalizes

3

entities to prevent duplication, and triggers re-renders in the view layer whenever data changes. The store decouples the UI from the backend, keeping the app responsive even during network delays.

In an SPA, this layer is initialized once and then persisted + updated throughout the session.

In practice, these are your state management libraries, such as

Redux

,

Zustand

,

Jotai

, etc.

Data access

The data access layer abstracts away the communication with the backend APIs.

It handles network requests, response parsing, and caching policies, and defines how data moves between the server and the store. This layer may also include retry logic, pagination handling, and transformations that convert raw API responses into normalized store-friendly structures. By centralizing all network operations here, the front end gains flexibility to change APIs, add batching, or introduce service workers without affecting the rest of the system.

In an SPA, this layer is also initialized once and persisted throughout the session, just like the store layer.

In practice, these are data-fetching libraries such as

RTK Query

,

Tanstack Query

,

Relay

,

and Apollo Client

.

Server

The server exposes HTTP endpoints for fetching timelines, posting tweets, and performing engagement actions such as likes or retweets.

While the server defines the data model and business rules, it relies on the front end to manage presentation logic, caching, and responsiveness. This layered architecture creates a clean separation of responsibilities. The view focuses on presentation; the store manages state; the data access layer handles communication; and the server provides data.

Since itâ€™s an SPA, the store and data access layers are initialized on the first load, persisted throughout the session across navigations, and continuously updated as requests and user interactions occur.

Interface (API design)

In the architecture layers diagram, several data entities are passed around.

All of them either originate from the server or are being sent to the server. Hence, we can focus on the server APIs.

Most of these APIs require authentication as theyâ€™re personalized or specific to the user. Itâ€™s important to note that the userâ€™s ID shouldnâ€™t be included in the request and shouldn't be relied on as the source of truth for identity, as it can be spoofed and create a security vulnerability. The userâ€™s identity should be derived from session cookies sent with the request.

Weâ€™ll assume the user is logged in and omit discussion of authentication and authorization mechanisms, since the focus is on the Twitter product.

1. Get Twitter timeline

GET /timeline?count=10&cursor=abc

Returns a list of tweets. Pagination parameters are included so that clients know how to fetch the next page of tweets.

Weâ€™ll look at pagination in more detail in the optimizations section.

2. Get a single tweet

GET /tweets/{id}

Returns a single tweet. Depending on the desired experience, replies can be fetched using another API (e.g.,

/tweets/{id}/replies

) or included in the tweet payload.

This API is available to the public, but if a session cookie is included in the request, the results can include personalized fields such as whether the user has liked the tweet.

3. Post a tweet

POST /tweets

Create a new tweet. It accepts the following parameters:

When creating tweets with attached media, the media is first uploaded to blob storage, and an ID is returned to include in the POST request payload. Therefore, we also need an API for uploading media.

This API creates a tweet for a user; hence, it requires authentication.

4. Upload media

POST /media/upload

Accepts binary data, uploads into blob storage, and returns a media object ID that can be included when creating a tweet. An alternative is for the API to provide a pre-signed URL that the client can use to upload directly to blob storage.

5. Tweet actions

POST /tweets/{id}/like

POST /tweets/{id}/retweet

Common actions that can be taken on tweets. There are more, but these are the two main ones.

Data model/entities

Next, letâ€™s look at the core entities that should be modelled in a way that is both efficient for rendering and resilient to frequent updates.

Similar to backend data models, which focus on efficient storage and relationships, we can model front-end data to prioritize ease of access, normalization, and caching.

Tweet entity

At the center of the timeline is the Tweet entity.

Each tweet contains a rich set of information:

text content,

author details,

timestamps,

engagement counts,

and possibly media attachments.

On the front end, data is often stored in a normalized form rather than as nested objects.

This means separating the author data and media into distinct entities, each referenced by an ID. This approach makes updates more efficient. For example, when a userâ€™s avatar or display name changes, every tweet referencing that user can automatically reflect the update without having redundant data.

User entity

The User entity represents the authors and participants in the timeline.

It includes metadata such as:

name,

username,

profile picture,

verification status,

and relationship flags (for example, whether the current user follows them).

Because user data is shared across many tweets and UI surfaces, itâ€™s often cached at the global application level to avoid repeated network requests and duplicated state.

Timeline entity

The Timeline entity itself represents the ordered feed of tweets.

It can be thought of as a list of tweet IDs, annotated with pagination information for fetching older or newer tweets.

Timelines are dynamic; as the user scrolls down, older tweets are fetched and appended to the list, and newer tweets can also be added to the top when the timeline is stale.

Normalized store structure

These entities live in the client store following a normalized structure.

Data normalization within client-side stores is the process of structuring data so that each unique entity is stored exactly once, with relationships between entities represented by references rather than nested copies. This concept mirrors relational database design, but applied in the context of a browser application.

On Twitter, a tweet might include a nested user object for its author, another for a retweet, and yet another for a reply. If these user objects were duplicated across many tweets, updating a single piece of user information (e.g., a display name or avatar) would require updating every instance across the store.

Normalization prevents this by separating users, tweets, and other entities into their own collections, using their IDs as keys. Entities then store references to other entities via IDs rather than embedding full user data.

This structure makes updates and caching far more efficient. If a userâ€™s profile changes, only the corresponding user entry needs to be updated, and every component referencing that user will automatically reflect the change.

Normalized data also simplifies pagination, deduplication, and cache merging when new results arrive from the server. Most importantly, it keeps the client-side store lean and organized as the app grows in complexity, ensuring data consistency across views and interactions.

NOTE:

Data fetched from the server

doesnâ€™t need to be in the finalized, normalized form

.

Normalization can be done on the server or the client, depending on where the processing should occur. If a product serves markets where user devices arenâ€™t powerfulâ€¦ it might be better, performance-wise, to have the server handle normalization.

As of 2025, Twitter normalizes timeline data on the server and sends a compact payload to clients.

Optimizations and deep dive

With the core pieces of the Twitter front-end system laid out, letâ€™s look at how the app can be optimized:

Weâ€™ll look at general optimizations across the board and zoom in on specific optimizations for each part of the app.

User experience

Loading states

are the first touchpoint in this process.

When users open the app or navigate between views, the UI should always provide a clear signal that content is being retrieved, with spinners.

Twitter uses a single loading spinner, but a better approach would be to use skeleton placeholdersâ€”gray blocks resembling tweet shapes, rather than traditional spinners.

Skeleton screens preserve layout stability and make loading feel shorter because the visual structure is already in place. As data arrives, the placeholders seamlessly transition into actual content.

Error handling

requires a similarly thoughtful approach.

When something goes wrong, the interface must communicate the issue clearly without breaking immersion. For transient failures, such as a dropped network connection, the timeline can display an inline error bar or toast notification, something non-blocking that encourages the user to retry.

More severe cases, such as an empty feed because of an API failure, call for a fallback UI that explains what happened and provides recovery options.

Twitter shows a â€œRetryâ€ button alongside a short message instead of a blank page, maintaining context while guiding the user back to normalcy.

Performance optimizations

Performance is one of the most critical dimensions of front-end system design, especially for an application as dynamic and content-heavy as the Twitter timeline.

Letâ€™s look at some performance techniques:

Reducing initial page load size through code splitting

Code splitting

4

is a key performance optimization technique that allows web applications to load only the JavaScript necessary for the current view, rather than delivering the entire bundle upfront.

Twitterâ€™s features, such as timelines, profiles, and notifications, each have distinct dependencies; sending all the code at once would significantly delay the initial load.

By splitting the code into smaller, route- or feature-specific chunks, the app reduces its initial JavaScript bundle size, improving time-to-first-render and time-to-interactive.

In Twitter, code splitting is used in the following scenarios:

Relevance

: Lazily loading only the JavaScript code for the tweet formats shown in the timeline. There are many types of tweets: text, image, video, ads, etc., and each has a different appearance and interactions. Only the code needed for the tweet formats in the timeline is downloaded.

Interaction

: Lazily loading JavaScript code for components that are only shown upon user interaction, e.g., Emojipicker when clicking on the emoji button in the composer, auto-completion of hashtags, and profile card that only appears when hovering over a tag/mention.

Navigational

: Other pages, such as the tweet detail page, can be loaded lazily when a user clicks a tweet, rather than bundled with the timeline code.

This improves performance for the average user, who might never visit certain routes or perform certain actions. Combined with pre-fetching (loading likely future chunks in the background), code splitting provides a balance between speed and flexibility.

Modern build tools like Webpack, Rollup, and Vite support dynamic imports, which make code splitting straightforward.

Optimizing rendering performance through list virtualization

The timeline consists of a long list of tweet components, each with rich media and nested interactions.

Rendering every tweet at once would overwhelm the DOM, so Twitter relies on

virtualized lists

that render only the visible items in the viewport, using invisible

<div>

s so that the scrollbars are still present without actual content.

As users scroll, off-screen elements are recycled or unmounted to minimize memory usage. This approach keeps frame rates high and prevents layout thrashing.

Improving perceived performance through optimistic updates

Optimistic updates work by immediately reflecting a userâ€™s action in the interface before the server confirms the change, creating the illusion of zero latency.

For actions such as liking, retweeting, or following, this technique ensures that the interface reacts instantly, maintaining the sense of momentum that defines a smooth social media experience.

When a user likes a tweet, for example, the heart icon fills in and the like count increments right away. Behind the scenes, the client simultaneously sends a request to the server to record the action. If the server responds successfully, nothing more needs to happen, since the UI has already been updated. But if the request fails, the client rolls back to the previous state. This approach hides network latency from the user, making interactions feel immediate even on slow or unreliable connections.

From an engineering perspective, implementing optimistic updates requires careful state management.

The client must maintain a consistent local representation of the data and reconcile it with the serverâ€™s eventual response. Using a client store, this is made easier as data is shared across many views (e.g., the same tweet appearing on multiple pages/placements), the store tracks a tweetâ€™s state in a normalized fashion and has to update a Tweetâ€™s

likedCount

value by

1

and

likedByUser

to

true

; there are no duplicated instances of tweets to search through and update.

Efficient media rendering

Every tweet can contain images, GIFs, or videos, and these assets are often the largest contributors to load time and memory usage.

Hence, itâ€™s important to optimize how media is loaded, displayed, and recycled to maintain both perceived performance and smooth scrolling.

The first principle is

lazy loading

.

Media should never be fetched or decoded until itâ€™s near the userâ€™s viewport. This minimizes initial bandwidth consumption and prevents layout shifts during scrolling. Modern browsers support the

loading=â€lazyâ€

attribute for images, but more advanced control can be achieved by using the

IntersectionObserver

5

API, which allows the app to load assets slightly before they appear on screen.

For videos and GIFs, loading low-resolution thumbnails or poster frames first gives the illusion of instant availability while deferring full playback until user interaction.

Next comes

media optimization and sizing

.

Twitterâ€™s front-end requests media in multiple resolutions and serves the appropriate variant based on device type, screen density, and layout size. This is typically achieved using the

<img srcset>

and

<picture>

elements or equivalent client-side logic. On high-density displays, high-resolution assets are used selectively to maintain visual sharpness without overfetching.

For videos, adaptive bitrate streaming ensures smooth playback even under varying network conditions, seamlessly switching between quality levels.

Finally,

perceived performance

plays a significant role.

Progressive image decoding and blurred low-resolution placeholders (often called LQIP

6

or â€œblurhashâ€) give users an instant visual cue that content is loading, reducing the sense of delay. Combined with skeleton loaders and smooth transitions, the feed feels continuously alive, even when data and media are still being fetched.

Timeline pagination approaches

Two common approaches to pagination in modern web applications are:

offset-based pagination,

cursor-based pagination.

Both are mechanisms for fetching data in chunks, but they differ in how they identify the next set of results and how well they handle dynamic, real-time data.

Offset-based pagination

It relies on numerical offsets to determine which results to fetch next.

For example, a request might specify

?offset=20&limit=10

to get the next ten tweets after the first twenty.

Offset-based pagination is straightforward and easy to implement, but it becomes inefficient and unreliable as the dataset grows or changes frequently. When new tweets are inserted into the feed, offsets can shift, leading to duplicate or missing items. It also performs poorly for large offsets

7

, as databases must skip an increasing number of records to reach the desired position.

Cursor-based pagination

Cursor-based pagination, on the other hand, uses a unique identifier (often a tweet ID or timestamp) as a cursor to mark the boundary between pages.

Instead of asking for the â€œnext 10 results after offset 20,â€ the client requests â€œthe next 10 results after tweet ID X.â€

Cursor-based pagination is more stable and efficient because it doesnâ€™t depend on the datasetâ€™s size or ordering at query time. It works well in environments where data is frequently updated, such as Twitterâ€™s timeline, where new tweets appear constantly and older tweets can be deleted or re-ranked.

For Twitter,

cursor-based pagination is the rational choice

.

It aligns naturally with the timeline's chronological nature, avoids inconsistencies caused by real-time updates, and scales efficiently across millions of records. It also enables bidirectional navigation, allowing users to load newer tweets and scroll down (to fetch older ones) without re-fetching or skipping content.

By combining cursor-based pagination with techniques such as infinite scrolling and background prefetching, Twitter can maintain a seamless, continuous feed experience that feels instantaneous, even as large volumes of data are loaded behind the scenes.",https://newsletter.systemdesign.one
https://newsletter.systemdesign.one/p/cloud-system-design,A Systematic Breakdown of 15 Cloud Architecture Pitfalls,#102: An Analytical Look at Common Design Mistakes on the Cloud,2026-01-02T16:55:27.003036,Neo Kim,"Get my system design playbook for FREE on newsletter signup:

Subscribe

Share this post

& I'll send you some rewards for the referrals.

Cloud computing has changed the way we build and run systems:

It gives us managed services that improve reliability. But this flexibility also means small mistakes can spread quickly. One incorrect setting, a missing tag policy, or a bad scaling rule can affect many environments at once. These issues can quickly become expensive or risky.

Many teams run into â€œsimilarâ€ problemsâ€¦ not because theyâ€™re careless, but because cloud platforms behave differently from traditional on-premise systems.

This newsletter will walk you through 15 common pitfalls in cloud environments.

Each section contains:

The pitfall - quick example or pattern

Why it happens - what design mistake causes it

Architectural impact - which parts of the system it affect

How to prevent it - ways to avoid it

The goal is simple:

Design systems that adapt to change instead of breaking under itâ€¦ and build systems that stay reliable even when things go wrong.

Letâ€™s start!

Learn Python, SQL, and AI with 50% Off â€” Become a Certified Data Analyst or AI Engineer! (Sponsor)

My favorite

Black Friday deal

is here! DataCamp is offering

50% off

for 600+ hands-on courses, certifications, and career tracks.

Explore their top tracks to boost your career in 2026!

Data Scientist Track

Data Analyst Track

AI Engineer Track

Or learn some of the most in-demand skills:

Python

SQL

R

DataCamp makes learning practical with:

âœ… Hands-on coding

âœ… Guided projects

âœ… Industry-recognized certifications

ğŸ‘‰

Donâ€™t miss it -

50% off ends soon!

I want to introduce

Magdalena

as a guest author.

Sheâ€™s an infrastructure and cloud architect passionate about automation and security.

Check out her blog and social media:

Cloud Architecture Blog

Book â€“ Mind The Gap: Most Common Cloud Mistakes

LinkedIn

You can learn in depth about the most common cloud pitfalls and how to avoid them with â€œ

Mind the Gap: Most Common Cloud Mistakes

â€.

Youâ€™ll also get a 50% discount for the ebook on Gumroad when you use the codeâ€“

SYSTEMDESIGN

. (Valid until 6 December.)

1.

Orphaned Resources

Orphaned resources are cloud assets that are no longer in use but continue to run.

Examples:

unattached storage disks,

unused public IPs,

old snapshots.

They pile up over time, cost â€œmoneyâ€, and clutter dashboardsâ€¦ thus making it harder to track whatâ€™s important.

A typical example is running a proof-of-concept on a large, expensive virtual machine and forgetting to shut it down or delete it. The VMs keep running, and the bill keeps growing until someone notices it.

Why it happens

Cloud makes it easy to create new resources. But deleting them is usually a manual taskâ€”and people forget. As every resource has a cost, even tiny leftovers add up. If left untracked, nobody knows what can be safely removed.

Architectural impact

Orphaned resources make it hard to understand where money is going. They can also create security risks. For example:

A forgotten test VM might still hold credentials.

An old IP address might still be allowed in firewall rules.

A leftover snapshot might contain sensitive data.

How to prevent it

Use tags: owner, purpose, and expiry date.

Automate the cleanup of unused or expired resources.

Send regular cost reports grouped by tags and resource age.

Add cleanup to every projectâ€™s â€œdoneâ€ checklist.

Schedule automatic scans to find and remove old resources.

Forgotten resources quietly waste money. While configuration mistakes break systems quickly.

2. Misconfigurations

Misconfiguration is one of the most common causes of cloud incidents.

A single wrong setting or missing rule can affect reliability, security, or cost. And its impact often spreads long before anyone notices.

Examples:

Wrong autoscaling limits

Encryption not turned on for storage

A storage bucket accidentally left public

Copy-pasting templates without checking differences

When these patterns are automated, a single mistake amplifies across multiple accounts or regions.

Why it happens

Cloud services offer hundreds of settings: permissions, scaling rules, encryption options, network controls, and so on.

Teams often rely on defaults or copy settings between environments without reviewing them. When Infrastructure as Code

1

(

IaC

) templates contain errors, those errors propagate every time the template is used.

And fast release cycles make it worse; thereâ€™s rarely enough time for reviews.

Architectural impact

Misconfiguration can:

Expose sensitive services to the public

Destabilize performance

Disable encryption

Break scaling logic

Increase costs

Because cloud automation pushes changes at scale, a tiny error can impact an entire production environment within minutes.

How to prevent it

Keep configurations in version control; review and test every change.

Use policy-as-code tools (Terraform Validate, AWS Config, Azure Policy).

Detect configuration drift and automatically enforce compliance.

Keep clear configuration baselines for production and non-production systems.

Monitor continuously for anomalies or security alerts.

Strong configurations are helpful, but they only work if teams communicate effectively. The next challenge isnâ€™t in the codeâ€¦ itâ€™s in coordination.

3. Poor Communication Between Teams

Cloud systems rely on effective coordination among development, operations, security, networking, and finance teams.

If they donâ€™t share information clearly, assumptions drift, ownership becomes unclear, and issues surface late in production.

For example:

A networking team updates routing rules for compliance, but doesnâ€™t inform data engineers who depend on that network.

Pipelines fail the next day - not because of a bug, but because one team didnâ€™t know what another had changed.

Why it happens

Cloud projects involve many specialties:

Developers focus on features.

Operations focus on reliability.

Security focuses on compliance.

Each team uses different tools and has different priorities.

If teams donâ€™t share information clearly, critical details get lost. Teams may not know which workloads are important or how specific changes impact cost or performance.

Architectural impact

Poor communication causes:

Duplicate services

Inconsistent architecture

Pipelines that fail to integrate

Security exceptions approved without full context

Conflicting Identity and Access Management

2

(

IAM

) roles

During incidents, no one is sure who owns what, which slows down the response and recovery.

How to prevent it

Define clear ownership and contact points for each system or account.

Keep shared documentation and Architecture Decision Records

3

(

ADRs

).

Hold cross-team reviews for key infrastructure or cost changes.

Encourage a â€œyou build it, you run itâ€ mindset.

Use consistent tagging, naming, and shared communication channels (shared dashboards and so on).

Communication gaps often cause teams to take shortcuts, such as relying on a single tool to solve every problem. But that approach can backfire!

4. Believing One Tool Solves Everything

Many teams assume that a single cloud platform, monitoring tool, or automation system can handle every need.

It feels efficient at first:

One interface, one workflow, one place to learn.

But as systems grow, this approach limits flexibilityâ€¦ and creates more problems than it solves.

Example:

A team standardized on a single deployment tool that worked well for virtual machines (but didnâ€™t support containers).

And developers began writing custom scripts to fill the gaps, and environments drifted apart.

So tracking changes became difficult.

Why it happens

Cloud tools evolve fast, and many are marketed as â€œall-in-oneâ€ solutions.

Teams under pressure to simplify or reduce costs often choose a single tool for everything: observability

4

, CI/CD

5

, security, deployments, and so on.

At first, it seems easier to manage and train people on a single platform. But it becomes difficult over time:

The tool doesnâ€™t scale across many accounts.

It doesnâ€™t work well in hybrid or multi-cloud environments.

It lacks the features needed for new services.

What started as â€œsimplicityâ€ ends up as inflexibility.

Architectural impact

Relying on one tool can cause:

Hidden dependencies

Poor multi-cloud or hybrid support

Large parts of the system break if the tool fails

Missing metrics, especially for newer workloads

Slow innovation, as teams wait for features that may never arrive

How to prevent it

Choose tools based on fit, not just standardization.

Review tool usage regularly as your architecture evolves.

Design for interoperability

6

using APIs and modular pipelines

7

.

Limit the number of tools you use. Also, group tools by purpose (monitoring, IaC, deployment). Plus, assign clear ownership.

Relying just on one tool is risky. However, a deeper issue arises when thereâ€™s a lack of understanding about how the cloud actually works.

Share this post & get rewards for the referrals.

Share

5. Weak Understanding of Cloud Mechanics

Many cloud problems come from a lack of understanding of how the cloud actually works: from billing and scaling to data movement and networking.

Without this knowledge, even well-designed systems can become expensive or unreliable.

Example:

A team built a data analytics job that copied large datasets between regions daily.

They assumed transfers were free, just like in their on-premise data center.

Their first monthâ€™s bill showed network charges higher than the compute costs.

Why it happens

Cloud platforms hide many infrastructure details.

Although this makes things easier, it doesnâ€™t remove responsibility. Teams used to traditional environments expect fixed costs and predictable performance.

But in the cloud, everything is usage-based:

How much data you store

How much data you move

How much data you process

Services scale automatically, but not always as teams expect. Thereâ€™s often a gap between how a service is described and how it behaves with real workloads.

Architectural impact

A weak understanding of cloud mechanics can cause:

Systems that scale too slowly or too fast

Inefficient storage usage (e.g., active data on slow, cheap tiers)

Silent cost growth in data transfer and API calls

Misunderstanding reliability features

8

like regional redundancy

9

or eventual consistency

10

These issues can cause high costs, data loss, or availability risks.

How to prevent it

Use cost calculators and load tests early.

Build and test small prototypes before scaling up.

Understand SLAs

11

, scaling mechanisms, and regional data flows.

Add platform training to onboarding and architecture reviews.

Test how systems behave during failures or heavy load.

Misunderstanding the platform often leads to copying old data-center patterns in the cloud. This could limit the benefits of using the cloud.

6. Rebuilding On-Prem in the Cloud

A common and costly mistake is treating the cloud like a traditional data center.

Many teams migrate their virtual machines, networks, and firewalls without changing the design. This feels safe and familiar. But it overlooks key cloud benefits, such as elasticity, automation, and managed services.

Example:

A company copied its data-center VMs and networks directly into the cloud.

Costs increased, updates remained manual, and scaling was limited.

Only after moving to managed databases and autoscaling groups did the system become efficient.

Why it happens

Most migrations happen under time pressure.

Teams take the fastest routeâ€¦ and lift their on-premise environment into the cloud to avoid redesign work (â€œlift-and-shiftâ€ strategy

12

).

But if these old habits,

Manual patching

Fixed-size servers

Strict network zones

â€¦ continue, it brings technical debt into a platform that charges for every resource used by the unit (time/call/execution).

Architectural impact

Rebuilding on-premise in the cloud can cause:

Poor use of managed services

High costs from fixed-size infrastructure

Limited scalability because of manual maintenance

Weak resilience compared to cloud-native designs

Overreliance on network-based security instead of identity-based access

These issues make systems harder to scale, more expensive to run, and slower to recover.

How to prevent it

Modernize step by step: start small and iterate slowly.

Adopt cloud-native services where they reduce effort.

Use identity and roles as the primary security boundary.

Design for flexibility and modularity, so the system evolves.

Run pilot projects to test scaling, recovery, and automation patterns.

After moving to the cloud, structure and visibility become critical. Without proper governance, even small setups can turn chaotic.

7. Missing Governance: No Tags, No Naming, No Monitoring

Without clear governance, even well-built cloud environments quickly become confusing.

When resources have unclear names, lack tags, or are not monitored, costs increase and ownership becomes unclear.

Teams canâ€™t answer simple questions like:

Who owns this?

What is it for?

Can we delete it?

Example:

A company discovered that half of its monthly bill was because of untracked compute instances and unused storage.

None had tags or meaningful names.

It took weeks to trace the ownership.

And only after adding tagging rules and cost dashboards could they safely remove the unnecessary resources.

Why it happens

Early in the cloud adoption process, teams focus on speed rather than structure.

They create resources fast, without standard tags or names. And as the environment grows across projects and regions, these missing details become a problem:

Monitoring and cost alerts are added too late

Nobody knows which VM supports production

Or which storage bucket belongs to a retired project

As a result, the environment becomes difficult to understand.

Architectural impact

Missing governance leads to:

Poor visibility and unclear ownership

Security teams unable to trace exposed resources

Finance unable to link costs to owners or projects

Untracked resources piling up and wasting money

Automation tools failing because of missing tags and names

How to prevent it

Use clear and consistent naming conventions.

Treat governance as core architecture, not as cleanup work.

Centralize monitoring, cost reporting, and alerting across all accounts.

Use governance tools (AWS Config, Azure Policy, GCP Organization Policies).

Define mandatory tagging standards: owner, environment, purpose, cost center, and expiry.

Good governance improves visibility. But it doesnâ€™t guarantee security, especially when teams rely too much on network boundaries and overlook identity and access controls.

8. Treating Network as the Main Security Layer

In traditional data centers, the network perimeter is usually the primary line of defense:

You could isolate systems, add firewalls, control traffic, and so on.

But in the cloud, that model no longer works. Identity and permissions now define the real security boundaryâ€¦ yet many teams still rely mainly on network rules and overlook identity.

Example:

A company isolated workloads in private subnets.

But they gave broad permissions to their automation tools.

When a CI/CD credential was compromised, attackers could access data directly through APIs.

The network stayed closed, but identity access was enough to cause damage.

Why it happens

Teams coming from on-premise environments often bring the same perimeter-based mindset into the cloud.

They continue using:

VPNs

13

IP allow lists

Tight subnets

This approach may work in small setups, but cloud environments are constantly evolving. Many services, such as serverless functions and managed databases, live outside fixed networks altogether.

Architectural impact

Relying too heavily on network controls causes:

Blind spots in security

Strict rules blocking valid communication

Misconfigured firewalls exposing internal systems

No protection against identity-based attacks (the most common breach type)

How to prevent it

Treat identity (IAM roles, service accounts, least-privilege policies

14

) as the first layer of defense.

Use network rules as an extra layer of protection, not as the core security layer.

Apply zero-trust principles

15

: verify every request based on identity and context, not location.

Review access paths regularly and use automated policy validation tools.

Prefer private endpoints

16

and managed connectivity for sensitive services over custom VPNs.

Solid security foundations are essential, but resilience also requires flexibility. Static designs failâ€¦ as soon as conditions change.

9. Static Designs That Donâ€™t Handle Change

Cloud systems are built to adapt:

Services evolve, usage changes, and regions shift.

But many teams design architectures as if nothing will ever change. Static, rigid designs may look stable at firstâ€¦ yet they fail the moment workloads, connections, or platform features change.

Example:

An application launched with fixed VM clusters and manual scaling.

During seasonal peaks, performance dropped sharply, so engineers had to add capacity manually.

After switching to autoscaling (with limits) and serverless processing, the system automatically handled the load and â€œreduced costsâ€ during quiet periods.

Why it happens

Teams coming from on-premise environments often use a â€œset it and forget itâ€ mindset. They keep the old patterns:

Hardcode limits

Fixed-size clusters

Manual deployments

Sometimes internal rules (long approval queues or fear of automation mistakes) also slow down change. The result is a system that works for current traffic but canâ€™t grow with demand.

Architectural impact

Static designs lead to:

Poor scalability

Crashes during traffic spikes

Slow recovery during incidents

Wasted money during low traffic

Infrastructure that becomes outdated as services evolve

They also discourage testing and experimentation.

How to prevent it

Design for elasticity: autoscaling, serverless, and event-driven models.

Use infrastructure as code for flexible and repeatable deployments.

Version configurations and test changes with

blue-green or canary

releases.

Review the architecture regularly as usage and platform features evolve.

Introduce chaos testing to validate how systems respond to failure.

A system that canâ€™t adapt is one problem. But even flexible systems waste money if development environments mirror production too closely.

Share this post & get rewards for the referrals.

Share

10. Treating Development Like Production (Same Tiers, Policies, Permissions)

Development and production should NOT look identical.

When both environments use the same instance sizes, retention rules, and permissions, the result isnâ€™t better control; itâ€™s unnecessary cost and reduced flexibility.

Development should be a safe environment for testing ideasâ€¦ not a complete replica of the production environment.

Example:

Every test stack used the same compute tiers, observability tools, and storage.

A company cloned its entire production setup into development to â€œkeep things consistentâ€.

Within months, development costs reached almost half of the production costs.

Why it happens

Teams often copy production settings into dev or test environments to avoid surprises. But in the cloud,

Broad permissions

Long retention rules

Strict production policies

â€¦ and identical configurations slow development, increase costs, and limit experimentation.

Architectural impact

Treating all environments the same leads to:

Unused data piling up

Broad permissions exposing secrets

High costs from expensive dev resources

Strict rules blocking quick tests or experiments

A slow, rigid setup that adds little value between releases

How to prevent it

Use smaller, cheaper instance types for non-production environments.

Set different permission levels for development and production.

Define separate budgets, data retention rules, and access policies.

Use anonymized or synthetic data

17

instead of production data.

Tune monitoring and alerts for each environmentâ€™s purpose.

Automate provisioning with clear parameters per environment (dev, test, prod).

As environments grow, tracking costs across accounts becomes difficult. Without visibility, spending becomes hard to explain and control.

11. No Cost Visibility

Cloud costs become clear only when someone actually tracks themâ€¦

Many teams run for months without knowing where their money is going. And this can quickly erode trust from finance and leadership.

Example:

A project stored large datasets across many buckets.

Without cost reports, nobody noticed that storage and data transfer costs were growing faster than compute.

After adding tagging rules and cost dashboards, the team identified unused datasets and significantly reduced the monthly bill.

Why it happens

Cloud billing provides detailed information but involves complexity.

Costs spread across services, regions, and accounts. And without proper structure, itâ€™s hard to understand who is spending what.

Common causes:

No tagging

No centralized reporting

No shared dashboards

If you prioritize speed without tracking costs, it creates â€œmystery spendâ€, where everyone assumes someone else is monitoring usage.

Architectural impact

Poor cost visibility leads to:

Slow or stalled optimization efforts

Over-provisioned or idle resources staying online

Design and scaling decisions disconnected from financial reality

Storage and autoscaling choices that cost more than expected

Teams afraid to delete resources because the impact is unknown

How to prevent it

Assign clear cost ownership to teams.

Tag resources by cost center, owner, and environment.

Use built-in cost tools (AWS Cost Explorer, Azure Cost Management, GCP Billing Reports).

Set budgets, forecasts, and anomaly alerts per project or account.

Review cost data regularly in architecture and operations meetings.

Add dashboards that show spend trends alongside performance metrics.

Poor cost visibility often hides waste, and one common reason is collecting too much data.",https://newsletter.systemdesign.one
https://newsletter.systemdesign.one/p/stock-exchange-system-design,How Stock Exchange Works,#101: Stock Exchange System Design - Part 1,2026-01-02T16:55:29.143238,Neo Kim,"Share this post

& I'll send you some rewards for the referrals.

I created block diagrams for this newsletter using

Eraser

.

Today is a big day because Iâ€™m excited to announce that the doors are officially open for our brand-new newsletter series...

INTRODUCING:

Design, Build, Scale

This newsletter series will elevate your software engineering career.

If youâ€™ve ever thought:

â€œI want to ace system design interviews, but donâ€™t know where to start.â€

â€œI want to master system design so I can become good at work.â€

â€œItâ€™s time. I should learn how big companies engineer their systems.â€

Then this is for you.

Hereâ€™s what youâ€™ll get inside Design, Build, Scale:

High-level architecture of real-world systems.

Deep dive into how popular real-world systems actually work.

How real-world systems handle scale, reliability, and performance.

And hereâ€™s the best part:

Youâ€™ll get 10x the results you currently get with 1/10th of your time, energy, and effort.

Onward.

Give Your AI Tools the Context They Need So They Stop Guessing (Sponsor)

Your AI tools are only as good as the context they have.

Unblocked

pieces together knowledge from your teamâ€™s GitHub, Slack, Confluence, and Jira, so your AI tools generate production-ready code.

See How

What is a Stock Exchange? (The Simple Answer)

Imagine the stock market as a farmerâ€™s market.

Each stock is like a stall area in the market where people gather to buy or sell a product. More trade brings more PROFIT for the stock exchange through fees. So their job is to facilitate as many â€œtransactionsâ€ as possible. Each seller can set up a stall and specify the price theyâ€™re willing to sell for.

If nobody buys, they might lower their price.

Buyers want the cheapest price, so they usually go to the stall offering the lowest price first.

In an â€œidealâ€ world, buyers would stand in a queue, ordered by the price theyâ€™re willing to pay for fairness. This means buyers offering higher prices stand closer to the front. Plus, a buyer can adjust their position in the queue by changing the price theyâ€™re willing to pay. A trade occurs when a buyerâ€™s price meets or exceeds a sellerâ€™s price.

Thatâ€™s when the exchange matches them!

In simple words:

BUY orders get sorted in decreasing order (highest bid first).

SELL orders get sorted in increasing order (buy as cheaply as possible).

The point where they overlap is the market price.

In reality, itâ€™s much more complicatedâ€¦ but this is the basic idea.

Letâ€™s Start with

Requirements

Donâ€™t worry, itâ€™s simple!

An exchange that trades ONLY stocks.

Users can place a BUY or SELL order.

Also users can cancel their order at any time.

Exchange must match buyers and sellers in real time.

And restrict the number of shares a user can trade per day.

It should also publish market data

1

in real-time.

Exchanges

2

make trading fair and transparent.

In most of them:

New orders = ~50% messages,

Cancels = ~40% messages,

Executions = ~2% messages.

The rest is... noise.

Trading Terms

(Simplified)

Broker

:

An app or site

3

that lets users BUY or SELL, and view prices from an exchange.

Order Book

:

A list of all current BUY and SELL orders for a stock, showing who wants to buy or sell, how much, and at what price.

Market Order

:

A BUY or SELL order where you donâ€™t set a price. It executes immediately at the current market price, as long as thereâ€™s enough liquidity

4

. Plus, it receives priority in the order book.

Limit Order

:

A BUY or SELL order where you choose the â€œexactâ€ price.

Example:

Buy at $100 â†’ fills

5

at $100 or lower.

Sell at $100 â†’ fills at $100 or higher.

Spread

:

The difference between the highest BUY price and the lowest SELL price.

Profit Formula

:

Profit = Sell price - Buy price

(So buy low, sell high.)

Stock Exchange Architecture

An exchange interacts with many â€œexternalâ€ services:

Live chat support.

CAPTCHA implementation.

User data management & tracking.

Service for sending emails, text messages, and mobile app notifications.

But letâ€™s focus on the core design itselfâ€¦

Its architecture is asynchronous and event-sourced.

6

Here are the three key components of an exchange:

Broker - allows people to buy and sell stocks on the exchange.

Gateway - entry point for brokers to send BUY or SELL orders to the exchange.

Matching Engine - component that matches buy and sell orders to create trades.

Letâ€™s dive in!

1. Broker

A broker interacts with an exchange to:

Send order requests - place orders, receive status updates, and access trade information.

Receive market data - historical data for analysis, stream live trade data, and so on.

A user connects to the broker using REST APIs and WebSockets for real-time data transfer. While brokers use the Financial Information Exchange (

FIX

) protocol to communicate with the gateway.

FIX is a bidirectional communication protocol for secure data exchange through a â€œpublic networkâ€.

7

It assigns unique sequence numbers to order messages. Besides, it uses checksums and message length to verify data integrity

8

.

2. Gateway

It receives orders from brokers and converts

9

them into the exchangeâ€™s internal format. Then it sends

10

the trade execution results back to the users. Itâ€™s also possible to put a gateway near exchange servers (

co-location

) for low latency.

A gateway has three key parts:

Risk Manager - ensures the user has enough funds and blocks any unusual trading activity. Also, it determines exchange fees.

Wallet - stores the userâ€™s funds and assets for trading.

Order Manager - assigns sequence numbers for fairness and updates order states. Plus, it sends cleared orders to the matching engine.

Letâ€™s dive inâ€¦

Gateway validates an order

11

with the risk manager and wallet service.

Then it passes the request to the order manager. Think of the

order manager

12

as a lightweight list containing ALL orders: open, cancelled, and rejected ones. It updates the order state and handles cancel requests.

Order manager assigns a globally increasing sequence number to each order (trade or cancel) and execution fill (for both BUY and SELL).

Gateway Internal Architecture

A sequence number guarantees:

Ordering for fairness, timeliness, and accuracy.

Fast recovery and deterministic replay.

Exactly once guarantee.

Plus, sequence numbers make it easy to find missing events in both inbound and outbound sequences

13

.

After clearance, the order manager routes the order to the matching engine.

Each message type (new order, cancel, trade) has its own topic queue, with its own sequence numbers. This separation helps to:

Maintain order within each stream.

Process each message type independently and reduce contention.

And make recovery easier as the exchange could â€œreplayâ€ events per topic in exact order.

Letâ€™s keep going!

3. Matching Engine

It verifies sequence numbers, matches BUY and SELL orders, and creates market data based on trades

14

.

A matching engine has two key parts:

Order Book

15

- an in-memory list of BUY and SELL orders.

Matching Logic - matches BUY and SELL orders and sends those trade results as market data.

Letâ€™s dive inâ€¦

Order Book

It keeps an in-memory list

16

of open orders for each stock

17

(symbol).

There are actually two lists for each stock:

One for BUY orders,

Another one for SELL orders.

Each list gets sorted by price and timestamp in a first-in, first-out (FIFO) manner.

And each price level gets a separate queue of orders (doubly linked list):

New orders get added at the tail - O(1) time complexity.

Filled or canceled orders get removed from the queue using a pointer

18

- O(1) time complexity.

It keeps track of the highest bid and lowest ask for quick matching. For example, BUY@96 and SELL@98.

Searching through all price levels and linked lists could be slow - O(n). So it uses an index to map order IDs to the order object in memory: order ID â†’ pointer (reference). It allows fast cancellations by looking up the order by ID in the index in O(1) and then setting the amount to 0.

A closed order gets removed from the order book and gets added to the transaction history.

Matching Logic

It checks if a message follows the correct sequence and matches BUY and SELL orders when:

buy price â‰¥ sell price

The same order sequence (input) must always produce the same execution sequence (output). So the matching function must be fast and accurate, and deterministic.

Ready for the best part?

Reminder: this is a teaser of the subscriber-only newsletter series, exclusive to my golden members.

When you upgrade, youâ€™ll get:

High-level architecture of real-world systems.

Deep dive into how popular real-world systems actually work.

How real-world systems handle scale, reliability, and performance.

Subscribe",https://newsletter.systemdesign.one
https://newsletter.systemdesign.one/p/code-review-best-practices,I Studied How Top 0.1% Engineering Teams Do Code Reviews,#100: Here Is What I Learnedâ€¦,2026-01-02T16:55:30.722114,Neo Kim,"Keep pull requests SMALL, so theyâ€™re easy to understand and review. Plus, small pull requests create fewer problems later.

Watch for duplicate & dead code. Remove unused code and abstract logic to avoid duplication.

Get my system design playbook on newsletter signup for FREE:

Subscribe

Good tests prevent regressions and show how code should work. So verify whether the new code has â€œsufficientâ€ test coverage.

Choose the correct reviewer for each change: code owners or domain experts can quickly catch domain-specific issues. If you assign many reviewers,,, ensure each understands their responsibilities to prevent delays.

Write clear pull request descriptions that explain the â€œwhatâ€ and â€œwhyâ€ of changes. Also, link relevant tickets and attach screenshots that help reviewers understand the context.

Use a code review CHECKLIST. It could cover design, readability, security, testing, and so on. This ensures consistency in reviews and reduces the chances of missing common issues.

Automate easy parts. Use tests, linters, and static analysis to catch errors and style issues. This way, reviewers can focus on logic & architecture.

Iâ€™m happy to partner with

CodeRabbit

on this newsletter. Code reviews usually delay feature deliveries and overload reviewers. And I genuinely believe CodeRabbit solves this problem.

Try CodeRabbit

Use review metrics to find â€œbottlenecksâ€. Measure: review time and bug rates, and pull request size. Then adjust the process based on data to improve speed without sacrificing quality.

Review quicklyâ€¦ but donâ€™t rush! The goal is to improve code health, not just quick approvals.

Keep reviews SHORT. Itâ€™s hard to stay focused after reading 100+ lines of code. If the change is big, break it up into smaller parts or focus on one section at a time to give effective feedback.

Get early feedback on big features to save time later. This helps to catch issues early and makes reviews more manageable.

Ask for a review ONLY after tests & builds pass. This prevents wasting the reviewerâ€™s time on broken code. Besides, it signals the code is stable enough to review.

Use review tools effectively to save time - threaded comments, suggested edits, and templates, and so on. The correct setup makes reviews smoother.

Watch out for potential bugs & logic mistakes that tests might miss. Think about â€œrace conditions or extreme inputsâ€. Human reviewers can often spot bugs that automated tests miss, especially in complex logic.

Iâ€™ll send you some rewards for the referrals.

Share

Encourage ALL team members to take part in code reviews. And donâ€™t let the same people handle all reviews. Rotation spreads knowledge and avoids burnout.

You canâ€™t review code effectively if you donâ€™t understand what it does. So read the code carefully and run it locally if necessary.

Keep the feedback within the â€œscopeâ€. If you notice any issues outside the scope of the change, log them separately. This keeps reviews constructive and prevents endless delays.

Review in layers: design then details. This approach helps you catch both major and minor issues efficiently.

Compare the implementation with the requirements. Ensure it handles acceptance criteria and edge cases and error conditions correctly.

Enforce coding standards for CONSISTENCY. Suggest refactoring if the logic is hard to follow.

Use AI tools to summarize changes or find issues. It saves time! But use those as a helper... and not a replacement for human reviews.

Guess what? When you open a pull request,

CodeRabbit

can generate a summary of code changes for the reviewer. It helps them quickly understand complex changes and assess the impact on the codebase. Speed up the code review process.

Try CodeRabbit

Set clear â€œguidelinesâ€ for how reviews get approved. For example, have at least two reviewers for critical code changes.

Consider how code performs at scale in â€œperformance-criticalâ€ areas. Look out for things that might cause slowdowns in critical paths - unnecessary loops and so on. Remember: fixing issues is easier during review than in production.

Use reviews as an opportunity to share KNOWLEDGE and grow together. Share tips and best practices, especially with junior engineers.

Ensure the code handles errors â€œgracefullyâ€. Functions must deal with null inputs or external call failures without crashing. Good error handling makes the system robust & easy to debug.

Adjust practices to fit your teamâ€™s needs. What works at one company might not work for another. Keep experimenting until you find your ideal flow.

Iâ€™ll send you some rewards for the referrals.

Share

Always review with the bigger picture in mind. Think about how the change interacts with the codebase. And consider cross-cutting concerns: performance, concurrency, and backward compatibility.

Itâ€™s better to clarifyâ€¦ than to assume. So ask clarifying questions when something is unclear about the change. A simple question can prevent misunderstandings or reveal missing requirements.

If possible, run the code locally, especially for complex & critical code changes. Seeing it in action can reveal issues that reading wonâ€™t.

Focus on code correctness & clarity,,, not personal style. If an issue is purely stylistic and not covered by a guideline, consider letting it pass or marking it as a nitpick. Remember, reviews are about improving the codebase.

Suggest a solution when pointing out a problem. If a function is complex, propose breaking it into smaller functions or using a design pattern. Reviews are most valuable when they teachâ€¦ not just criticize.

Consider whether the documentation requires any updates because of the change. An API change may need changes to the API docs or the README file. Ensure everything remains accurate and complete.

Treat code review as a â€œteam effortâ€, not a fight. Focus on making the product better rather than proving someone wrong. A friendly tone makes feedback easier to accept.

Mention explicitly which comments are essential & which are optional. Label important fixes separately from small â€œnice-to-haveâ€ ideas. This helps the author to prioritize and stay focused.

Bet you didnâ€™t knowâ€¦

CodeRabbit CLI

brings instant code reviews directly to your terminal, seamlessly integrating with Claude Code, Cursor CLI, and other AI coding agents. While they generate code, CodeRabbit ensures itâ€™s production-ready - catching bugs, security issues, and AI hallucinations before they hit your codebase.

Install CodeRabbit CLI

Involve a neutral third party in disagreements over CRITICAL issues - ask a tech lead or architect. Also, create a follow-up task if the problem is outside the current scope.

Explain the â€œwhyâ€ behind your feedback. Understanding the reason behind feedback helps others learn. This way, theyâ€™re less likely to repeat the issue.

Secure code protects users and the business. So always think about SECURITY. Be cautious of weak data validation, exposed data, or improper error handling.

Be open to discussion when opinions differ. Ask for the authorâ€™s reasoning and listen before insisting. Talking through disagreements often leads to better solutions.

Point out whatâ€™s done well tooâ€¦ it motivates people to keep doing it. Keep a balance between criticism and appreciation for high morale.

Donâ€™t use code reviews for PERFORMANCE EVALUATIONS! Reviews exist to improve code, not to measure people. When engineers feel safe, they write better code & review honestly.

Respond to feedback with curiosity,,, not defensiveness. Treat comments as learning opportunities.

Having another set of eyes helps catch mistakes. So make sure someone else reviews â€œeveryâ€ change. Even small changes benefit from peer review.

I could go on and on and on.

But if those 42 ways arenâ€™t enough to 10x your code reviews, then probably anything else I say will go in one ear and right out the other.

As far as AI code reviews to catch bugs, security flaws, and performance issues

as

you write code?

Thatâ€™s why

CodeRabbit

exists.

It brings real-time, AI code reviews straight into VS Code, Cursor, and Windsurf.

ğŸ‘‰

Install CodeRabbit in VSCode for FREE

If you find this newsletter valuable, share it with a friend, and subscribe if you havenâ€™t already. There are

group discounts

,

gift options

, and

referral rewards

available.

Subscribe

ğŸ‘‹ Find me on

LinkedIn

|

Twitter

|

Threads

|

Instagram

Want to advertise in this newsletter?

ğŸ“°

If your company wants to reach a 190K+ tech audience,

advertise with me

.

Thank you for supporting this newsletter.

You are now 190,001+ readers strong, very close to 191k. Letâ€™s try to get 191k readers by 21 November. Consider sharing this post with your friends and get rewards.

Yâ€™all are the best.

Share",https://newsletter.systemdesign.one
https://newsletter.systemdesign.one/p/frontend-system-design,21 Frontend System Design Concepts for Software Engineers,"#99: A practical guide to building fast, scalable, and reliable web apps",2026-01-02T16:55:32.626746,Neo Kim,"Get my system design playbook for FREE on newsletter signup:

Subscribe

Share this post

& I'll send you some rewards for the referrals.

If youâ€™re coming from the backend, you probably think the frontend is just â€œHTML, CSS, maybe some JavaScript.â€ But honestly? Modern frontend engineering has grown into something much closer to backend system design.

Just like your APIs need to be fast, scalable, and reliable, frontend apps also have to handle millions of users, load content quickly, and stay observable and secure.

This newsletter is a quick introduction to frontend system design.

Weâ€™ll take concepts you already know from the backend, like caching, deployment pipelines, observability, and security, and see how they apply in the browser.

By the end, youâ€™ll see that the frontend isnâ€™t just about buttons and forms. Itâ€™s about building systems that run right in the userâ€™s browser.

Onward.

Supercharge AI Coding With Your Teamâ€™s Context (Sponsored)

Your AI tools are only as good as the context they have.

Unblocked

connects your code, docs, and conversations so Cursor, Claude, and Copilot finally understand your system like your best engineer.

Start a free trial

I want to introduce

Shefali Jangid

as a guest author.

Sheâ€™s a web developer, technical writer, and content creator with a love for frontend architecture and building things that scale.

Check out her work and socials:

Shefali.dev

GitHub

Twitter

Youâ€™ll often find her writing about web development, sharing UI tips, and building tools that make developersâ€™ lives easier.

Rendering & Delivery Models

One of the first things to understand is how webpages reach your users.

The way you build and load them affects how fast, reliable, and smooth your site feels. You can pre-build pages, render them on the server, build them in the browser, or mix these approaches.

Building web pages works much like a server handles API responses. The trade-offs change depending on when and where the HTML gets generated.

Letâ€™s start with pre-built pages and move to fully dynamic ones. Weâ€™ll see how each affects speed, scalability, and content freshness.

1 Static Site Generation (SSG)

Before SSG, websites worked in two fundamental ways. The server either built the page for every request, or the browser built it on the client side. That means:

Every request needed work to generate the page.

Pages could get slow if many people visit at once.

Caching was tricky, so scaling was hard.

SSG solves this by pre-building the HTML when you deploy your site. The system can fetch data during the build process, even for pages with dynamic content, which means all content is baked into static HTML files before any user visits them.

During the build process, the framework executes data-fetching code, queries your database, and generates complete HTML files for each route. The framework then uploads them to your CDN or hosting provider.

When users request a page, they receive a fully formed HTML document immediately, without waiting for server-side processing or client-side data fetching.

This makes SSG super fast for users because thereâ€™s no rendering delay. The trade-off is that if your content changes, youâ€™ll need to rebuild and redeploy to update the static files, which is why SSG works best for content that doesnâ€™t change frequently.

Itâ€™s like preparing API responses in advance; the hard work is done before anyone asks.

Why it matters:

Pages load super fast.

Easy to handle millions of users.

SEO is better because pages get fully rendered from the start.

Use case:

Documentation sites, marketing landing pages, or personal blogs where content updates happen through deployments, not user actions.

2 Incremental Static Regeneration (ISR)

Static Site Generation (SSG) is fast, but what if your content changes frequently? Rebuilding the whole site every time would be a pain.

Thatâ€™s where Incremental Static Regeneration (ISR) comes in.

Pages are still pre-built, but they can update automatically without a full redeploy.

You just set a revalidation time; after that period, the next visitor triggers a background rebuild of that specific page on the server, not a full deployment. The old version loads instantly, so users donâ€™t wait. After regeneration, the new version replaces the cached one. This occurs per page, not site-wide, allowing you to set different revalidation intervals for individual pages.

Itâ€™s like cached API responses with an expiry timer; users might glimpse an older version until itâ€™s refreshed, but the update happens quietly behind the scenes.

Why it matters:

Just as fast as SSG, but the content stays fresh.

Perfect for dynamic websites like blogs or e-commerce sites.

Works with CDNs, so updates happen without downtime.

Use case:

E-commerce product pages where most content (e.g., descriptions or images) is static, but some parts, like prices or stock info, update occasionally.

ISR keeps the page fresh without full redeploys, while real-time data, such as live prices, can come from APIs.

3 Server-Side Rendering (SSR)

Server-Side Rendering (SSR) works the other way around: the server builds the page for each request. It fetches the data, generates the HTML, and sends it to the user.

Unlike Static Site Generation (SSG), which is great for mostly static pages, SSR is useful when content needs to stay fresh or personalised. For example, dashboards, user profiles, or live feeds. Because the system generates pages in real time, they always display the latest data instead of relying on pre-built files.

Think of it like a regular API endpoint; everything gets computed on demand.

Why it matters:

Keeps content fresh and easy to personalise.

Perfect for pages that need real-time data or personalised content.

Note:

Under heavy traffic, SSR can slow down because it builds each page on demand, but caching can help balance load and speed.

Use case:

Social media feeds, admin dashboards, or user-specific pages where content varies by session.

4 Client-Side Rendering (CSR)

CSR means the browser does most of the work instead of the server. The server sends only a basic HTML page and some JavaScript. The browser then loads the data and builds the page on the fly.

This approach is useful when you need rich interactivity, real-time updates, or pages that change often based on user actions  - things that static or server-rendered pages canâ€™t handle easily.

Think of it like sending raw JSON and letting the client put it together.

Note:

The first page load can be slower because the browser needs to download and run JavaScript before showing the content. Since pages get built in the browser, search engines might not see them immediately, so you might need extra setup like pre-rendering or server-side rendering for better SEO.

Why it matters:

Reduces pressure on the server.

Makes the app more interactive and responsive.

Works best for apps people use for a long time, like dashboards or editors.

Use case:

Complex apps like Figma, Notion, or Google Docs, where the app is highly interactive and users stay on the page for extended sessions.

5 Hybrid Rendering

Sometimes, one approach just isnâ€™t enough.

Different parts of your app might have different needs. For example, some pages stay mostly the same, while others need fresh or personalised data. Thatâ€™s where hybrid rendering comes in.

It mixes different strategies:

Server-side rendering (SSR) for pages that need live or personalised content,

Static site generation (SSG) for pages that rarely change,

And client-side rendering (CSR) for sections with lots of interactivity.

Think of it like combining pre-computed API responses with on-demand endpoints - all in the same system.

Why it matters:

You get the best of everything: speed, fresh content, and interactivity.

Allows you to choose the right approach for each page or component.

Reduces overloading the server while keeping content dynamic where needed.

Use case:

Large-scale apps like e-commerce platforms often combine different rendering strategies:

The homepage and category pages use static generation for speed.

Product pages use incremental static regeneration to keep content fresh.

User account pages use server-side rendering for personalised data.

The shopping cart uses client-side rendering for real-time updates without page reloads.

6 Content Delivery Networks (CDNs) & Edge Delivery

No matter which rendering method you choose, serving content efficiently is super important. CDNs keep copies of your static files on servers worldwide. This lets users download them from a nearby location instead of your main server.

This is especially useful for global audiences. For example, when someone in India visits a site hosted in the US, the CDN delivers the content from a local server, making it load much faster.

Edge rendering takes this idea a step further. Instead of just serving static files, it can actually run code or build pages at the edge, closer to the user, which reduces latency even more.

Think of it like having caches and compute nodes near your users, so requests go to a nearby server instead of your main database.

Why it matters:

Faster load times everywhere.

Easy to scale to millions of users.

Works perfectly with SSG, ISR, SSR, or hybrid setups.

Use case:

Any globally distributed application. Media sites like The New York Times use CDNs to serve articles instantly worldwide.

Performance & Optimisation

Now that you understand how your pages get rendered, the next obvious question is, â€œ

How quickly do they actually load?

â€

Even the most beautiful app can be frustrating if it takes too long to open or lags while being used. In frontend system design, speed really matters.

Letâ€™s dive in!

7 Web Performance Metrics

To really understand your appâ€™s speed, there are a few key metrics you should watch closely:

TTFB (Time to First Byte):

The time it takes for your browser to get the first piece of data back from the server or CDN after making a request.

FCP (First Contentful Paint):

The moment when something first appears on the screen, like text, an image, or a button, so the user knows the page is loading.

LCP (Largest Contentful Paint):

The time it takes for the main part of the page, like a large image or headline, to fully appear on the screen.

CLS (Cumulative Layout Shift):

It measures how much the page layout jumps around while loading, like when text or buttons suddenly shift because images or ads are still loading.

These are basically the frontend versions of response time, throughput, and latency in backend systems. Itâ€™s important to keep a close eye on them; users can notice even minor delays of a few hundred milliseconds.

Why it matters:

You can spot slow pages before users even notice.

Improves engagement and reduces bounce rates.

Helps guide your optimisations for a smoother experience.

Use case:

E-commerce sites must optimise for LCP (product images) and CLS (avoid layout shifts during checkout). News sites focus on FCP to show headlines quickly.

8 Lazy Loading

Of course, fast pages arenâ€™t just about metrics; theyâ€™re also about smart resource management.

Not everything on a page needs to load immediately. Lazy loading means loading heavy assets, like images, videos, or big components, only when theyâ€™re actually needed.

This works by using techniques like the

Intersection Observer API

or conditional imports, which tell the browser to fetch those resources only when they come into view or are triggered by user interaction.

Itâ€™s like fetching extra data from an API only when the user asks for it.

Why it matters:

Cuts down the initial load time.

Makes the pages feel faster and smoother.

Saves bandwidth for users who donâ€™t need everything immediately.

Use case:

Image-heavy sites like Pinterest or Instagram use lazy loading extensively; images below the fold donâ€™t load until you scroll.

9 Service Workers & Caching

Once youâ€™ve optimised loading, you can make your app faster and more reliable using service workers and caching.

Service workers are background scripts that run in a separate thread from your main web page. They can intercept network requests and cache important files or data, helping your app load faster and even work offline.

Think of them as a smart middle layer between the browser and the network; if something is already cached, itâ€™s served instantly instead of being fetched again.

Why it matters:

Speeds up repeat visits.

Reduces the load on servers.

Keeps apps usable even with poor or no internet connection.

Use case:

Progressive Web Apps like Twitter Lite or Starbucks PWA, which cache core UI and recent content, so users can browse even on unstable mobile networks.

Data & State Management

Once your UI loads quickly, the next step is to think about the data behind it.

In real apps, this data (also called state) can come from different places:

Some live inside a single component

(a reusable piece of the UI, like a button),

Some are shared across the app,

And others come from APIs.

How you manage this state can make or break your appâ€™s speed, reliability, and scalability.

10 State Management (Local, Global, Server Cache)

Local state:

data that lives inside a single component, used for things like toggles, forms, or small interactions. Itâ€™s simple to manage and doesnâ€™t add much complexity.

Global state:

data thatâ€™s shared across multiple components or pages, like user info or theme settings. Tools like

Redux

,

Zustand

, or

React Context

help manage it.

Server cache:

stores frequently used API data on the client so the app doesnâ€™t have to fetch it again and again, making it faster and reducing server load.

Think of it like database caching: by deciding where data should live, you can make your app more responsive, reliable, and easier to scale.

Why it matters:

Keeps your app responsive.

Reduces unnecessary API calls.

Makes scaling smoother as your app grows.

Use case:

Local state for a modalâ€™s open/closed status. Global state for theme preference (dark mode) that affects every component. Server-side cache for user profile data displayed by multiple components.

11 API Caching with Expiration

Caching doesnâ€™t stop at the component level. You can store API responses in memory,

IndexedDB

(a browser database for larger data)

,

or

localStorage

(for smaller key-value data)

,

and set expiration rules to make sure data stays fresh.

Itâ€™s like having a Redis cache server, but right in the browser instead of on your server.

Why it matters:

Keeps data up-to-date for users.

Reduces repeated server requests.

Makes your app feel faster.

Use case:

A news app might cache articles for a few minutes so users can read offline, while comments refresh more often to stay up to date. Similarly, a SaaS dashboard could cache chart data while the user is on the page, then refresh it when they come back later.

12 GraphQL vs REST (Reducing Over/Under-Fetching)

How you fetch data also affects performance.

REST:

Can sometimes send too much data or not enough, making your app fetch extra information or require additional requests.

GraphQL:

A query language for APIs that lets the client ask for exactly the data it needs, avoiding extra or missing information. This avoids over-fetching or under-fetching data and helps reduce unnecessary requests.

Itâ€™s like how you optimise database queries on the backend to make them faster and use less bandwidth, but this happens on the frontend.

GraphQL sits between the client and the server as one endpoint. The client asks for exactly the data it needs, and the serverâ€™s GraphQL layer collects that data from databases or other APIs, then sends back a clean, organised response.

This way, you make one flexible request instead of several REST calls, making it faster and more data-efficient.

Why it matters:

Saves bandwidth, especially on mobile networks.

Reduces unnecessary requests.

Simplifies client-side data handling.

Use case:

GraphQL works best for complex apps that need data from many places at once, like GitHub. One GraphQL query can get a pull request, comments, and author info in a single request instead of several REST calls. While REST is simpler and great for apps with stable data, like blogs or public APIs that rely on caching.

13 Pagination Strategies (Cursor vs Offset)

Loading large lists or tables all at once can be heavy. Pagination helps break the data into manageable chunks.

Offset pagination:

Uses page numbers or record counts (like

?page=2

or

?offset=20

) to fetch data. Itâ€™s simple and works well for lists that donâ€™t change often. But the list order shifts if new items are added or old ones are removed. This can make the same offset return different items, leading to duplicates or missing entries.

Cursor pagination:

Uses a pointer to mark where the last item ended, so the next request starts right after it. Itâ€™s more reliable for live or frequently updated data (social feeds or chat messages) because it keeps track of the exact position in the dataset. That means even if new items are added or removed while youâ€™re scrolling, you wonâ€™t see duplicates or miss entries.

Why it matters:

Handles large datasets efficiently.

Prevents slowdowns and performance bottlenecks.

Keeps dynamic lists reliable and consistent.

Use case:

Offset pagination:

best for data tables with stable data and clear page numbers, such as admin panels or product catalogs.

Cursor pagination

: ideal for infinite scroll feeds like social media timelines, notification lists, or any real-time list where items are frequently added or removed.

14 Real-Time Data & Networking (WebSockets, SSE, Polling)

Finally, some apps need live updates, like chat apps, dashboards, or notifications. How you handle real-time data matters.

WebSockets:

Let the client and server send messages to each other in real time, both ways, without constantly asking for updates.

Server-Sent Events (SSE):

The server can push updates to the client in real time, but communication only goes one way, from server to client.

Polling:

The client regularly asks the server for updates. Itâ€™s simple to set up, but it can put more load on the server.

Itâ€™s like building event-driven systems on the backend, but here it happens in the browser.

Why it matters:

Supports live dashboards, chat, and notifications.

Improves interactivity and user engagement.

Allows you to choose the right strategy for your appâ€™s needs.

Use case:

WebSockets:

chat apps (Slack), multiplayer games, collaborative editing (Google Docs).

SSE:

live notifications, stock tickers, server logs streaming to a dashboard.

Polling:

simple use cases like checking for new emails or status updates.

Architecture & Scalability

As your app grows, managing complexity becomes just as important as writing features. Frontend architecture isnâ€™t just about code; itâ€™s about building systems that are maintainable, scalable, and predictable.

15 Micro Frontends

When multiple teams work on the same app, things can get messy fast.

Micro frontends let each team build and deploy their part of the app separately. For example, one team handles the dashboard while another builds the settings page. Technically, the app is divided into smaller frontend projects that are combined at runtime to work as one seamless app.

A module federation feature (for example, in tools like

Webpack

) lets these separate projects share code (like components or utilities) directly in the browser, without rebuilding or duplicating code across projects.

Why it matters:

Teams can develop features faster and in parallel.

Reduces duplicated code across bundles.

Supports independent deployment cycles, so updates donâ€™t block each other.

Use case:

Large enterprises with multiple teams working on different product areas. For example, big companies like

Zalando, IKEA, DAZN, and Spotify use micro-frontends

so each team can build and release their part of the app on their own.

16 Component-Based Architecture & Design Systems

Components are the building blocks of your app. A design system ensures these components stay consistent and reusable across teams and projects.

Itâ€™s like having reusable backend modules or libraries, but for your UI.

Why it matters:

Makes the UI predictable and easier to maintain.

Encourages code reuse across pages and projects.

Helps teams scale efficiently without creating chaos.

Use case:

Used by companies with many products or teams to keep design consistent, like

Shopifyâ€™s Polaris

or

IBMâ€™s Carbon

, which are open-source design systems containing ready-to-use UI components, styles, and guidelines.

Even small startups benefit: a shared set of 10â€“20 components (like buttons and modals) helps teams build faster and keep the UI consistent.

17 Build & Deployment Pipelines (CI/CD for Frontend)

Frontend apps also benefit from CI/CD (Continuous Integration and Continuous Deployment) pipelines, just like backend services. These pipelines automatically handle steps like building the app, running tests, and deploying updates.

In simple terms, every time you push code, CI/CD tools check that nothing breaks and then safely release the latest version, making deployments faster, more reliable, and less manual.

Why it matters:

Minimises human errors during deployment.

Enables fast, reliable releases.

Makes scaling and frequent updates much smoother.

Use case:

Works for any app with regular updates, from small teams auto-deploying to Vercel to big companies like Netflix releasing thousands of times a day. It keeps updates fast, safe, and reliable.

User Experience & Reliability

Your users donâ€™t care about your architectures or caching strategies; they just want the app to be fast, reliable, and easy to use.

18 Accessibility (a11y) & Mobile-First Design

Accessibility and mobile-first design arenâ€™t just design principles; theyâ€™re system-level considerations. Accessibility ensures your appâ€™s UI and code structure work for everyone, including people using assistive technologies.

Mobile-first design forces you to build efficient layouts, load lighter assets, and prioritize key features, all of which influence performance, scalability, and overall frontend architecture.

Why it matters:

Reaches more users.

Makes your app easier and more pleasant to use.

Ensures a consistent experience across devices.

Use case:

Government sites (accessibility is legally required in many countries), e-commerce, and content platforms. Mobile-first is essential for apps in developing markets where mobile is the main or only device.

19 Progressive Web Apps (PWAs) & Offline-First

Progressive Web Apps (PWAs) are web apps that behave like native apps. They can work offline, send notifications, and even be installed on a device.

They use a few key technologies:

Service workers run in the background to cache important files like HTML, CSS, and API responses.

A web app manifest defines how the app looks and behaves when installed.

And HTTPS keeps everything secure.

Together, these make the app fast, reliable, and installable.

Why it matters:

Users can access your app anywhere.

Reduces load on servers.

Improves reliability and user trust.

Use case:

Apps where offline access is valuable: Twitter Lite, Starbucks PWA, field service apps, and news apps.

20 Security Basics (XSS, CSRF, CSP, Authentication)

Speed means nothing without security. Frontend isnâ€™t just about the UI; itâ€™s also the first line of defence for your app.

XSS (Cross-Site Scripting):

Stop attackers from injecting malicious scripts into your app.

CSRF (Cross-Site Request Forgery):

Protect your forms and actions that change data from being triggered by attackers without the userâ€™s consent.

CSP (Content Security Policy):

A rule set that helps prevent malicious scripts from running in your app.

Authentication:

Make sure user tokens and sessions are stored and handled securely in the browser.

Why it matters:

Protects your users and their data.

Prevents common attacks before they reach the backend.

Builds trust and helps with compliance.

Use case:

Any app handling sensitive data. Financial apps need strict CSP and token handling. Social platforms must prevent XSS to avoid account takeovers. E-commerce sites need CSRF protection on checkout to prevent unauthorised purchases.

21 Observability & Error Monitoring (Client-Side)

Even if everything works well, things can still break in production. Thatâ€™s why observability is important.

Frontend errors are just like 500 errors in your backend; they happen. Monitoring tools like

Sentry

or

LogRocket

help you track:

JS exceptions:

errors that happen in your JavaScript code while the app is running.

Performance bottlenecks:

parts of your app that slow it down or make it lag.

User interactions leading to errors:

actions by users that trigger bugs or crashes in your app.

These tools add a small script to your app. When something breaks, it collects information like the error message, what the user was doing, and browser details. Then it sends that data to the toolâ€™s server, where you can see and fix the issue from your dashboard.

Why it matters:

Detects and resolves issues faster.

Keeps your app stable and performant.

Improves the overall user experience and trust.

Use case:

Used in production apps with real users. SaaS teams track errors right after deployment, e-commerce sites watch checkout issues, and session replay tools help support teams see what confused users without extra bug reports.

Conclusion

Frontend system design is basically backend system design, just happening in the userâ€™s browser.

Every choice you make, like rendering method, caching strategy, state management, architecture, and security, affects speed, scalability, and reliability.

So next time youâ€™re building a frontend, ask yourself:

Where should computation happen?

On the server, in the clientâ€™s browser, or at the edge?

When does the data need to be up-to-date?

Prebuilt, cached, or real-time?

How can we keep the app fast and reliable?

Lazy loading, smart caching, or micro frontends?

How do we scale this?

Can the architecture handle 10x traffic? 100x?

How do we maintain this?

Will new developers understand the architecture? Can teams work independently?

Think of your frontend as a distributed system. Treat it that way, and your users will get an app thatâ€™s fast, smooth, and seamless, exactly what they expect.

ğŸ‘‹ Iâ€™d like to thank

Shefali

for writing this newsletter!

Plus, donâ€™t forget to check out her work and socials:

Shefali.dev

GitHub

Twitter

Youâ€™ll often find her writing about web development, sharing UI tips, and building tools that make developersâ€™ lives easier.

ğŸš¨

Design, Build, Scale

ğŸš¨

Iâ€™m excited to let you know weâ€™re launching Design, Build, Scale!

(3-part newsletter series that breaks down popular interview questions.)

Exclusive to PAID members...

Here are a few of the things youâ€™ll get:

High-level architecture of real-world case studies.

Deep dive into how popular real-world systems actually work.

How real-world systems handle scale, reliability, and performance.

10x the results you currently get with 1/10th of your time, energy, and effort.

Start Date: November 2025

ğŸ‘‰

Click here to join

(takes 60 seconds)

If you find this newsletter valuable, share it with a friend, and subscribe if you havenâ€™t already. There are

group discounts

,

gift options

, and

referral bonuses

available.

Subscribe

ğŸ‘‹ Find me on

LinkedIn

|

Twitter

|

Threads

|

Instagram

Want to advertise in this newsletter?

ğŸ“°

If your company wants to reach a 180K+ tech audience,

advertise with me

.

Thank you for supporting this newsletter.

You are now 180,001+ readers strong, very close to 181k. Letâ€™s try to get 181k readers by 21 November. Consider sharing this post with your friends and get rewards.

Yâ€™all are the best.

Share

Block diagrams created with

Eraser",https://newsletter.systemdesign.one
